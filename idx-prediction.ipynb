{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4a1cd4",
   "metadata": {},
   "source": [
    "# INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c64822-54a0-4aca-b326-6d8827d2a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from IPython.display import display\n",
    "\n",
    "# Daftar ticker yang ingin diambil\n",
    "tickers = [\n",
    "    # Saham LQ45 awal:\n",
    "    \"BBRI.JK\", \"BMRI.JK\", \"BBCA.JK\", \"TLKM.JK\", \"BBNI.JK\", \"BRIS.JK\", \"BRPT.JK\", \n",
    "    \"PTRO.JK\", \"ADRO.JK\", \"ASII.JK\", \"ANTM.JK\", \"BBTN.JK\", \"CPIN.JK\", \"ERAA.JK\", \n",
    "    \"GGRM.JK\", \"HMSP.JK\", \"ICBP.JK\", \"INCO.JK\", \"INDF.JK\", \"INKP.JK\", \"INTP.JK\", \n",
    "    \"ITMG.JK\", \"JPFA.JK\", \"JSMR.JK\", \"KLBF.JK\", \"MDKA.JK\", \"MEDC.JK\", \"MIKA.JK\", \n",
    "    \"MNCN.JK\", \"PGAS.JK\", \"PTBA.JK\", \"SCMA.JK\", \"SMGR.JK\", \"SMRA.JK\", \"TBIG.JK\", \n",
    "    \"TINS.JK\", \"TKIM.JK\", \"TOWR.JK\", \"UNTR.JK\", \"UNVR.JK\", \"WSKT.JK\", \"EXCL.JK\", \"AMRT.JK\",\n",
    "    \n",
    "    # Grup Prajogo Pangestu\n",
    "    \"TPIA.JK\", \"STAR.JK\", \"BREN.JK\", \"CUAN.JK\",\n",
    "    \n",
    "    # Grup Salim\n",
    "    \"IMAS.JK\", \"ROTI.JK\", \"DPNS.JK\",\n",
    "    \n",
    "    # Grup Hartono (Djarum)\n",
    "    \"BELI.JK\",\n",
    "    \n",
    "    # Grup Chairul Tanjung\n",
    "    \"MEGA.JK\",\n",
    "    \n",
    "    # Grup Garibaldi (Boy) Thohir\n",
    "    \"GOTO.JK\",\n",
    "    \n",
    "    # Grup Mochtar Riady (Lippo)\n",
    "    \"LPKR.JK\", \"SILO.JK\", \"LPPF.JK\", \"MLPL.JK\",\n",
    "    \n",
    "    # Grup Hary Tanoesoedibjo (MNC)\n",
    "    \"MSIN.JK\", \"BCAP.JK\", \"BABP.JK\",\n",
    "    \n",
    "    # Grup Eddy Kusnadi Sariaatmadja (Emtek)\n",
    "    \"EMTK.JK\", \"BUKA.JK\",\n",
    "    \n",
    "    # Grup Peter Sondakh (Rajawali)\n",
    "    \"ARCI.JK\", \"BWPT.JK\", \"META.JK\",\n",
    "    \n",
    "    # Grup Alexander Tedja (Pakuwon)\n",
    "    \"PWON.JK\",\n",
    "    \n",
    "    # Grup Eka Tjipta Widjaja (Sinarmas)\n",
    "    \"SMAR.JK\", \"DSSA.JK\", \"BSDE.JK\",\n",
    "    \n",
    "    # Grup Sugianto Kusuma (Aguan - Agung Sedayu)\n",
    "    \"PANI.JK\",\n",
    "    \n",
    "    # Grup Saratoga (Edwin Soeryadjaya & Sandiaga Uno)\n",
    "    \"SRTG.JK\", \"MPMX.JK\", \"PALM.JK\", \"AGII.JK\", \"PRAY.JK\"\n",
    "]\n",
    "\n",
    "# Simpan semua hasil dalam satu list (untuk warm-up stage)\n",
    "data_rows = []\n",
    "failed_tickers = []\n",
    "\n",
    "def warm_up_data():\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Ambil data 8 hari terakhir untuk setiap ticker\n",
    "            data = yf.download(ticker, interval=\"1m\", period=\"8d\", timeout=20)\n",
    "            # Ubah timezone ke Jakarta (WIB)\n",
    "            data.index = data.index.tz_convert(\"Asia/Jakarta\")\n",
    "\n",
    "            # Ambil tanggal unik\n",
    "            data['Date'] = data.index.date\n",
    "            unique_dates = data['Date'].unique()\n",
    "\n",
    "            for date in unique_dates:\n",
    "                # Filter data hanya untuk tanggal ini\n",
    "                daily_data = data[data['Date'] == date]\n",
    "\n",
    "                # Ambil data menit pertama (09:00) dan kedua (09:01)\n",
    "                first_minute = daily_data.between_time(\"09:00:00\", \"09:00:59\")\n",
    "                second_minute = daily_data.between_time(\"09:01:00\", \"09:01:59\")\n",
    "\n",
    "                # Ambil harga penutupan harian (16:00)\n",
    "                close_data = daily_data.between_time(\"15:59:00\", \"16:00:00\")\n",
    "                close_price = close_data[\"Close\"].iloc[0] if not close_data.empty else None\n",
    "\n",
    "                # Pastikan tidak ada data yang kosong\n",
    "                if not first_minute.empty and not second_minute.empty:\n",
    "                    data_rows.append([\n",
    "                        ticker.replace(\".JK\", \"\"), date,\n",
    "                        first_minute[\"Open\"].iloc[0].item(), first_minute[\"High\"].iloc[0].item(), \n",
    "                        first_minute[\"Low\"].iloc[0].item(), first_minute[\"Close\"].iloc[0].item(),\n",
    "                        second_minute[\"Open\"].iloc[0].item(), second_minute[\"High\"].iloc[0].item(), \n",
    "                        second_minute[\"Low\"].iloc[0].item(), second_minute[\"Close\"].iloc[0].item(),\n",
    "                        close_price.item() if close_price is not None else None\n",
    "                    ])\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Gagal mengambil data {ticker}: {e}\")\n",
    "            failed_tickers.append(ticker)\n",
    "\n",
    "    # Coba lagi untuk ticker yang gagal\n",
    "    if failed_tickers:\n",
    "        print(\"ðŸ”„ Mencoba ulang untuk ticker yang gagal...\")\n",
    "        for ticker in failed_tickers:\n",
    "            download_data_for_ticker(ticker)\n",
    "\n",
    "# Lakukan warming-up dengan mengunduh data\n",
    "warm_up_data()\n",
    "\n",
    "# Buat DataFrame hasil\n",
    "columns = [\"Ticker\", \"Tanggal\", \"O1\", \"H1\", \"L1\", \"C1\", \"O2\", \"H2\", \"L2\", \"C2\", \"Close Price\"]\n",
    "result_df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "# Tampilkan hasil data sebelum aplikasi logika\n",
    "print(\"Data setelah warm-up:\")\n",
    "display(result_df)\n",
    "\n",
    "# Logic Validation: Tambahkan Kolom is_2min_valid\n",
    "if not result_df.empty:\n",
    "    result_df['is_2min_valid'] = result_df.apply(\n",
    "        lambda row: 1 if row['C1'] > row['O1'] and row['Close Price'] >= row['C2'] else 0, axis=1\n",
    "    )\n",
    "\n",
    "    # Setup tampilan Pandas\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "\n",
    "    # Tampilkan hasil setelah logic validasi\n",
    "    print(\"Data setelah logic validasi:\")\n",
    "    display(result_df)\n",
    "else:\n",
    "    print(\"Tidak ada data yang berhasil diproses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0c020-a2fc-4096-b12e-37409ea57cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from IPython.display import display\n",
    "\n",
    "# Daftar ticker yang ingin diambil\n",
    "tickers = [\n",
    "    \"BBRI.JK\", \"BMRI.JK\", \"BBCA.JK\", \"TLKM.JK\", \"BBNI.JK\"\n",
    "]\n",
    "\n",
    "# Ticker untuk indeks Hang Seng dan Nikkei\n",
    "index_tickers = {\n",
    "    'hang_seng': \"^HSI\",\n",
    "    'nikkei': \"^N225\"\n",
    "}\n",
    "\n",
    "# Simpan semua hasil dalam satu list (untuk warm-up stage)\n",
    "data_rows = []\n",
    "failed_tickers = []\n",
    "\n",
    "def download_index_data(ticker, start, end):\n",
    "    try:\n",
    "        # Ambil data trading pada hari yang bersangkutan\n",
    "        data = yf.download(ticker, start=start, end=end, interval=\"1d\", timeout=20)  # Daily OHLC data\n",
    "        if not data.empty:\n",
    "            first_ohlc = data.iloc[0][['Open', 'High', 'Low', 'Close']]\n",
    "            return first_ohlc.tolist()\n",
    "        else:\n",
    "            return [None, None, None, None]\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Gagal mengambil data {ticker}: {e}\")\n",
    "        return [None, None, None, None]\n",
    "\n",
    "def warm_up_data():\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Ambil data 8 hari terakhir untuk setiap ticker\n",
    "            data = yf.download(ticker, interval=\"1m\", period=\"8d\", timeout=20)\n",
    "            # Ubah timezone ke Jakarta (WIB)\n",
    "            data.index = data.index.tz_convert(\"Asia/Jakarta\")\n",
    "\n",
    "            # Ambil tanggal unik\n",
    "            data['Date'] = data.index.date\n",
    "            unique_dates = data['Date'].unique()\n",
    "\n",
    "            for date in unique_dates:\n",
    "                # Filter data hanya untuk tanggal ini\n",
    "                daily_data = data[data['Date'] == date]\n",
    "\n",
    "                # Load index values for Hang Seng and Nikkei\n",
    "                hs_ohlc = download_index_data(index_tickers['hang_seng'], date, date + pd.Timedelta(days=1))\n",
    "                nikkei_ohlc = download_index_data(index_tickers['nikkei'], date, date + pd.Timedelta(days=1))\n",
    "\n",
    "                # Ambil data menit pertama (09:00) dan kedua (09:01)\n",
    "                first_minute = daily_data.between_time(\"09:00:00\", \"09:00:59\")\n",
    "                second_minute = daily_data.between_time(\"09:01:00\", \"09:01:59\")\n",
    "\n",
    "                # Ambil harga penutupan harian (16:00)\n",
    "                close_data = daily_data.between_time(\"15:59:00\", \"16:00:00\")\n",
    "                close_price = close_data[\"Close\"].iloc[0] if not close_data.empty else None\n",
    "\n",
    "                if not first_minute.empty and not second_minute.empty:\n",
    "                    hs_open, hs_high, hs_low, hs_close = hs_ohlc\n",
    "                    nikkei_open, nikkei_high, nikkei_low, nikkei_close = nikkei_ohlc\n",
    "\n",
    "                    # Tentukan apakah Hang Seng dan Nikkei Bullish atau Bearish\n",
    "                    hs_bull = 1 if (hs_close > hs_open) else 0\n",
    "                    nikkei_bull = 1 if (nikkei_close > nikkei_open) else 0\n",
    "\n",
    "                    data_rows.append([\n",
    "                        date,\n",
    "                        hs_open, hs_high, hs_low, hs_close,  # Hang Seng\n",
    "                        nikkei_open, nikkei_high, nikkei_low, nikkei_close,  # Nikkei\n",
    "                        ticker.replace(\".JK\", \"\"),\n",
    "                        first_minute[\"Open\"].iloc[0].item(), first_minute[\"High\"].iloc[0].item(), \n",
    "                        first_minute[\"Low\"].iloc[0].item(), first_minute[\"Close\"].iloc[0].item(),\n",
    "                        second_minute[\"Open\"].iloc[0].item(), second_minute[\"High\"].iloc[0].item(), \n",
    "                        second_minute[\"Low\"].iloc[0].item(), second_minute[\"Close\"].iloc[0].item(),\n",
    "                        close_price.item() if close_price is not None else None,\n",
    "                        hs_bull,  # Kolom untuk ngeset bullish atau bearish Hang Seng\n",
    "                        nikkei_bull  # Kolom untuk ngeset bullish atau bearish Nikkei\n",
    "                    ])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Gagal mengambil data {ticker}: {e}\")\n",
    "            failed_tickers.append(ticker)\n",
    "\n",
    "    if failed_tickers:\n",
    "        print(\"ðŸ”„ Mencoba ulang untuk ticker yang gagal...\")\n",
    "        for ticker in failed_tickers:\n",
    "            download_index_data(ticker)\n",
    "\n",
    "# Lakukan warm-up dengan mengunduh data\n",
    "warm_up_data()\n",
    "\n",
    "# Buat DataFrame hasil dengan kolom terstruktur\n",
    "columns = [\n",
    "    \"TGL\",\n",
    "    \"HS_O\", \"HS_H\", \"HS_L\", \"HS_C\", \n",
    "    \"Ni_O\", \"Ni_H\", \"Ni_L\", \"Ni_C\",\n",
    "    \"SAHAM\", \n",
    "    \"O1\", \"H1\", \"L1\", \"C1\", \n",
    "    \"O2\", \"H2\", \"L2\", \"C2\", \n",
    "    \"CLOSE\",\n",
    "    \"HS_Bull\", \"Ni_Bull\"  # Kolom tambahan untuk bull/bear\n",
    "]\n",
    "result_df = pd.DataFrame(data_rows, columns=columns)\n",
    "\n",
    "# Batasi semua nilai float ke 2 desimal\n",
    "float_columns = [\n",
    "    \"HS_O\", \"HS_H\", \"HS_L\", \"HS_C\", \n",
    "    \"Ni_O\", \"Ni_H\", \"Ni_L\", \"Ni_C\",\n",
    "    \"O1\", \"H1\", \"L1\", \"C1\", \n",
    "    \"O2\", \"H2\", \"L2\", \"C2\", \n",
    "    \"CLOSE\"\n",
    "]\n",
    "\n",
    "result_df[float_columns] = result_df[float_columns].round(2)\n",
    "\n",
    "# Tampilkan hasil data sebelum aplikasi logika\n",
    "print(\"Data setelah warm-up:\")\n",
    "display(result_df)\n",
    "\n",
    "# Logic Validation: Tambahkan Kolom is_2min_valid\n",
    "if not result_df.empty:\n",
    "    result_df['is_2min_valid'] = result_df.apply(\n",
    "        lambda row: 1 if (row.get('C1', 0) > row.get('O1', 0) and \n",
    "                           row.get('C2', 0) >= row.get('C1', 0) and \n",
    "                           row.get('Close Price', 0) >= row.get('C2', 0)) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Setup tampilan Pandas agar hanya menampilkan 2 desimal\n",
    "    pd.options.display.float_format = '{:.2f}'.format\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "\n",
    "    # Tampilkan hasil setelah logic validasi\n",
    "    print(\"Data setelah logic validasi:\")\n",
    "    display(result_df)\n",
    "else:\n",
    "    print(\"Tidak ada data yang berhasil diproses.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f7e81",
   "metadata": {},
   "source": [
    "# PREFLIGHT FOR YFINANCE CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4727f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREFLIGHT: Cek koneksi ke Yahoo Finance / yfinance sebelum download berat ===\n",
    "# aman dijalankan berulang\n",
    "!pip install -q requests yfinance\n",
    "\n",
    "import socket, ssl, time, json, os\n",
    "import requests, yfinance as yf\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "TIMEOUT = 15  # detik\n",
    "\n",
    "ENDPOINTS = [\n",
    "    \"https://query1.finance.yahoo.com/v7/finance/quote?symbols=AAPL\",\n",
    "    \"https://query1.finance.yahoo.com/v8/finance/chart/TLKM.JK?range=5d&interval=1d\",\n",
    "]\n",
    "\n",
    "TEST_SYMBOLS = [\"AAPL\", \"TLKM.JK\"]  # 1 global, 1 Indonesia\n",
    "RESULTS = []\n",
    "\n",
    "def check_dns(host):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        ip = socket.gethostbyname(host)\n",
    "        return True, ip, (time.time()-t0)\n",
    "    except Exception as e:\n",
    "        return False, str(e), (time.time()-t0)\n",
    "\n",
    "def check_tls(host, port=443):\n",
    "    t0 = time.time()\n",
    "    ctx = ssl.create_default_context()\n",
    "    try:\n",
    "        with socket.create_connection((host, port), timeout=TIMEOUT) as sock:\n",
    "            with ctx.wrap_socket(sock, server_hostname=host) as ssock:\n",
    "                cert = ssock.getpeercert()\n",
    "        return True, \"ok\", (time.time()-t0)\n",
    "    except Exception as e:\n",
    "        return False, str(e), (time.time()-t0)\n",
    "\n",
    "def http_probe(url):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        r = requests.get(url, timeout=TIMEOUT, allow_redirects=False,\n",
    "                         headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "        return True, {\"status\": r.status_code, \"location\": r.headers.get(\"Location\"), \"len\": len(r.content)}, (time.time()-t0)\n",
    "    except Exception as e:\n",
    "        return False, str(e), (time.time()-t0)\n",
    "\n",
    "def yf_probe(symbol, period=\"5d\", interval=\"1d\"):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        df = yf.download(symbol, period=period, interval=interval, progress=False, threads=False, timeout=TIMEOUT)\n",
    "        ok = not df.empty\n",
    "        return ok, {\"rows\": len(df)}, (time.time()-t0)\n",
    "    except Exception as e:\n",
    "        return False, str(e), (time.time()-t0)\n",
    "\n",
    "print(\"ðŸ”Ž Preflight: Yahoo Finance connectivity\\n\")\n",
    "\n",
    "# 1) DNS + TLS untuk host utama\n",
    "hosts = {\"query1.finance.yahoo.com\", \"guce.yahoo.com\", \"www.yahoo.com\"}\n",
    "for h in hosts:\n",
    "    ok, info, dt = check_dns(h)\n",
    "    RESULTS.append((\"DNS\", h, ok, info, dt))\n",
    "    print(f\"DNS  {h:<28} {'OK' if ok else 'FAIL'}  {info}  ({dt:.2f}s)\")\n",
    "for h in hosts:\n",
    "    ok, info, dt = check_tls(h, 443)\n",
    "    RESULTS.append((\"TLS\", h, ok, info, dt))\n",
    "    print(f\"TLS  {h:<28} {'OK' if ok else 'FAIL'}  {info}  ({dt:.2f}s)\")\n",
    "\n",
    "# 2) HTTP GET ke endpoint quote/chart (tanpa yfinance)\n",
    "for url in ENDPOINTS:\n",
    "    host = urlparse(url).hostname\n",
    "    ok, info, dt = http_probe(url)\n",
    "    RESULTS.append((\"HTTP\", host, ok, info, dt))\n",
    "    if ok:\n",
    "        status = info[\"status\"]\n",
    "        loc    = info[\"location\"]\n",
    "        note   = f\"status={status}\" + (f\", redirectâ†’{loc}\" if loc else \"\")\n",
    "        print(f\"HTTP {host:<28} OK    {note}  ({dt:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"HTTP {host:<28} FAIL  {info}  ({dt:.2f}s)\")\n",
    "\n",
    "# 3) Tes ringan via yfinance untuk 2 simbol\n",
    "for sym in TEST_SYMBOLS:\n",
    "    ok, info, dt = yf_probe(sym, period=\"5d\", interval=\"1d\")\n",
    "    RESULTS.append((\"YF\", sym, ok, info, dt))\n",
    "    if ok:\n",
    "        print(f\"YF   {sym:<10} OK    rows={info['rows']}  ({dt:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"YF   {sym:<10} FAIL  {info}  ({dt:.2f}s)\")\n",
    "\n",
    "# 4) Rekomendasi cepat berdasarkan hasil\n",
    "print(\"\\nðŸ©º Diagnosis & saran:\")\n",
    "dns_fail = [r for r in RESULTS if r[0]==\"DNS\" and not r[2]]\n",
    "http_fail= [r for r in RESULTS if r[0]==\"HTTP\" and not r[2]]\n",
    "yf_fail  = [r for r in RESULTS if r[0]==\"YF\" and not r[2]]\n",
    "\n",
    "if dns_fail:\n",
    "    print(\"- DNS gagal untuk:\", \", \".join(set([h for _,h,_,_,_ in dns_fail])))\n",
    "    print(\"  âžœ Coba ganti DNS (mis. 8.8.8.8/1.1.1.1) atau cek VPN/firewall.\")\n",
    "if http_fail:\n",
    "    # deteksi redirect ke guce (consent)\n",
    "    redir_to_guce = any(isinstance(info, dict) and info.get(\"location\",\"\",) and \"guce.yahoo.com\" in info.get(\"location\",\"\") for _,_,_,info,_ in RESULTS if _==\"HTTP\")\n",
    "    if redir_to_guce:\n",
    "        print(\"- Ter-redirect ke guce.yahoo.com (consent).\")\n",
    "        print(\"  âžœ Buka https://www.yahoo.com di browser (set consent/cookies), lalu jalankan ulang.\")\n",
    "    else:\n",
    "        print(\"- HTTP ke endpoint Yahoo gagal/time-out.\")\n",
    "        print(\"  âžœ Cek koneksi umum & coba lagi beberapa menit (bisa throttling).\")\n",
    "if yf_fail and not http_fail:\n",
    "    print(\"- yfinance gagal, tapi HTTP oke â†’ kemungkinan rate limit.\")\n",
    "    print(\"  âžœ Kecilkan batch, tambah jeda, pakai slow mode (1-per-1), atau lanjut saat off-peak.\")\n",
    "if not (dns_fail or http_fail or yf_fail):\n",
    "    print(\"- Semua cek OK âœ… â€” lanjutkan pipeline unduhan.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1acb1",
   "metadata": {},
   "source": [
    "# GET ACTIVE EMITEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ee829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL CLEANER (pakai config kamu) ===\n",
    "# progress bar + cache + retry/backoff + slow-mode; output & cache di folder emiten/\n",
    "!pip install -q yfinance pandas numpy tqdm openpyxl xlrd\n",
    "\n",
    "import os, re, time, random, warnings\n",
    "import numpy as np, pandas as pd, yfinance as yf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== CONFIG (ambil punyamu) =====\n",
    "INPUT_PATH         = \"candidates_from_excel.csv\"\n",
    "PREFER_EXCEL       = False\n",
    "FOLDER_OUT         = \"emiten\"\n",
    "\n",
    "LOOKBACK_DAYS      = 30         # cek aktivitas 30 hari terakhir\n",
    "MIN_NONZERO_DAYS   = 15\n",
    "MIN_PCT_NONZERO    = 0.50\n",
    "MAX_CONSEC_ZERO    = 10\n",
    "MIN_MED_VALUE_90D  = 7.5e9\n",
    "MIN_PRICE_FLOOR    = 75\n",
    "MIN_TURNOVER       = 0.0005\n",
    "\n",
    "DL_PERIOD_FOR_ACTIVITY = 60     # -> '60d'\n",
    "DL_PERIOD_FOR_LIQ      = 100    # -> '100d'\n",
    "CHUNK_SIZE_MAIN        = 5\n",
    "PAUSE_MAIN             = 2.0\n",
    "CHUNK_SIZE_SECOND      = 10\n",
    "PAUSE_SECOND           = 1.5\n",
    "MAX_RETRIES            = 3\n",
    "RETRY_BACKOFF          = 1.6\n",
    "TIMEOUT_SEC            = 45\n",
    "SLOWMODE_COOLDOWN      = 120    # detik; jeda panjang saat rate-limit\n",
    "\n",
    "# ===== Setup output =====\n",
    "DATE_TAG = datetime.now().strftime(\"%Y%m%d\")\n",
    "os.makedirs(FOLDER_OUT, exist_ok=True)\n",
    "OUTPUT_ACTIVE_CSV  = os.path.join(FOLDER_OUT, f\"candidates_active_filtered_{DATE_TAG}.csv\")\n",
    "OUTPUT_FULL_CSV    = os.path.join(FOLDER_OUT, f\"candidates_full_with_flags_{DATE_TAG}.csv\")\n",
    "\n",
    "# ===== Helpers =====\n",
    "def extract_tickers_from_df(df):\n",
    "    tickers = set()\n",
    "    for c in df.columns:\n",
    "        s = df[c].astype(str).str.upper().str.strip()\n",
    "        extracted = s.str.extract(r'\\b([A-Z]{2,5}(?:\\.JK)?)\\b')[0].dropna()\n",
    "        for sym in extracted:\n",
    "            base = sym.replace(\".JK\",\"\")\n",
    "            if 2 <= len(base) <= 5 and base.isalpha():\n",
    "                tickers.add(sym if sym.endswith(\".JK\") else f\"{sym}.JK\")\n",
    "    return sorted(tickers)\n",
    "\n",
    "def load_candidates(path, prefer_excel=False):\n",
    "    p = path.lower()\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File input tidak ada: {path}\")\n",
    "    if prefer_excel and (p.endswith(\".xlsx\") or p.endswith(\".xls\")):\n",
    "        try:\n",
    "            engine = \"openpyxl\" if p.endswith(\".xlsx\") else \"xlrd\"\n",
    "            xl = pd.ExcelFile(path, engine=engine)\n",
    "            codes=set()\n",
    "            for sh in xl.sheet_names:\n",
    "                df = xl.parse(sh)\n",
    "                codes |= set(extract_tickers_from_df(df))\n",
    "            return sorted(codes)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Gagal baca Excel ({e}), fallback CSVâ€¦\")\n",
    "    df = pd.read_csv(path)\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    if \"ticker\" in cols_lower:\n",
    "        s = df[df.columns[cols_lower.index(\"ticker\")]]\n",
    "        return extract_tickers_from_df(pd.DataFrame({\"ticker\": s}))\n",
    "    if \"code\" in cols_lower:\n",
    "        s = df[df.columns[cols_lower.index(\"code\")]]\n",
    "        return extract_tickers_from_df(pd.DataFrame({\"code\": s}))\n",
    "    return extract_tickers_from_df(df)\n",
    "\n",
    "def _slice_px(px, t, batch_len):\n",
    "    if isinstance(px.columns, pd.MultiIndex):\n",
    "        try: return px[t].dropna()\n",
    "        except Exception: return pd.DataFrame()\n",
    "    else:\n",
    "        return px.dropna() if batch_len == 1 else pd.DataFrame()\n",
    "\n",
    "def _period_arg(period):  # int -> 'Nd'\n",
    "    return period if isinstance(period, str) else f\"{int(period)}d\"\n",
    "\n",
    "def _cache_paths(base_dir, period, ticker):\n",
    "    cache_dir = Path(base_dir) / f\"cache_{period}\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return cache_dir, cache_dir / f\"{ticker}.csv\"\n",
    "\n",
    "def _save_cache(df, path_csv):\n",
    "    try: df.to_csv(path_csv, index=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _load_cache(path_csv):\n",
    "    try:\n",
    "        df = pd.read_csv(path_csv, parse_dates=True, index_col=0)\n",
    "        req = {\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"}\n",
    "        return df if req.issubset(df.columns) else pd.DataFrame()\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def download_panel(\n",
    "    tickers, period_days,\n",
    "    cache_base=\"emiten\",\n",
    "    chunk_main=15, pause_main=1.2,\n",
    "    chunk_second=10, pause_second=1.5,\n",
    "    max_retries=3, backoff=1.6,\n",
    "    timeout=45, jitter=0.3,\n",
    "    slowmode_cooldown=120\n",
    "):\n",
    "    period = _period_arg(period_days)\n",
    "    out = {}\n",
    "    # cache hit\n",
    "    cached = 0\n",
    "    for t in tickers:\n",
    "        _, cpath = _cache_paths(cache_base, period, t)\n",
    "        df = _load_cache(cpath)\n",
    "        if not df.empty:\n",
    "            out[t] = df; cached += 1\n",
    "    if cached: print(f\"ðŸ’¾ Cache hit: {cached}/{len(tickers)}\")\n",
    "\n",
    "    remaining = [t for t in tickers if t not in out]\n",
    "    if not remaining: return out\n",
    "\n",
    "    # main pass batched (threads=False)\n",
    "    batches = list(range(0, len(remaining), chunk_main))\n",
    "    pbar = tqdm(batches, desc=f\"Downloading {period} OHLCV (main)\", unit=\"batch\")\n",
    "    rate_limited = False\n",
    "\n",
    "    for start in pbar:\n",
    "        batch = [t for t in remaining[start:start+chunk_main] if t not in out]\n",
    "        if not batch: continue\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                px = yf.download(\n",
    "                    batch, period=period, interval=\"1d\",\n",
    "                    auto_adjust=False, group_by=\"ticker\",\n",
    "                    progress=False, threads=False, timeout=timeout\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt > max_retries:\n",
    "                    rate_limited = True\n",
    "                    tqdm.write(f\"[main] batch failed: {type(e).__name__} â†’ switch to slow mode\")\n",
    "                    px = pd.DataFrame(); break\n",
    "                sleep_s = pause_main * (backoff ** (attempt-1)) + random.uniform(0, jitter)\n",
    "                tqdm.write(f\"[main] retry {attempt}/{max_retries} after {sleep_s:.1f}s ({type(e).__name__})\")\n",
    "                time.sleep(sleep_s)\n",
    "        for t in batch:\n",
    "            try: df = _slice_px(px, t, len(batch))\n",
    "            except Exception: df = pd.DataFrame()\n",
    "            if not df.empty:\n",
    "                out[t] = df\n",
    "                _, cpath = _cache_paths(cache_base, period, t)\n",
    "                _save_cache(df, cpath)\n",
    "        time.sleep(pause_main + random.uniform(0, jitter))\n",
    "\n",
    "    # slow mode sequential for remaining\n",
    "    still = [t for t in tickers if t not in out]\n",
    "    if still:\n",
    "        if rate_limited:\n",
    "            tqdm.write(f\"ðŸ•’ Rate-limited. Cooling down {slowmode_cooldown}s before slow modeâ€¦\")\n",
    "            time.sleep(slowmode_cooldown)\n",
    "        pbar2 = tqdm(still, desc=\"Slow mode (1-by-1)\", unit=\"ticker\")\n",
    "        for t in pbar2:\n",
    "            _, cpath = _cache_paths(cache_base, period, t)\n",
    "            df = _load_cache(cpath)\n",
    "            if not df.empty:\n",
    "                out[t] = df; continue\n",
    "            attempt = 0; pause = pause_main\n",
    "            while True:\n",
    "                try:\n",
    "                    px = yf.download(t, period=period, interval=\"1d\",\n",
    "                                     auto_adjust=False, progress=False,\n",
    "                                     threads=False, timeout=timeout)\n",
    "                    df = _slice_px(px, t, 1); break\n",
    "                except Exception as e:\n",
    "                    attempt += 1\n",
    "                    if attempt > (max_retries + 1):\n",
    "                        df = pd.DataFrame(); break\n",
    "                    sleep_s = pause * (backoff ** (attempt-1)) + random.uniform(0, jitter)\n",
    "                    pbar2.set_postfix_str(f\"retry {attempt} ({type(e).__name__})\")\n",
    "                    time.sleep(sleep_s)\n",
    "            if not df.empty:\n",
    "                out[t] = df; _save_cache(df, cpath)\n",
    "            else:\n",
    "                time.sleep(pause_main + random.uniform(0, jitter))\n",
    "\n",
    "    missing = [t for t in tickers if t not in out or getattr(out[t], \"empty\", True)]\n",
    "    if missing:\n",
    "        fail_csv = Path(cache_base) / f\"failed_{period}_{DATE_TAG}.csv\"\n",
    "        pd.DataFrame({\"ticker\": missing}).to_csv(fail_csv, index=False)\n",
    "        print(f\"âš ï¸ Gagal final: {len(missing)} tickers. Disimpan ke {fail_csv}\")\n",
    "    else:\n",
    "        print(\"âœ… Semua ticker berhasil/cached.\")\n",
    "    return out\n",
    "\n",
    "def fetch_meta(tickers):\n",
    "    rows=[]\n",
    "    for t in tickers:\n",
    "        tk = yf.Ticker(t)\n",
    "        try: info = tk.info or {}\n",
    "        except Exception: info = {}\n",
    "        rows.append({\n",
    "            \"ticker\": t,\n",
    "            \"marketCap\": info.get(\"marketCap\"),\n",
    "            \"sector\": info.get(\"sector\"),\n",
    "            \"industry\": info.get(\"industry\"),\n",
    "            \"sharesOut\": info.get(\"sharesOutstanding\") or info.get(\"floatShares\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ===== Run =====\n",
    "CANDIDATES_ALL = load_candidates(INPUT_PATH, prefer_excel=PREFER_EXCEL)\n",
    "print(f\"ðŸ“¥ Total kandidat masuk: {len(CANDIDATES_ALL)}\")\n",
    "if not CANDIDATES_ALL:\n",
    "    raise SystemExit(\"Daftar kandidat kosong.\")\n",
    "\n",
    "print(\"â¬ Unduh OHLCV aktivitasâ€¦\")\n",
    "px_act = download_panel(\n",
    "    CANDIDATES_ALL, DL_PERIOD_FOR_ACTIVITY,\n",
    "    cache_base=FOLDER_OUT,\n",
    "    chunk_main=CHUNK_SIZE_MAIN, pause_main=PAUSE_MAIN,\n",
    "    chunk_second=CHUNK_SIZE_SECOND, pause_second=PAUSE_SECOND,\n",
    "    max_retries=MAX_RETRIES, backoff=RETRY_BACKOFF,\n",
    "    timeout=TIMEOUT_SEC, slowmode_cooldown=SLOWMODE_COOLDOWN\n",
    ")\n",
    "\n",
    "print(\"â¬ Unduh OHLCV likuiditasâ€¦\")\n",
    "px_liq = download_panel(\n",
    "    CANDIDATES_ALL, DL_PERIOD_FOR_LIQ,\n",
    "    cache_base=FOLDER_OUT,\n",
    "    chunk_main=CHUNK_SIZE_MAIN, pause_main=PAUSE_MAIN,\n",
    "    chunk_second=CHUNK_SIZE_SECOND, pause_second=PAUSE_SECOND,\n",
    "    max_retries=MAX_RETRIES, backoff=RETRY_BACKOFF,\n",
    "    timeout=TIMEOUT_SEC, slowmode_cooldown=SLOWMODE_COOLDOWN\n",
    ")\n",
    "\n",
    "print(\"â„¹ï¸ Ambil metadataâ€¦\")\n",
    "meta = fetch_meta(CANDIDATES_ALL)\n",
    "\n",
    "# ===== Hitung metrik & klasifikasi =====\n",
    "rows=[]\n",
    "for t in CANDIDATES_ALL:\n",
    "    dfa = px_act.get(t, pd.DataFrame())\n",
    "    dfl = px_liq.get(t, pd.DataFrame())\n",
    "\n",
    "    last_close = float(dfa[\"Close\"].iloc[-1]) if not dfa.empty else np.nan\n",
    "    nonzero_days = 0; pct_nonzero = 0.0; max_zero_run = LOOKBACK_DAYS\n",
    "    if not dfa.empty:\n",
    "        look = dfa.tail(LOOKBACK_DAYS)\n",
    "        if not look.empty:\n",
    "            v = look[\"Volume\"].fillna(0)\n",
    "            nonzero_days = int((v>0).sum())\n",
    "            pct_nonzero  = float(nonzero_days/len(look))\n",
    "            runs = (v==0).astype(int)\n",
    "            if runs.any():\n",
    "                grp = (runs != runs.shift()).cumsum()\n",
    "                max_zero_run = int(runs.groupby(grp).sum().max())\n",
    "            else:\n",
    "                max_zero_run = 0\n",
    "\n",
    "    med_value_90d = 0.0; med_vol_90d = 0.0\n",
    "    if not dfl.empty:\n",
    "        val = (dfl[\"Close\"]*dfl[\"Volume\"])\n",
    "        med_value_90d = float(val.rolling(90).median().dropna().iloc[-1]) if len(val)>=90 else float(val.median())\n",
    "        med_vol_90d   = float(dfl[\"Volume\"].rolling(90).median().dropna().iloc[-1]) if len(dfl)>=90 else float(dfl[\"Volume\"].median())\n",
    "\n",
    "    rows.append({\n",
    "        \"ticker\": t,\n",
    "        \"last_close\": last_close,\n",
    "        \"nonzero_days_30d\": nonzero_days,\n",
    "        \"pct_nonzero_30d\": round(pct_nonzero,3),\n",
    "        \"max_consec_zero_30d\": max_zero_run,\n",
    "        \"med_value_90d\": med_value_90d,\n",
    "        \"med_volume_90d\": med_vol_90d,\n",
    "    })\n",
    "\n",
    "feat = pd.DataFrame(rows)\n",
    "df = meta.merge(feat, on=\"ticker\", how=\"right\")\n",
    "df[\"turnover_med\"] = np.where(df[\"sharesOut\"].fillna(0)>0, df[\"med_volume_90d\"]/df[\"sharesOut\"], np.nan)\n",
    "\n",
    "def classify(r):\n",
    "    # full zero / sangat tidak aktif (window 30d)\n",
    "    if r[\"nonzero_days_30d\"] == 0 or r[\"pct_nonzero_30d\"] < 0.1 or r[\"max_consec_zero_30d\"] >= LOOKBACK_DAYS:\n",
    "        return \"SUSPECT_SUSPENDED\", \"no trading in window\"\n",
    "    reasons=[]\n",
    "    if not pd.isna(r[\"last_close\"]) and r[\"last_close\"] < MIN_PRICE_FLOOR: reasons.append(\"price < floor\")\n",
    "    if r[\"pct_nonzero_30d\"] < MIN_PCT_NONZERO: reasons.append(\"pct_nonzero < threshold\")\n",
    "    if r[\"nonzero_days_30d\"] < MIN_NONZERO_DAYS: reasons.append(\"nonzero_days < threshold\")\n",
    "    if r[\"max_consec_zero_30d\"] > MAX_CONSEC_ZERO: reasons.append(\"max zero run too long\")\n",
    "    if r[\"med_value_90d\"] < MIN_MED_VALUE_90D: reasons.append(\"med_value_90d < min\")\n",
    "    if not pd.isna(r[\"turnover_med\"]) and r[\"turnover_med\"] < MIN_TURNOVER: reasons.append(\"turnover < min\")\n",
    "    return (\"DORMANT\", \"; \".join(reasons)) if reasons else (\"ACTIVE\",\"pass\")\n",
    "\n",
    "lab = df.apply(classify, axis=1, result_type=\"expand\")\n",
    "lab.columns = [\"status\",\"why\"]\n",
    "df = pd.concat([df, lab], axis=1).sort_values([\"status\",\"med_value_90d\"], ascending=[True,False]).reset_index(drop=True)\n",
    "\n",
    "# ===== Save =====\n",
    "df.to_csv(OUTPUT_FULL_CSV, index=False, encoding=\"utf-8\")\n",
    "active = df[df[\"status\"]==\"ACTIVE\"][[\"ticker\"]]\n",
    "active.to_csv(OUTPUT_ACTIVE_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nâœ… Disaring: {len(df)} total | ACTIVE: {len(active)} | DORMANT: {len(df[df.status=='DORMANT'])} | SUSPECT_SUSPENDED: {len(df[df.status=='SUSPECT_SUSPENDED'])}\")\n",
    "print(f\"ðŸ’¾ Saved (full flags): {OUTPUT_FULL_CSV}\")\n",
    "print(f\"ðŸ’¾ Saved (active-only): {OUTPUT_ACTIVE_CSV}\")\n",
    "display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e4f5f",
   "metadata": {},
   "source": [
    "# BESOKARA V.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ARA PROB (DAILY INCREMENTAL + 1m, single cell) =========================\n",
    "# Output:\n",
    "#   emiten/prob_ara_today_YYYYMMDD_HHMM.csv\n",
    "#   emiten/prob_ara_tomorrow_YYYYMMDD_HHMM.csv\n",
    "#   emiten/cache_daily/<TICKER>.csv   (incremental append)\n",
    "#   emiten/cache_1m/<TICKER>.csv      (merge 7d)\n",
    "#   emiten/cache_5m/<TICKER>.csv      (fallback only, merge 12d)\n",
    "# ============================================================================\n",
    "!pip install -q yfinance pandas numpy\n",
    "\n",
    "import os, glob, time, math, warnings\n",
    "import numpy as np, pandas as pd, yfinance as yf\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, time as dtime\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "EMITEN_DIR           = \"emiten\"\n",
    "ACTIVE_PREFIX        = \"candidates_active_filtered_\"   # roster aktif harian\n",
    "MAX_TICKERS          = 200       # naikan sesuai kebutuhan (700 kalau mau full)\n",
    "\n",
    "# DAILY cache (append only)\n",
    "USE_CACHE_DAILY      = True\n",
    "CACHE_DAILY_DIR      = os.path.join(EMITEN_DIR, \"cache_daily\")\n",
    "DAILY_INIT_YEARS     = 5         # initial fetch if cache missing\n",
    "DAILY_WINDOW_DAYS    = 1500      # simpan ~6 thn; prune saat write\n",
    "\n",
    "# 1m cache (merge)\n",
    "USE_CACHE_1M         = True\n",
    "CACHE_1M_DIR         = os.path.join(EMITEN_DIR, \"cache_1m\")\n",
    "CACHE_1M_WINDOW_D    = 7         # Yahoo 1m â‰ˆ 7 hari\n",
    "\n",
    "# 5m fallback cache (merge)\n",
    "USE_CACHE_5M         = True\n",
    "CACHE_5M_DIR         = os.path.join(EMITEN_DIR, \"cache_5m\")\n",
    "CACHE_5M_WINDOW_D    = 12\n",
    "\n",
    "# ARA heuristik (boleh sesuaikan)\n",
    "def ara_limit_pct(price):\n",
    "    if price < 200: return 0.35\n",
    "    if price < 5000: return 0.25\n",
    "    return 0.20\n",
    "\n",
    "# Sinyal probabilistik (longgar)\n",
    "LAST_WINDOW_MIN      = 60   # menit akhir sesi\n",
    "RECENT_MOM_MIN       = 15   # momentum 15 menit\n",
    "VOL_SPIKE_MULT       = 1.6\n",
    "CR_MIN               = 0.70\n",
    "VIRGIN_HINT          = 0.06\n",
    "MIN_PRICE            = 50\n",
    "YF_TIMEOUT           = 45\n",
    "EXCHANGE_CLOSE_UTC   = (8, 0)  # 15:00 WIB = 08:00 UTC\n",
    "\n",
    "# =========================\n",
    "# UTIL & TZ\n",
    "# =========================\n",
    "def _now_date_tag(): return datetime.now().strftime(\"%Y%m%d\")\n",
    "def _now_ts_tag():   return datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "def _utc_naive_index(idx):\n",
    "    di = pd.to_datetime(idx, utc=True, errors=\"coerce\")\n",
    "    return pd.DatetimeIndex(di).tz_localize(None)\n",
    "\n",
    "def minutes_to_close(last_ts):\n",
    "    if not isinstance(last_ts, pd.Timestamp):\n",
    "        last_ts = pd.Timestamp(last_ts)\n",
    "    if getattr(last_ts, \"tz\", None) is not None:\n",
    "        last_ts = last_ts.tz_localize(None)\n",
    "    close_dt = datetime.combine(last_ts.date(), dtime(*EXCHANGE_CLOSE_UTC))\n",
    "    m = int((close_dt - last_ts).total_seconds() // 60)\n",
    "    return max(0, min(120, m))\n",
    "\n",
    "def _latest_file(folder, prefix):\n",
    "    today = os.path.join(folder, f\"{prefix}{_now_date_tag()}.csv\")\n",
    "    if os.path.exists(today): return today\n",
    "    cand = sorted(glob.glob(os.path.join(folder, f\"{prefix}*.csv\")))\n",
    "    if not cand: raise FileNotFoundError(f\"Tidak ada file {prefix}*.csv di {folder}\")\n",
    "    return cand[-1]\n",
    "\n",
    "def _load_tickers(path, col=\"ticker\"):\n",
    "    df = pd.read_csv(path)\n",
    "    cl = [c.lower() for c in df.columns]\n",
    "    assert col in cl, f\"Kolom '{col}' tidak ditemukan di {path}\"\n",
    "    s = df[df.columns[cl.index(col)]].astype(str).str.upper().str.strip()\n",
    "    return s.apply(lambda x: x if x.endswith(\".JK\") else f\"{x}.JK\").dropna().drop_duplicates().tolist()\n",
    "\n",
    "# =========================\n",
    "# TEST KONEKSI (harian & 1m)\n",
    "# =========================\n",
    "def test_yf():\n",
    "    try:\n",
    "        d = yf.download(\"TLKM.JK\", period=\"3mo\", interval=\"1d\", progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        m = yf.download(\"TLKM.JK\", period=\"7d\", interval=\"1m\",  progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        if d.empty or m.empty:\n",
    "            print(\"âš ï¸ Koneksi YF lemah (data kosong). Coba ulangi beberapa menit lagi.\")\n",
    "            return False\n",
    "        print(f\"âœ… Koneksi OK: daily={len(d)} bar, 1m={len(m)} bar\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gagal koneksi YF: {e}\")\n",
    "        return False\n",
    "\n",
    "# =========================\n",
    "# CACHE I/O GENERIK\n",
    "# =========================\n",
    "def _read_cache_csv(path):\n",
    "    if not os.path.exists(path): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.index = _utc_naive_index(df.index)\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _write_cache_csv(path, df, window_days):\n",
    "    if df.empty: return\n",
    "    dfx = df.copy()\n",
    "    dfx.index = _utc_naive_index(dfx.index)\n",
    "    cutoff = datetime.utcnow() - timedelta(days=window_days+1)\n",
    "    dfx = dfx[dfx.index >= cutoff]\n",
    "    dfx = dfx[~dfx.index.duplicated(keep=\"last\")].sort_index()\n",
    "    Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(path).write_text(dfx.to_csv())\n",
    "\n",
    "# =========================\n",
    "# FETCH DAILY (incremental append)\n",
    "# =========================\n",
    "def get_daily_incremental(t):\n",
    "    \"\"\"Append-only: jika cache ada â†’ unduh start=last_date+1, else initial fetch N years\"\"\"\n",
    "    path = os.path.join(CACHE_DAILY_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_DAILY else pd.DataFrame()\n",
    "\n",
    "    if base.empty:\n",
    "        fresh = yf.download(t, period=f\"{DAILY_INIT_YEARS}y\", interval=\"1d\",\n",
    "                            progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        if fresh.empty: return pd.DataFrame()\n",
    "        fresh.index = _utc_naive_index(fresh.index)\n",
    "        if USE_CACHE_DAILY: _write_cache_csv(path, fresh, DAILY_WINDOW_DAYS)\n",
    "        return fresh\n",
    "\n",
    "    # incremental\n",
    "    last_dt = pd.to_datetime(base.index.max())\n",
    "    start = (last_dt + pd.Timedelta(days=1)).date().isoformat()\n",
    "    fresh = yf.download(t, start=start, interval=\"1d\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty:\n",
    "        return base\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_DAILY: _write_cache_csv(path, comb, DAILY_WINDOW_DAYS)\n",
    "    return comb\n",
    "\n",
    "# =========================\n",
    "# FETCH 1m (merge 7d) + fallback 5m\n",
    "# =========================\n",
    "def get_1m_merge(t):\n",
    "    path = os.path.join(CACHE_1M_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_1M else pd.DataFrame()\n",
    "    fresh = yf.download(t, period=\"7d\", interval=\"1m\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty and base.empty:\n",
    "        # fallback 5m\n",
    "        return get_5m_merge(t), \"5m\"\n",
    "    if fresh.empty:\n",
    "        return base, \"1m\"\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    if base.empty:\n",
    "        if USE_CACHE_1M: _write_cache_csv(path, fresh, CACHE_1M_WINDOW_D)\n",
    "        return fresh, \"1m\"\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_1M: _write_cache_csv(path, comb, CACHE_1M_WINDOW_D)\n",
    "    return comb, \"1m\"\n",
    "\n",
    "def get_5m_merge(t):\n",
    "    path = os.path.join(CACHE_5M_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_5M else pd.DataFrame()\n",
    "    fresh = yf.download(t, period=f\"{CACHE_5M_WINDOW_D}d\", interval=\"5m\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty and base.empty: return pd.DataFrame()\n",
    "    if fresh.empty: return base\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    if base.empty:\n",
    "        if USE_CACHE_5M: _write_cache_csv(path, fresh, CACHE_5M_WINDOW_D)\n",
    "        return fresh\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_5M: _write_cache_csv(path, comb, CACHE_5M_WINDOW_D)\n",
    "    return comb\n",
    "\n",
    "# =========================\n",
    "# FEATS: daily context & intraday micro\n",
    "# =========================\n",
    "def closing_range_bar(row):\n",
    "    high = float(row[\"High\"]); low = float(row[\"Low\"]); close = float(row[\"Close\"])\n",
    "    rng = max(1e-8, high - low)\n",
    "    return 1.0 - float((high - close)/rng)\n",
    "\n",
    "def obv_series(df):\n",
    "    delta = np.sign(df[\"Close\"].diff()).fillna(0.0)\n",
    "    return (delta * df[\"Volume\"].fillna(0)).cumsum()\n",
    "\n",
    "def daily_regime_score(d1d):\n",
    "    if d1d is None or d1d.empty or len(d1d) < 40:\n",
    "        return 0.5, {}\n",
    "    df = d1d.dropna().copy()\n",
    "    df[\"EMA20\"] = df[\"Close\"].ewm(span=20).mean()\n",
    "    df[\"EMA50\"] = df[\"Close\"].ewm(span=50).mean()\n",
    "    delta = np.sign(df[\"Close\"].diff()).fillna(0.0)\n",
    "    obv = (delta * df[\"Volume\"].fillna(0)).cumsum()\n",
    "    obv_up = bool(len(obv)>5 and (obv.iloc[-1]-obv.iloc[-6])>0)\n",
    "    last = df.iloc[-1]\n",
    "    above_ema = bool((last[\"Close\"] > last[\"EMA20\"]) and (last[\"Close\"] > last[\"EMA50\"]))\n",
    "    hh20 = float(df[\"High\"].rolling(20).max().shift(1).iloc[-1]) if len(df)>20 else np.inf\n",
    "    breakout20 = bool(last[\"Close\"] > hh20)\n",
    "    # rVol daily 20\n",
    "    med20 = df[\"Volume\"].rolling(20).median().iloc[-1]\n",
    "    rvol_d20 = float(df[\"Volume\"].iloc[-1]/med20) if med20 and med20>0 else 1.0\n",
    "    # score 0..1\n",
    "    sc = 0.0\n",
    "    sc += 0.28 if above_ema else 0.0\n",
    "    sc += 0.26 if breakout20 else 0.0\n",
    "    sc += 0.24*min(1.5, rvol_d20)/1.5\n",
    "    sc += 0.22 if obv_up else 0.0\n",
    "    sc = float(max(0.0, min(1.0, sc)))\n",
    "    return sc, dict(above_ema=above_ema, breakout20=breakout20, rvol_d20=round(rvol_d20,2))\n",
    "\n",
    "def rvol_lastNmin(df_today, N, base_df):\n",
    "    if df_today.empty or base_df.empty: return np.nan\n",
    "    recent_vol = float(df_today[\"Volume\"].tail(max(1,N)).sum())\n",
    "    b = base_df.copy()\n",
    "    b[\"date\"] = b.index.date\n",
    "    buckets = [float(g[\"Volume\"].tail(max(1,N)).sum()) for _, g in b.groupby(\"date\")]\n",
    "    if len(buckets) < 2: return np.nan\n",
    "    med = float(np.median(buckets))\n",
    "    return (recent_vol / med) if med > 0 else np.nan\n",
    "\n",
    "def sigm(x): return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "def prob_today(df_day, base_df, ara_pct):\n",
    "    if df_day.empty or len(df_day) < 10: return 0.0, {}\n",
    "    o = float(df_day[\"Open\"].iloc[0]); c = float(df_day[\"Close\"].iloc[-1])\n",
    "    day_roc = (c/o - 1.0) if o>0 else 0.0\n",
    "    prox, dist = max(0.0, min(1.0, day_roc/ara_pct)), max(0.0, ara_pct - day_roc)\n",
    "    # momentum 15m\n",
    "    N = RECENT_MOM_MIN\n",
    "    if len(df_day) > N:\n",
    "        c_prev = float(df_day[\"Close\"].iloc[-(N+1)])\n",
    "        mom_ret = (c/c_prev - 1.0) if c_prev>0 else 0.0\n",
    "        per_min = mom_ret / N\n",
    "    else:\n",
    "        per_min = 0.0\n",
    "    mleft = minutes_to_close(df_day.index[-1])\n",
    "    coverage = (per_min*max(1,mleft))/max(1e-6, dist) if dist>0 else 1.0\n",
    "    coverage = max(0.0, min(1.0, coverage))\n",
    "    rvol30 = rvol_lastNmin(df_day, 30, base_df)\n",
    "    rvol30_n = max(0.0, min(3.0, (rvol30 if not np.isnan(rvol30) else 0.0)))/3.0\n",
    "    last = df_day.iloc[-1]\n",
    "    cr = closing_range_bar(last)\n",
    "    hh_before = float(df_day[\"High\"].iloc[:-1].max()) if len(df_day)>1 else float(last[\"High\"])\n",
    "    breakout = 1.0 if float(last[\"Close\"]) > float(hh_before) else 0.0\n",
    "    x = (1.6*prox) + (1.4*coverage) + (1.2*rvol30_n) + (1.0*cr) + (0.6*breakout) - 2.2\n",
    "    p = sigm(x)\n",
    "    feat = dict(day_roc=round(day_roc,4), prox=round(prox,3), coverage=round(coverage,3),\n",
    "                rvol30=round(rvol30 if rvol30==rvol30 else 0.0,2), cr=round(cr,3),\n",
    "                breakout=bool(breakout), mleft=int(mleft))\n",
    "    return float(p), feat\n",
    "\n",
    "def prob_tomorrow(df_day, base_df, ara_pct):\n",
    "    if df_day.empty or len(df_day) < 10: return 0.0, {}\n",
    "    o = float(df_day[\"Open\"].iloc[0]); c = float(df_day[\"Close\"].iloc[-1])\n",
    "    day_roc = (c/o - 1.0) if o>0 else 0.0\n",
    "    prox_t  = max(0.0, min(1.0, day_roc/ara_pct))\n",
    "    last = df_day.iloc[-1]\n",
    "    cr = closing_range_bar(last)\n",
    "    obv = obv_series(df_day)\n",
    "    obv_slope = float(obv.iloc[-1] - obv.iloc[-31]) if len(obv)>30 else 0.0\n",
    "    obv_up = 1.0 if obv_slope > 0 else 0.0\n",
    "    rvol60 = rvol_lastNmin(df_day, 60, base_df)\n",
    "    rvol60_n = max(0.0, min(3.0, (rvol60 if not np.isnan(rvol60) else 0.0)))/3.0\n",
    "    N = LAST_WINDOW_MIN\n",
    "    if len(df_day) > N:\n",
    "        roc_series = (df_day[\"Close\"]/o - 1.0)\n",
    "        prev_max = float(roc_series.iloc[:-N].max())\n",
    "        virgin = 1.0 if (day_roc >= VIRGIN_HINT and prev_max < VIRGIN_HINT) else 0.0\n",
    "    else:\n",
    "        virgin = 0.0\n",
    "    x = (1.5*prox_t) + (1.2*cr) + (1.0*rvol60_n) + (0.8*obv_up) + (0.6*virgin) - 1.8\n",
    "    p = sigm(x)\n",
    "    feat = dict(day_roc=round(day_roc,4), prox=round(prox_t,3), rvol60=round(rvol60 if rvol60==rvol60 else 0.0,2),\n",
    "                cr=round(cr,3), obv_up=bool(obv_up), virgin_hint=bool(virgin))\n",
    "    return float(p), feat\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "# ensure folders\n",
    "Path(EMITEN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_DAILY_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_1M_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_5M_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# roster\n",
    "active_csv = _latest_file(EMITEN_DIR, ACTIVE_PREFIX)\n",
    "tickers_all = _load_tickers(active_csv)[:MAX_TICKERS]\n",
    "print(f\"[Roster] {active_csv} | tickers={len(tickers_all)}\")\n",
    "\n",
    "# test koneksi\n",
    "if not test_yf():\n",
    "    raise SystemExit(\"Stop: koneksi yfinance bermasalah.\")\n",
    "\n",
    "rows_today, rows_tmr = [], []\n",
    "\n",
    "for i, t in enumerate(tickers_all, 1):\n",
    "    # 1) DAILY incremental (append-only)\n",
    "    d1d = get_daily_incremental(t)\n",
    "    if d1d.empty: \n",
    "        if i % 20 == 0: print(f\"â€¦ {i}/{len(tickers_all)} (daily empty: {t})\")\n",
    "        continue\n",
    "\n",
    "    # 2) 1m (merge 7d) / fallback 5m\n",
    "    df_all, gran = get_1m_merge(t)\n",
    "    if df_all.empty:\n",
    "        if i % 20 == 0: print(f\"â€¦ {i}/{len(tickers_all)} (intraday empty: {t})\")\n",
    "        continue\n",
    "\n",
    "    # hari ini\n",
    "    today_date = df_all.index.max().date()\n",
    "    df_today = df_all[df_all.index.date == today_date].copy()\n",
    "    if df_today.empty or len(df_today) < 10:\n",
    "        continue\n",
    "\n",
    "    last_close = float(df_today[\"Close\"].iloc[-1])\n",
    "    if last_close < MIN_PRICE:\n",
    "        continue\n",
    "\n",
    "    # daily context (prior multiplier)\n",
    "    reg_score, _reg = daily_regime_score(d1d)\n",
    "\n",
    "    # baseline intraday untuk rVol = hari2 sebelum hari ini\n",
    "    base_intra = df_all[df_all.index.date < today_date].copy()\n",
    "\n",
    "    # target ARA\n",
    "    target_pct = ara_limit_pct(last_close)\n",
    "\n",
    "    # probabilitas\n",
    "    p_today, feat_t = prob_today(df_today, base_intra, target_pct)\n",
    "    p_tmr,  feat_m  = prob_tomorrow(df_today, base_intra, target_pct)\n",
    "\n",
    "    # adjust by daily regime (penopang)\n",
    "    p_today_adj = min(1.0, p_today * (0.85 + 0.30*reg_score))\n",
    "    p_tmr_adj   = min(1.0, p_tmr   * (0.80 + 0.40*reg_score))\n",
    "\n",
    "    rows_today.append({\n",
    "        \"ticker\": t, \"gran\": gran, \"price\": round(last_close,2),\n",
    "        \"prob_ara_today\": round(100*p_today,1),\n",
    "        \"prob_ara_today_adj\": round(100*p_today_adj,1),\n",
    "        \"ara_target_pct\": round(100*target_pct,1),\n",
    "        **feat_t, \"regime_score\": round(reg_score,2)\n",
    "    })\n",
    "    rows_tmr.append({\n",
    "        \"ticker\": t, \"gran\": gran, \"price\": round(last_close,2),\n",
    "        \"prob_ara_tomorrow\": round(100*p_tmr,1),\n",
    "        \"prob_ara_tomorrow_adj\": round(100*p_tmr_adj,1),\n",
    "        \"ara_target_pct\": round(100*target_pct,1),\n",
    "        **feat_m, \"regime_score\": round(reg_score,2)\n",
    "    })\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"â€¦ processed {i}/{len(tickers_all)}\")\n",
    "    time.sleep(0.03)\n",
    "\n",
    "# ranking & save\n",
    "df_today = pd.DataFrame(rows_today).sort_values(\n",
    "    [\"prob_ara_today_adj\",\"prob_ara_today\",\"price\"],\n",
    "    ascending=[False, False, False]).reset_index(drop=True)\n",
    "df_tmr = pd.DataFrame(rows_tmr).sort_values(\n",
    "    [\"prob_ara_tomorrow_adj\",\"prob_ara_tomorrow\",\"price\"],\n",
    "    ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "ts_tag = _now_ts_tag()\n",
    "out_today = os.path.join(EMITEN_DIR, f\"prob_ara_today_{ts_tag}.csv\")\n",
    "out_tmr   = os.path.join(EMITEN_DIR, f\"prob_ara_tomorrow_{ts_tag}.csv\")\n",
    "\n",
    "if not df_today.empty:\n",
    "    df_today.to_csv(out_today, index=False)\n",
    "    print(f\"âœ… Saved â†’ {out_today}\")\n",
    "    display(df_today.head(25))\n",
    "else:\n",
    "    print(\"âš ï¸ Tidak ada kandidat 'prob ARA hari ini'.\")\n",
    "\n",
    "if not df_tmr.empty:\n",
    "    df_tmr.to_csv(out_tmr, index=False)\n",
    "    print(f\"âœ… Saved â†’ {out_tmr}\")\n",
    "    display(df_tmr.head(25))\n",
    "else:\n",
    "    print(\"âš ï¸ Tidak ada kandidat 'prob ARA besok'.\")\n",
    "# ============================================================================\n",
    "# ==== ARA PROB (DAILY INCREMENTAL + 1m, single cell) =========================\n",
    "# Output:\n",
    "#   emiten/prob_ara_today_YYYYMMDD_HHMM.csv\n",
    "#   emiten/prob_ara_tomorrow_YYYYMMDD_HHMM.csv\n",
    "#   emiten/cache_daily/<TICKER>.csv   (incremental append)\n",
    "#   emiten/cache_1m/<TICKER>.csv      (merge 7d)\n",
    "#   emiten/cache_5m/<TICKER>.csv      (fallback only, merge 12d)\n",
    "# ============================================================================\n",
    "!pip install -q yfinance pandas numpy\n",
    "\n",
    "import os, glob, time, math, warnings\n",
    "import numpy as np, pandas as pd, yfinance as yf\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, time as dtime\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "EMITEN_DIR           = \"emiten\"\n",
    "ACTIVE_PREFIX        = \"candidates_active_filtered_\"   # roster aktif harian\n",
    "MAX_TICKERS          = 700       # naikan sesuai kebutuhan (700 kalau mau full)\n",
    "\n",
    "# DAILY cache (append only)\n",
    "USE_CACHE_DAILY      = True\n",
    "CACHE_DAILY_DIR      = os.path.join(EMITEN_DIR, \"cache_daily\")\n",
    "DAILY_INIT_YEARS     = 5         # initial fetch if cache missing\n",
    "DAILY_WINDOW_DAYS    = 1500      # simpan ~6 thn; prune saat write\n",
    "\n",
    "# 1m cache (merge)\n",
    "USE_CACHE_1M         = True\n",
    "CACHE_1M_DIR         = os.path.join(EMITEN_DIR, \"cache_1m\")\n",
    "CACHE_1M_WINDOW_D    = 7         # Yahoo 1m â‰ˆ 7 hari\n",
    "\n",
    "# 5m fallback cache (merge)\n",
    "USE_CACHE_5M         = True\n",
    "CACHE_5M_DIR         = os.path.join(EMITEN_DIR, \"cache_5m\")\n",
    "CACHE_5M_WINDOW_D    = 12\n",
    "\n",
    "# ARA heuristik (boleh sesuaikan)\n",
    "def ara_limit_pct(price):\n",
    "    if price < 200: return 0.35\n",
    "    if price < 5000: return 0.25\n",
    "    return 0.20\n",
    "\n",
    "# Sinyal probabilistik (longgar)\n",
    "LAST_WINDOW_MIN      = 60   # menit akhir sesi\n",
    "RECENT_MOM_MIN       = 15   # momentum 15 menit\n",
    "VOL_SPIKE_MULT       = 1.6\n",
    "CR_MIN               = 0.70\n",
    "VIRGIN_HINT          = 0.06\n",
    "MIN_PRICE            = 50\n",
    "YF_TIMEOUT           = 45\n",
    "EXCHANGE_CLOSE_UTC   = (8, 0)  # 15:00 WIB = 08:00 UTC\n",
    "\n",
    "# =========================\n",
    "# UTIL & TZ\n",
    "# =========================\n",
    "def _now_date_tag(): return datetime.now().strftime(\"%Y%m%d\")\n",
    "def _now_ts_tag():   return datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "def _utc_naive_index(idx):\n",
    "    di = pd.to_datetime(idx, utc=True, errors=\"coerce\")\n",
    "    return pd.DatetimeIndex(di).tz_localize(None)\n",
    "\n",
    "def minutes_to_close(last_ts):\n",
    "    if not isinstance(last_ts, pd.Timestamp):\n",
    "        last_ts = pd.Timestamp(last_ts)\n",
    "    if getattr(last_ts, \"tz\", None) is not None:\n",
    "        last_ts = last_ts.tz_localize(None)\n",
    "    close_dt = datetime.combine(last_ts.date(), dtime(*EXCHANGE_CLOSE_UTC))\n",
    "    m = int((close_dt - last_ts).total_seconds() // 60)\n",
    "    return max(0, min(120, m))\n",
    "\n",
    "def _latest_file(folder, prefix):\n",
    "    today = os.path.join(folder, f\"{prefix}{_now_date_tag()}.csv\")\n",
    "    if os.path.exists(today): return today\n",
    "    cand = sorted(glob.glob(os.path.join(folder, f\"{prefix}*.csv\")))\n",
    "    if not cand: raise FileNotFoundError(f\"Tidak ada file {prefix}*.csv di {folder}\")\n",
    "    return cand[-1]\n",
    "\n",
    "def _load_tickers(path, col=\"ticker\"):\n",
    "    df = pd.read_csv(path)\n",
    "    cl = [c.lower() for c in df.columns]\n",
    "    assert col in cl, f\"Kolom '{col}' tidak ditemukan di {path}\"\n",
    "    s = df[df.columns[cl.index(col)]].astype(str).str.upper().str.strip()\n",
    "    return s.apply(lambda x: x if x.endswith(\".JK\") else f\"{x}.JK\").dropna().drop_duplicates().tolist()\n",
    "\n",
    "# =========================\n",
    "# TEST KONEKSI (harian & 1m)\n",
    "# =========================\n",
    "def test_yf():\n",
    "    try:\n",
    "        d = yf.download(\"TLKM.JK\", period=\"3mo\", interval=\"1d\", progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        m = yf.download(\"TLKM.JK\", period=\"7d\", interval=\"1m\",  progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        if d.empty or m.empty:\n",
    "            print(\"âš ï¸ Koneksi YF lemah (data kosong). Coba ulangi beberapa menit lagi.\")\n",
    "            return False\n",
    "        print(f\"âœ… Koneksi OK: daily={len(d)} bar, 1m={len(m)} bar\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gagal koneksi YF: {e}\")\n",
    "        return False\n",
    "\n",
    "# =========================\n",
    "# CACHE I/O GENERIK\n",
    "# =========================\n",
    "def _read_cache_csv(path):\n",
    "    if not os.path.exists(path): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.index = _utc_naive_index(df.index)\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _write_cache_csv(path, df, window_days):\n",
    "    if df.empty: return\n",
    "    dfx = df.copy()\n",
    "    dfx.index = _utc_naive_index(dfx.index)\n",
    "    cutoff = datetime.utcnow() - timedelta(days=window_days+1)\n",
    "    dfx = dfx[dfx.index >= cutoff]\n",
    "    dfx = dfx[~dfx.index.duplicated(keep=\"last\")].sort_index()\n",
    "    Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(path).write_text(dfx.to_csv())\n",
    "\n",
    "# =========================\n",
    "# FETCH DAILY (incremental append)\n",
    "# =========================\n",
    "def get_daily_incremental(t):\n",
    "    \"\"\"Append-only: jika cache ada â†’ unduh start=last_date+1, else initial fetch N years\"\"\"\n",
    "    path = os.path.join(CACHE_DAILY_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_DAILY else pd.DataFrame()\n",
    "\n",
    "    if base.empty:\n",
    "        fresh = yf.download(t, period=f\"{DAILY_INIT_YEARS}y\", interval=\"1d\",\n",
    "                            progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "        if fresh.empty: return pd.DataFrame()\n",
    "        fresh.index = _utc_naive_index(fresh.index)\n",
    "        if USE_CACHE_DAILY: _write_cache_csv(path, fresh, DAILY_WINDOW_DAYS)\n",
    "        return fresh\n",
    "\n",
    "    # incremental\n",
    "    last_dt = pd.to_datetime(base.index.max())\n",
    "    start = (last_dt + pd.Timedelta(days=1)).date().isoformat()\n",
    "    fresh = yf.download(t, start=start, interval=\"1d\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty:\n",
    "        return base\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_DAILY: _write_cache_csv(path, comb, DAILY_WINDOW_DAYS)\n",
    "    return comb\n",
    "\n",
    "# =========================\n",
    "# FETCH 1m (merge 7d) + fallback 5m\n",
    "# =========================\n",
    "def get_1m_merge(t):\n",
    "    path = os.path.join(CACHE_1M_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_1M else pd.DataFrame()\n",
    "    fresh = yf.download(t, period=\"7d\", interval=\"1m\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty and base.empty:\n",
    "        # fallback 5m\n",
    "        return get_5m_merge(t), \"5m\"\n",
    "    if fresh.empty:\n",
    "        return base, \"1m\"\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    if base.empty:\n",
    "        if USE_CACHE_1M: _write_cache_csv(path, fresh, CACHE_1M_WINDOW_D)\n",
    "        return fresh, \"1m\"\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_1M: _write_cache_csv(path, comb, CACHE_1M_WINDOW_D)\n",
    "    return comb, \"1m\"\n",
    "\n",
    "def get_5m_merge(t):\n",
    "    path = os.path.join(CACHE_5M_DIR, f\"{t}.csv\")\n",
    "    base = _read_cache_csv(path) if USE_CACHE_5M else pd.DataFrame()\n",
    "    fresh = yf.download(t, period=f\"{CACHE_5M_WINDOW_D}d\", interval=\"5m\",\n",
    "                        progress=False, threads=False, timeout=YF_TIMEOUT)\n",
    "    if fresh.empty and base.empty: return pd.DataFrame()\n",
    "    if fresh.empty: return base\n",
    "    fresh.index = _utc_naive_index(fresh.index)\n",
    "    if base.empty:\n",
    "        if USE_CACHE_5M: _write_cache_csv(path, fresh, CACHE_5M_WINDOW_D)\n",
    "        return fresh\n",
    "    comb = pd.concat([base, fresh]).sort_index()\n",
    "    comb = comb[~comb.index.duplicated(keep=\"last\")]\n",
    "    if USE_CACHE_5M: _write_cache_csv(path, comb, CACHE_5M_WINDOW_D)\n",
    "    return comb\n",
    "\n",
    "# =========================\n",
    "# FEATS: daily context & intraday micro\n",
    "# =========================\n",
    "def closing_range_bar(row):\n",
    "    high = float(row[\"High\"]); low = float(row[\"Low\"]); close = float(row[\"Close\"])\n",
    "    rng = max(1e-8, high - low)\n",
    "    return 1.0 - float((high - close)/rng)\n",
    "\n",
    "def obv_series(df):\n",
    "    delta = np.sign(df[\"Close\"].diff()).fillna(0.0)\n",
    "    return (delta * df[\"Volume\"].fillna(0)).cumsum()\n",
    "\n",
    "def daily_regime_score(d1d):\n",
    "    if d1d is None or d1d.empty or len(d1d) < 40:\n",
    "        return 0.5, {}\n",
    "    df = d1d.dropna().copy()\n",
    "    # EMA harian\n",
    "    df[\"EMA20\"] = df[\"Close\"].ewm(span=20, adjust=False).mean()\n",
    "    df[\"EMA50\"] = df[\"Close\"].ewm(span=50, adjust=False).mean()\n",
    "\n",
    "    # OBV harian (pastikan skalarnya)\n",
    "    delta = np.sign(df[\"Close\"].diff()).fillna(0.0)\n",
    "    obv = (delta * df[\"Volume\"].fillna(0)).cumsum()\n",
    "    if len(obv) > 6:\n",
    "        delta_obv = float(obv.iloc[-1]) - float(obv.iloc[-6])\n",
    "        obv_up = bool(delta_obv > 0.0)\n",
    "    else:\n",
    "        obv_up = False\n",
    "\n",
    "    # Last values as floats (hindari Series ambiguous)\n",
    "    last_close = float(df[\"Close\"].iloc[-1])\n",
    "    ema20_last = float(df[\"EMA20\"].iloc[-1])\n",
    "    ema50_last = float(df[\"EMA50\"].iloc[-1])\n",
    "\n",
    "    above_ema = bool((last_close > ema20_last) and (last_close > ema50_last))\n",
    "\n",
    "    if len(df) > 20:\n",
    "        hh20 = float(df[\"High\"].rolling(20).max().shift(1).iloc[-1])\n",
    "        breakout20 = bool(last_close > hh20)\n",
    "    else:\n",
    "        breakout20 = False\n",
    "\n",
    "    # rVol daily 20\n",
    "    vol_med20 = float(df[\"Volume\"].rolling(20).median().iloc[-1]) if len(df) >= 20 else 0.0\n",
    "    rvol_d20 = (float(df[\"Volume\"].iloc[-1]) / vol_med20) if vol_med20 > 0 else 1.0\n",
    "\n",
    "    # Skor 0..1 (penopang)\n",
    "    sc = 0.0\n",
    "    sc += 0.28 if above_ema   else 0.0\n",
    "    sc += 0.26 if breakout20  else 0.0\n",
    "    sc += 0.24 * min(1.5, rvol_d20) / 1.5\n",
    "    sc += 0.22 if obv_up      else 0.0\n",
    "    sc = float(max(0.0, min(1.0, sc)))\n",
    "\n",
    "    return sc, {\n",
    "        \"above_ema\": above_ema,\n",
    "        \"breakout20\": breakout20,\n",
    "        \"rvol_d20\": round(rvol_d20, 2)\n",
    "    }\n",
    "\n",
    "def rvol_lastNmin(df_today, N, base_df):\n",
    "    if df_today.empty or base_df.empty: return np.nan\n",
    "    recent_vol = float(df_today[\"Volume\"].tail(max(1,N)).sum())\n",
    "    b = base_df.copy()\n",
    "    b[\"date\"] = b.index.date\n",
    "    buckets = [float(g[\"Volume\"].tail(max(1,N)).sum()) for _, g in b.groupby(\"date\")]\n",
    "    if len(buckets) < 2: return np.nan\n",
    "    med = float(np.median(buckets))\n",
    "    return (recent_vol / med) if med > 0 else np.nan\n",
    "\n",
    "def sigm(x): return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "def prob_today(df_day, base_df, ara_pct):\n",
    "    if df_day.empty or len(df_day) < 10: return 0.0, {}\n",
    "    o = float(df_day[\"Open\"].iloc[0]); c = float(df_day[\"Close\"].iloc[-1])\n",
    "    day_roc = (c/o - 1.0) if o>0 else 0.0\n",
    "    prox, dist = max(0.0, min(1.0, day_roc/ara_pct)), max(0.0, ara_pct - day_roc)\n",
    "    # momentum 15m\n",
    "    N = RECENT_MOM_MIN\n",
    "    if len(df_day) > N:\n",
    "        c_prev = float(df_day[\"Close\"].iloc[-(N+1)])\n",
    "        mom_ret = (c/c_prev - 1.0) if c_prev>0 else 0.0\n",
    "        per_min = mom_ret / N\n",
    "    else:\n",
    "        per_min = 0.0\n",
    "    mleft = minutes_to_close(df_day.index[-1])\n",
    "    coverage = (per_min*max(1,mleft))/max(1e-6, dist) if dist>0 else 1.0\n",
    "    coverage = max(0.0, min(1.0, coverage))\n",
    "    rvol30 = rvol_lastNmin(df_day, 30, base_df)\n",
    "    rvol30_n = max(0.0, min(3.0, (rvol30 if not np.isnan(rvol30) else 0.0)))/3.0\n",
    "    last = df_day.iloc[-1]\n",
    "    cr = closing_range_bar(last)\n",
    "    hh_before = float(df_day[\"High\"].iloc[:-1].max()) if len(df_day)>1 else float(last[\"High\"])\n",
    "    breakout = 1.0 if float(last[\"Close\"]) > float(hh_before) else 0.0\n",
    "    x = (1.6*prox) + (1.4*coverage) + (1.2*rvol30_n) + (1.0*cr) + (0.6*breakout) - 2.2\n",
    "    p = sigm(x)\n",
    "    feat = dict(day_roc=round(day_roc,4), prox=round(prox,3), coverage=round(coverage,3),\n",
    "                rvol30=round(rvol30 if rvol30==rvol30 else 0.0,2), cr=round(cr,3),\n",
    "                breakout=bool(breakout), mleft=int(mleft))\n",
    "    return float(p), feat\n",
    "\n",
    "def prob_tomorrow(df_day, base_df, ara_pct):\n",
    "    if df_day.empty or len(df_day) < 10: return 0.0, {}\n",
    "    o = float(df_day[\"Open\"].iloc[0]); c = float(df_day[\"Close\"].iloc[-1])\n",
    "    day_roc = (c/o - 1.0) if o>0 else 0.0\n",
    "    prox_t  = max(0.0, min(1.0, day_roc/ara_pct))\n",
    "    last = df_day.iloc[-1]\n",
    "    cr = closing_range_bar(last)\n",
    "    obv = obv_series(df_day)\n",
    "    obv_slope = float(obv.iloc[-1] - obv.iloc[-31]) if len(obv)>30 else 0.0\n",
    "    obv_up = 1.0 if obv_slope > 0 else 0.0\n",
    "    rvol60 = rvol_lastNmin(df_day, 60, base_df)\n",
    "    rvol60_n = max(0.0, min(3.0, (rvol60 if not np.isnan(rvol60) else 0.0)))/3.0\n",
    "    N = LAST_WINDOW_MIN\n",
    "    if len(df_day) > N:\n",
    "        roc_series = (df_day[\"Close\"]/o - 1.0)\n",
    "        prev_max = float(roc_series.iloc[:-N].max())\n",
    "        virgin = 1.0 if (day_roc >= VIRGIN_HINT and prev_max < VIRGIN_HINT) else 0.0\n",
    "    else:\n",
    "        virgin = 0.0\n",
    "    x = (1.5*prox_t) + (1.2*cr) + (1.0*rvol60_n) + (0.8*obv_up) + (0.6*virgin) - 1.8\n",
    "    p = sigm(x)\n",
    "    feat = dict(day_roc=round(day_roc,4), prox=round(prox_t,3), rvol60=round(rvol60 if rvol60==rvol60 else 0.0,2),\n",
    "                cr=round(cr,3), obv_up=bool(obv_up), virgin_hint=bool(virgin))\n",
    "    return float(p), feat\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "# ensure folders\n",
    "Path(EMITEN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_DAILY_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_1M_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(CACHE_5M_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# roster\n",
    "active_csv = _latest_file(EMITEN_DIR, ACTIVE_PREFIX)\n",
    "tickers_all = _load_tickers(active_csv)[:MAX_TICKERS]\n",
    "print(f\"[Roster] {active_csv} | tickers={len(tickers_all)}\")\n",
    "\n",
    "# test koneksi\n",
    "if not test_yf():\n",
    "    raise SystemExit(\"Stop: koneksi yfinance bermasalah.\")\n",
    "\n",
    "rows_today, rows_tmr = [], []\n",
    "\n",
    "for i, t in enumerate(tickers_all, 1):\n",
    "    # 1) DAILY incremental (append-only)\n",
    "    d1d = get_daily_incremental(t)\n",
    "    if d1d.empty: \n",
    "        if i % 20 == 0: print(f\"â€¦ {i}/{len(tickers_all)} (daily empty: {t})\")\n",
    "        continue\n",
    "\n",
    "    # 2) 1m (merge 7d) / fallback 5m\n",
    "    df_all, gran = get_1m_merge(t)\n",
    "    if df_all.empty:\n",
    "        if i % 20 == 0: print(f\"â€¦ {i}/{len(tickers_all)} (intraday empty: {t})\")\n",
    "        continue\n",
    "\n",
    "    # hari ini\n",
    "    today_date = df_all.index.max().date()\n",
    "    df_today = df_all[df_all.index.date == today_date].copy()\n",
    "    if df_today.empty or len(df_today) < 10:\n",
    "        continue\n",
    "\n",
    "    last_close = float(df_today[\"Close\"].iloc[-1])\n",
    "    if last_close < MIN_PRICE:\n",
    "        continue\n",
    "\n",
    "    # daily context (prior multiplier)\n",
    "    reg_score, _reg = daily_regime_score(d1d)\n",
    "\n",
    "    # baseline intraday untuk rVol = hari2 sebelum hari ini\n",
    "    base_intra = df_all[df_all.index.date < today_date].copy()\n",
    "\n",
    "    # target ARA\n",
    "    target_pct = ara_limit_pct(last_close)\n",
    "\n",
    "    # probabilitas\n",
    "    p_today, feat_t = prob_today(df_today, base_intra, target_pct)\n",
    "    p_tmr,  feat_m  = prob_tomorrow(df_today, base_intra, target_pct)\n",
    "\n",
    "    # adjust by daily regime (penopang)\n",
    "    p_today_adj = min(1.0, p_today * (0.85 + 0.30*reg_score))\n",
    "    p_tmr_adj   = min(1.0, p_tmr   * (0.80 + 0.40*reg_score))\n",
    "\n",
    "    rows_today.append({\n",
    "        \"ticker\": t, \"gran\": gran, \"price\": round(last_close,2),\n",
    "        \"prob_ara_today\": round(100*p_today,1),\n",
    "        \"prob_ara_today_adj\": round(100*p_today_adj,1),\n",
    "        \"ara_target_pct\": round(100*target_pct,1),\n",
    "        **feat_t, \"regime_score\": round(reg_score,2)\n",
    "    })\n",
    "    rows_tmr.append({\n",
    "        \"ticker\": t, \"gran\": gran, \"price\": round(last_close,2),\n",
    "        \"prob_ara_tomorrow\": round(100*p_tmr,1),\n",
    "        \"prob_ara_tomorrow_adj\": round(100*p_tmr_adj,1),\n",
    "        \"ara_target_pct\": round(100*target_pct,1),\n",
    "        **feat_m, \"regime_score\": round(reg_score,2)\n",
    "    })\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f\"â€¦ processed {i}/{len(tickers_all)}\")\n",
    "    time.sleep(0.03)\n",
    "\n",
    "# ranking & save\n",
    "df_today = pd.DataFrame(rows_today).sort_values(\n",
    "    [\"prob_ara_today_adj\",\"prob_ara_today\",\"price\"],\n",
    "    ascending=[False, False, False]).reset_index(drop=True)\n",
    "df_tmr = pd.DataFrame(rows_tmr).sort_values(\n",
    "    [\"prob_ara_tomorrow_adj\",\"prob_ara_tomorrow\",\"price\"],\n",
    "    ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "ts_tag = _now_ts_tag()\n",
    "out_today = os.path.join(EMITEN_DIR, f\"prob_ara_today_{ts_tag}.csv\")\n",
    "out_tmr   = os.path.join(EMITEN_DIR, f\"prob_ara_tomorrow_{ts_tag}.csv\")\n",
    "\n",
    "if not df_today.empty:\n",
    "    df_today.to_csv(out_today, index=False)\n",
    "    print(f\"âœ… Saved â†’ {out_today}\")\n",
    "    display(df_today.head(25))\n",
    "else:\n",
    "    print(\"âš ï¸ Tidak ada kandidat 'prob ARA hari ini'.\")\n",
    "\n",
    "if not df_tmr.empty:\n",
    "    df_tmr.to_csv(out_tmr, index=False)\n",
    "    print(f\"âœ… Saved â†’ {out_tmr}\")\n",
    "    display(df_tmr.head(25))\n",
    "else:\n",
    "    print(\"âš ï¸ Tidak ada kandidat 'prob ARA besok'.\")\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c01452",
   "metadata": {},
   "source": [
    "## PATCH MERGE FOLDER INTO LEANER DAILY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-CELL: Merge daily caches (legacy) â†’ emiten/cache_daily/\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# ==== CONFIG ====\n",
    "SRC_DIRS = [\n",
    "    Path(\"emiten/cache_60d\"),\n",
    "    Path(\"emiten/cache_100d\"),\n",
    "    Path(\"emiten/cache_6mo\"),\n",
    "]\n",
    "DEST_DIR = Path(\"emiten/cache_daily\")\n",
    "\n",
    "BACKUP_DEST_BEFORE = False   # True â†’ backup folder cache_daily sebelum merge\n",
    "DELETE_SOURCE_DIRS_AFTER = False  # True â†’ hapus folder sumber setelah merge sukses\n",
    "\n",
    "# ==== HELPERS ====\n",
    "def _read_daily_csv(p: Path) -> pd.DataFrame:\n",
    "    \"\"\"Baca CSV harian (OHLCV); index = Date (harian), dedupe & sort.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(p, parse_dates=[\"Date\"]).set_index(\"Date\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(p, parse_dates=[0], index_col=0)\n",
    "\n",
    "    # pastikan datetime index harian (tz-naive)\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    if df.index.tz is not None:\n",
    "        df.index = df.index.tz_convert(None)\n",
    "    df.index = df.index.normalize()\n",
    "\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in df.columns]\n",
    "    if not keep:\n",
    "        # kalau kolomnya aneh, simpan semuaâ€”nanti tetap dedupe per tanggal\n",
    "        keep = list(df.columns)\n",
    "    df = df[keep].sort_index()\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    return df\n",
    "\n",
    "def _safe_backup(dest: Path):\n",
    "    if not BACKUP_DEST_BEFORE or not dest.exists():\n",
    "        return None\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup = dest.parent / f\"{dest.name}_backup_{ts}\"\n",
    "    if backup.exists():\n",
    "        shutil.rmtree(backup, ignore_errors=True)\n",
    "    shutil.copytree(dest, backup)\n",
    "    return backup\n",
    "\n",
    "# ==== MAIN ====\n",
    "def merge_legacy_daily_to_cache_daily():\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    backup_dir = _safe_backup(DEST_DIR)\n",
    "\n",
    "    from collections import defaultdict\n",
    "    by_ticker = defaultdict(list)\n",
    "    total_source_files = 0\n",
    "\n",
    "    # Kumpulkan semua CSV sumber\n",
    "    for src in SRC_DIRS:\n",
    "        if not src.exists():\n",
    "            continue\n",
    "        csvs = list(src.glob(\"*.csv\"))\n",
    "        total_source_files += len(csvs)\n",
    "        for f in csvs:\n",
    "            by_ticker[f.stem].append(f)\n",
    "\n",
    "    tickers_updated = 0\n",
    "    rows_added_total = 0\n",
    "\n",
    "    for tkr, files in sorted(by_ticker.items()):\n",
    "        dest_file = DEST_DIR / f\"{tkr}.csv\"\n",
    "\n",
    "        # baca existing dest (jika ada)\n",
    "        if dest_file.exists():\n",
    "            try:\n",
    "                dest = _read_daily_csv(dest_file)\n",
    "            except Exception:\n",
    "                dest = pd.DataFrame()\n",
    "        else:\n",
    "            dest = pd.DataFrame()\n",
    "\n",
    "        before = len(dest)\n",
    "\n",
    "        # merge semua sumber utk ticker ini\n",
    "        for f in files:\n",
    "            try:\n",
    "                src_df = _read_daily_csv(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if dest.empty:\n",
    "                dest = src_df\n",
    "            else:\n",
    "                dest = pd.concat([dest, src_df]).sort_index()\n",
    "                dest = dest[~dest.index.duplicated(keep=\"last\")]\n",
    "\n",
    "        after = len(dest)\n",
    "        if after > before:\n",
    "            out = dest.copy()\n",
    "            out.index.name = \"Date\"\n",
    "            out.to_csv(dest_file)\n",
    "            rows_added_total += (after - before)\n",
    "            tickers_updated += 1\n",
    "\n",
    "    # Opsional: hapus folder sumber\n",
    "    if DELETE_SOURCE_DIRS_AFTER:\n",
    "        for src in SRC_DIRS:\n",
    "            if src.exists():\n",
    "                shutil.rmtree(src, ignore_errors=True)\n",
    "\n",
    "    print(\"âœ… Merge selesai\")\n",
    "    print(f\"  Source CSV total : {total_source_files}\")\n",
    "    print(f\"  Tickers updated  : {tickers_updated}\")\n",
    "    print(f\"  Rows added       : {rows_added_total}\")\n",
    "    if backup_dir:\n",
    "        print(f\"  Backup created   : {backup_dir}\")\n",
    "\n",
    "merge_legacy_daily_to_cache_daily()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99631d4",
   "metadata": {},
   "source": [
    "## BSJP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872744df",
   "metadata": {},
   "source": [
    "### v.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== BSJP v.2.0 â€” STRICT OFFLINE RUNNER =====================\n",
    "# Input  : emiten/cache_daily/, emiten/cache_1m/, emiten/cache_5m/, roster aktif\n",
    "# Output : result/prob_ara_today_YYYYMMDD.csv\n",
    "#          result/prob_ara_tomorrow_YYYYMMDD.csv\n",
    "# Note   : TANPA fetch internet; seluruh data dari cache warm-up service.\n",
    "\n",
    "import os, glob, math, warnings, time\n",
    "import numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, time as dtime\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "EMITEN_DIR      = \"emiten\"\n",
    "RESULTS_DIR     = \"result\"                     # sejajar dgn emiten/\n",
    "ACTIVE_PREFIX   = \"candidates_active_filtered_\"\n",
    "MAX_TICKERS     = 0                            # 0 = no cap\n",
    "MIN_PRICE       = 50\n",
    "\n",
    "# Cache paths & freshness guards\n",
    "CACHE_DAILY_DIR = os.path.join(EMITEN_DIR, \"cache_daily\")\n",
    "CACHE_1M_DIR    = os.path.join(EMITEN_DIR, \"cache_1m\")\n",
    "CACHE_5M_DIR    = os.path.join(EMITEN_DIR, \"cache_5m\")\n",
    "CACHE_STALE_M_1M= 15                           # menit; hanya untuk warning (tidak fail)\n",
    "DAILY_MIN_BARS  = 40\n",
    "\n",
    "# Intraday windows (minute)\n",
    "WIN_MOM_RECENT  = 15\n",
    "WIN_RVOL_30M    = 30\n",
    "WIN_RVOL_60M    = 60\n",
    "\n",
    "# Exchange anchors (yfinance intraday UTC-naive)\n",
    "CLOSE_UTC       = (8, 0)   # 15:00 WIB = 08:00 UTC\n",
    "\n",
    "# Logit weights (longgar)\n",
    "B_TODAY  = dict(prox=1.6, coverage=1.4, rvol30=1.2, cr=1.0, breakout=0.6, bias=-2.2)\n",
    "B_TMR    = dict(prox=1.5, cr=1.2, rvol60=1.0, obv=0.8, virgin=0.6, bias=-1.8)\n",
    "\n",
    "# Prior harian (regime) multipliers â†’ p_adj\n",
    "REG_TODAY_M = (0.85, 0.30)   # base, slope\n",
    "REG_TMR_M   = (0.80, 0.40)\n",
    "\n",
    "# ------------------- UTILS --------------------\n",
    "def _now_date():      return datetime.now().strftime(\"%Y%m%d\")\n",
    "def _latest_file(folder, prefix):\n",
    "    today = os.path.join(folder, f\"{prefix}{_now_date()}.csv\")\n",
    "    if os.path.exists(today): return today\n",
    "    cand = sorted(glob.glob(os.path.join(folder, f\"{prefix}*.csv\")))\n",
    "    if not cand: raise FileNotFoundError(f\"Tidak ada {prefix}*.csv di {folder}\")\n",
    "    return cand[-1]\n",
    "\n",
    "def _load_roster(path):\n",
    "    df = pd.read_csv(path)\n",
    "    cl = [c.lower() for c in df.columns]\n",
    "    assert \"ticker\" in cl, \"CSV roster butuh kolom 'ticker'\"\n",
    "    s = df[df.columns[cl.index(\"ticker\")]].astype(str).str.upper().str.strip()\n",
    "    return s.apply(lambda x: x if x.endswith(\".JK\") else f\"{x}.JK\").dropna().drop_duplicates().tolist()\n",
    "\n",
    "def _utc_naive_index(idx):\n",
    "    di = pd.to_datetime(idx, utc=True, errors=\"coerce\")\n",
    "    return pd.DatetimeIndex(di).tz_localize(None)\n",
    "\n",
    "def _read_cache_csv(path):\n",
    "    if not os.path.exists(path): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.index = _utc_naive_index(df.index)\n",
    "        # keep essential cols only\n",
    "        keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"] if c in df.columns]\n",
    "        df = df[keep].dropna(subset=[c for c in [\"High\",\"Low\",\"Close\"] if c in keep]).sort_index()\n",
    "        return df[~df.index.duplicated(keep=\"last\")]\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def ara_limit_pct(p):\n",
    "    if p < 200:   return 0.35\n",
    "    if p < 5000:  return 0.25\n",
    "    return 0.20\n",
    "\n",
    "def sigmoid(x):  return 1.0/(1.0+math.exp(-x))\n",
    "\n",
    "def minutes_to_close(ts):\n",
    "    ts = pd.Timestamp(ts)\n",
    "    if getattr(ts, \"tz\", None) is not None: ts = ts.tz_localize(None)\n",
    "    tgt = datetime.combine(ts.date(), dtime(*CLOSE_UTC))\n",
    "    m = int((tgt - ts).total_seconds() // 60)\n",
    "    return max(0, min(180, m))\n",
    "\n",
    "def closing_range(row):  # 0..1 (near high)\n",
    "    rng = max(1e-9, float(row[\"High\"] - row[\"Low\"]))\n",
    "    return 1.0 - float((row[\"High\"] - row[\"Close\"]) / rng)\n",
    "\n",
    "def obv_series(df):\n",
    "    delta = np.sign(df[\"Close\"].diff()).fillna(0.0)\n",
    "    return (delta * df[\"Volume\"].fillna(0.0)).cumsum()\n",
    "\n",
    "def regime_daily(df_daily):\n",
    "    if df_daily is None or df_daily.empty or len(df_daily)<DAILY_MIN_BARS: return 0.5\n",
    "    d = df_daily.copy()\n",
    "    d[\"EMA20\"] = d[\"Close\"].ewm(span=20).mean()\n",
    "    d[\"EMA50\"] = d[\"Close\"].ewm(span=50).mean()\n",
    "    above_ema  = 1.0 if (d[\"Close\"].iloc[-1] > d[\"EMA20\"].iloc[-1] and d[\"Close\"].iloc[-1] > d[\"EMA50\"].iloc[-1]) else 0.0\n",
    "    breakout20 = 1.0 if (len(d)>20 and d[\"Close\"].iloc[-1] > d[\"High\"].rolling(20).max().shift(1).iloc[-1]) else 0.0\n",
    "    vol_med20  = d[\"Volume\"].rolling(20).median().iloc[-1] if len(d)>=20 else np.nan\n",
    "    rvol_d20   = (d[\"Volume\"].iloc[-1]/vol_med20) if (isinstance(vol_med20,(int,float)) and vol_med20>0) else 1.0\n",
    "    rvol_d20   = min(1.5, max(0.0, rvol_d20))\n",
    "    obv        = obv_series(d)\n",
    "    obv_up     = 1.0 if (len(obv)>6 and (obv.iloc[-1]-obv.iloc[-6])>0) else 0.0\n",
    "    sc = 0.28*above_ema + 0.26*breakout20 + 0.24*(rvol_d20/1.5) + 0.22*obv_up\n",
    "    return float(max(0.0, min(1.0, sc)))\n",
    "\n",
    "def rvol_lastNbars(df_today, N_bars, df_all_prev_days):\n",
    "    if df_today.empty or df_all_prev_days.empty: return np.nan\n",
    "    recent = float(df_today[\"Volume\"].tail(max(1,N_bars)).sum())\n",
    "    b = df_all_prev_days.copy(); b[\"date\"] = b.index.date\n",
    "    buckets = [float(g[\"Volume\"].tail(max(1,N_bars)).sum()) for _, g in b.groupby(\"date\")]\n",
    "    if len(buckets)<2: return np.nan\n",
    "    med = float(np.median(buckets))\n",
    "    if med<=0: return np.nan\n",
    "    return recent/med\n",
    "\n",
    "# ----------------- PROBS & CONFIDENCE -----------------\n",
    "def compute_probs_bsJP(df_all, gran_minutes, df_daily):\n",
    "    # split hari ini vs baseline\n",
    "    today_date = df_all.index.max().date()\n",
    "    df_today   = df_all[df_all.index.date == today_date].copy()\n",
    "    base_intra = df_all[df_all.index.date < today_date].copy()\n",
    "    if df_today.empty or len(df_today)<(10 if gran_minutes==1 else 3):\n",
    "        return None  # not enough\n",
    "\n",
    "    reg = regime_daily(df_daily)\n",
    "\n",
    "    open0 = float(df_today[\"Open\"].iloc[0])\n",
    "    last  = df_today.iloc[-1]\n",
    "    close = float(last[\"Close\"])\n",
    "    if close < MIN_PRICE: return None\n",
    "\n",
    "    day_roc = (close/open0 - 1.0) if open0>0 else 0.0\n",
    "    ara_pct = ara_limit_pct(close)\n",
    "    prox    = max(0.0, min(1.0, day_roc/ara_pct))\n",
    "    dist    = max(0.0, ara_pct - day_roc)\n",
    "\n",
    "    # momentum & coverage\n",
    "    N_rec   = max(1, int(WIN_MOM_RECENT/gran_minutes))\n",
    "    if len(df_today) > N_rec:\n",
    "        c_prev  = float(df_today[\"Close\"].iloc[-(N_rec+1)])\n",
    "        mom_ret = (close/c_prev - 1.0) if c_prev>0 else 0.0\n",
    "        per_min = mom_ret / (N_rec*gran_minutes)\n",
    "    else:\n",
    "        per_min = 0.0\n",
    "    mleft   = minutes_to_close(df_today.index[-1])\n",
    "    coverage= 1.0 if dist==0 else max(0.0, min(1.0, (per_min*max(1,mleft))/dist))\n",
    "\n",
    "    # rVol\n",
    "    N30     = max(1, int(WIN_RVOL_30M/gran_minutes))\n",
    "    N60     = max(1, int(WIN_RVOL_60M/gran_minutes))\n",
    "    rvol30  = rvol_lastNbars(df_today, N30, base_intra)\n",
    "    rvol60  = rvol_lastNbars(df_today, N60, base_intra)\n",
    "    r30n    = min(3.0, max(0.0, rvol30 if not np.isnan(rvol30) else 0.0))/3.0\n",
    "    r60n    = min(3.0, max(0.0, rvol60 if not np.isnan(rvol60) else 0.0))/3.0\n",
    "\n",
    "    # tape\n",
    "    cr      = closing_range(last)\n",
    "    hh_before = float(df_today[\"High\"].iloc[:-1].max()) if len(df_today)>1 else float(last[\"High\"])\n",
    "    breakout  = 1.0 if close > hh_before else 0.0\n",
    "\n",
    "    # OBV micro + virgin hint (untuk besok)\n",
    "    obv     = obv_series(df_today)\n",
    "    obv_up  = 1.0 if (len(obv)>N30 and (obv.iloc[-1]-obv.iloc[-N30])>0) else 0.0\n",
    "    roc_series = (df_today[\"Close\"]/open0 - 1.0)\n",
    "    prev_max   = float(roc_series.iloc[:-N60].max()) if len(roc_series)>N60 else -1.0\n",
    "    virgin     = 1.0 if (day_roc>=0.06 and prev_max<0.06) else 0.0\n",
    "\n",
    "    # logits â†’ probs\n",
    "    x_today = (B_TODAY[\"prox\"]*prox + B_TODAY[\"coverage\"]*coverage +\n",
    "               B_TODAY[\"rvol30\"]*r30n + B_TODAY[\"cr\"]*cr +\n",
    "               B_TODAY[\"breakout\"]*breakout + B_TODAY[\"bias\"])\n",
    "    p_today = sigmoid(x_today)\n",
    "    mul_t   = REG_TODAY_M[0] + REG_TODAY_M[1]*reg\n",
    "    p_today_adj = min(1.0, p_today * mul_t)\n",
    "\n",
    "    x_tmr = (B_TMR[\"prox\"]*prox + B_TMR[\"cr\"]*cr + B_TMR[\"rvol60\"]*r60n +\n",
    "             B_TMR[\"obv\"]*obv_up + B_TMR[\"virgin\"]*virgin + B_TMR[\"bias\"])\n",
    "    p_tmr = sigmoid(x_tmr)\n",
    "    mul_m = REG_TMR_M[0] + REG_TMR_M[1]*reg\n",
    "    p_tmr_adj = min(1.0, p_tmr * mul_m)\n",
    "\n",
    "    # -------- confidence (0..1) & band --------\n",
    "    # data_quality: gran=1m, bar cukup, fresh <15m, ada baseline\n",
    "    age_mins = (datetime.utcnow() - df_all.index.max()).total_seconds()/60.0\n",
    "    data_quality = 0.0\n",
    "    data_quality += 0.4 if gran_minutes==1 else 0.2\n",
    "    data_quality += 0.3 if len(df_today) >= (30 if gran_minutes==1 else 6) else 0.1\n",
    "    data_quality += 0.2 if age_mins <= CACHE_STALE_M_1M else 0.0\n",
    "    data_quality += 0.1 if not base_intra.empty else 0.0\n",
    "\n",
    "    # signal_quality: rVol kuat, CR tinggi, breakout valid\n",
    "    signal_quality = 0.0\n",
    "    signal_quality += 0.35 if r30n >= 0.5 else (0.15 if r30n >= 0.25 else 0.0)\n",
    "    signal_quality += 0.25 if r60n >= 0.5 else (0.10 if r60n >= 0.25 else 0.0)\n",
    "    signal_quality += 0.25 if cr >= 0.6  else 0.10 if cr >= 0.5 else 0.0\n",
    "    signal_quality += 0.15 if breakout==1.0 else 0.0\n",
    "\n",
    "    confidence = 0.3*data_quality + 0.4*signal_quality + 0.3*reg\n",
    "    confidence = float(max(0.0, min(1.0, confidence)))\n",
    "    band = \"Low\" if confidence < 0.45 else (\"Med\" if confidence < 0.7 else \"High\")\n",
    "\n",
    "    return {\n",
    "        # meta\n",
    "        \"gran\": f\"{gran_minutes}m\",\n",
    "        \"price\": round(close,2),\n",
    "        # today\n",
    "        \"day_roc\": round(day_roc,4),\n",
    "        \"ara_target_pct\": round(100*ara_pct,1),\n",
    "        \"prox\": round(prox,3),\n",
    "        \"coverage\": round(coverage,3),\n",
    "        \"rvol30\": round(rvol30 if not np.isnan(rvol30) else 0.0,2),\n",
    "        \"cr\": round(cr,3),\n",
    "        \"breakout\": bool(breakout),\n",
    "        \"mleft\": int(mleft),\n",
    "        # tmr\n",
    "        \"rvol60\": round(rvol60 if not np.isnan(rvol60) else 0.0,2),\n",
    "        \"obv_up\": bool(obv_up),\n",
    "        \"virgin_hint\": bool(virgin),\n",
    "        # probs\n",
    "        \"p_today\": round(100*p_today,1),\n",
    "        \"p_today_adj\": round(100*p_today_adj,1),\n",
    "        \"p_tmr\": round(100*p_tmr,1),\n",
    "        \"p_tmr_adj\": round(100*p_tmr_adj,1),\n",
    "        # regime & confidence\n",
    "        \"regime_score\": round(reg,2),\n",
    "        \"confidence\": round(confidence,2),\n",
    "        \"confidence_band\": band\n",
    "    }\n",
    "\n",
    "# ------------------- RUN ----------------------\n",
    "# ensure folder\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "for p in [EMITEN_DIR, CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# roster\n",
    "roster_csv = _latest_file(EMITEN_DIR, ACTIVE_PREFIX)\n",
    "tickers    = _load_roster(roster_csv)\n",
    "tickers    = tickers if (MAX_TICKERS in [0, None]) else tickers[:MAX_TICKERS]\n",
    "print(f\"[BSJP v2.0] Roster={len(tickers)} | {os.path.basename(roster_csv)}\")\n",
    "print(\"â†ªï¸  Mode: STRICT OFFLINE (baca cache harian & 1m/5m)\")\n",
    "\n",
    "rows_today, rows_tmr = [], []\n",
    "cnt_ok=cnt_skip_daily=cnt_skip_intraday=0\n",
    "\n",
    "for i, t in enumerate(tickers, 1):\n",
    "    # DAILY (from cache only)\n",
    "    d1d = _read_cache_csv(os.path.join(CACHE_DAILY_DIR, f\"{t}.csv\"))\n",
    "    reg = regime_daily(d1d)\n",
    "\n",
    "    # INTRADAY (1m â†’ fallback 5m)\n",
    "    df = _read_cache_csv(os.path.join(CACHE_1M_DIR, f\"{t}.csv\"))\n",
    "    gran = 1\n",
    "    if df.empty:\n",
    "        df = _read_cache_csv(os.path.join(CACHE_5M_DIR, f\"{t}.csv\"))\n",
    "        gran = 5\n",
    "    if df.empty:\n",
    "        cnt_skip_intraday += 1\n",
    "        if i % 50 == 0: print(f\"â€¦ {i}/{len(tickers)} (intraday missing: {t})\")\n",
    "        continue\n",
    "\n",
    "    feats = compute_probs_bsJP(df, gran, d1d)\n",
    "    if feats is None:\n",
    "        cnt_skip_intraday += 1\n",
    "        continue\n",
    "\n",
    "    # split ke 2 tabel output\n",
    "    rec_today = {\n",
    "        \"ticker\": t,\n",
    "        \"gran\": feats[\"gran\"],\n",
    "        \"price\": feats[\"price\"],\n",
    "        \"prob_ara_today\": feats[\"p_today\"],\n",
    "        \"prob_ara_today_adj\": feats[\"p_today_adj\"],\n",
    "        \"ara_target_pct\": feats[\"ara_target_pct\"],\n",
    "        \"day_roc\": feats[\"day_roc\"],\n",
    "        \"prox\": feats[\"prox\"],\n",
    "        \"coverage\": feats[\"coverage\"],\n",
    "        \"rvol30\": feats[\"rvol30\"],\n",
    "        \"cr\": feats[\"cr\"],\n",
    "        \"breakout\": feats[\"breakout\"],\n",
    "        \"mleft\": feats[\"mleft\"],\n",
    "        \"regime_score\": feats[\"regime_score\"],\n",
    "        \"confidence\": feats[\"confidence\"],\n",
    "        \"confidence_band\": feats[\"confidence_band\"],\n",
    "    }\n",
    "    rows_today.append(rec_today)\n",
    "\n",
    "    rec_tmr = {\n",
    "        \"ticker\": t,\n",
    "        \"gran\": feats[\"gran\"],\n",
    "        \"price\": feats[\"price\"],\n",
    "        \"prob_ara_tomorrow\": feats[\"p_tmr\"],\n",
    "        \"prob_ara_tomorrow_adj\": feats[\"p_tmr_adj\"],\n",
    "        \"ara_target_pct\": feats[\"ara_target_pct\"],\n",
    "        \"day_roc\": feats[\"day_roc\"],\n",
    "        \"prox\": feats[\"prox\"],\n",
    "        \"rvol60\": feats[\"rvol60\"],\n",
    "        \"cr\": feats[\"cr\"],\n",
    "        \"obv_up\": feats[\"obv_up\"],\n",
    "        \"virgin_hint\": feats[\"virgin_hint\"],\n",
    "        \"regime_score\": feats[\"regime_score\"],\n",
    "        \"confidence\": feats[\"confidence\"],\n",
    "        \"confidence_band\": feats[\"confidence_band\"],\n",
    "    }\n",
    "    rows_tmr.append(rec_tmr)\n",
    "\n",
    "    cnt_ok += 1\n",
    "    if i % 50 == 0:\n",
    "        # freshness warn\n",
    "        age_m = (datetime.utcnow() - df.index.max()).total_seconds()/60.0\n",
    "        fresh = f\"{int(age_m)}m{' (stale!)' if age_m>CACHE_STALE_M_1M else ''}\"\n",
    "        print(f\"â€¦ processed {i}/{len(tickers)} | ok={cnt_ok} | skip_daily={cnt_skip_daily} | skip_intraday={cnt_skip_intraday} | last_age={fresh}\")\n",
    "    time.sleep(0.01)  # kecilkan/naikkan sesuai beban\n",
    "\n",
    "# ranking & save (overwrite per tanggal)\n",
    "cols_today = [\"ticker\",\"gran\",\"price\",\"prob_ara_today_adj\",\"prob_ara_today\",\"confidence\",\"confidence_band\",\n",
    "              \"ara_target_pct\",\"day_roc\",\"prox\",\"coverage\",\"rvol30\",\"cr\",\"breakout\",\"mleft\",\"regime_score\"]\n",
    "cols_tmr   = [\"ticker\",\"gran\",\"price\",\"prob_ara_tomorrow_adj\",\"prob_ara_tomorrow\",\"confidence\",\"confidence_band\",\n",
    "              \"ara_target_pct\",\"day_roc\",\"prox\",\"rvol60\",\"cr\",\"obv_up\",\"virgin_hint\",\"regime_score\"]\n",
    "\n",
    "df_today = pd.DataFrame(rows_today)\n",
    "if df_today.empty:\n",
    "    df_today = pd.DataFrame(columns=cols_today)\n",
    "else:\n",
    "    for c in [\"prob_ara_today_adj\",\"prob_ara_today\",\"price\",\"confidence\"]:\n",
    "        if c not in df_today.columns: df_today[c] = np.nan\n",
    "    df_today = df_today.sort_values([\"prob_ara_today_adj\",\"prob_ara_today\",\"confidence\",\"price\"],\n",
    "                                    ascending=[False, False, False, False]).reset_index(drop=True)\n",
    "\n",
    "df_tmr = pd.DataFrame(rows_tmr)\n",
    "if df_tmr.empty:\n",
    "    df_tmr = pd.DataFrame(columns=cols_tmr)\n",
    "else:\n",
    "    for c in [\"prob_ara_tomorrow_adj\",\"prob_ara_tomorrow\",\"price\",\"confidence\"]:\n",
    "        if c not in df_tmr.columns: df_tmr[c] = np.nan\n",
    "    df_tmr = df_tmr.sort_values([\"prob_ara_tomorrow_adj\",\"prob_ara_tomorrow\",\"confidence\",\"price\"],\n",
    "                                ascending=[False, False, False, False]).reset_index(drop=True)\n",
    "\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "out_today = os.path.join(RESULTS_DIR, f\"prob_ara_today_{_now_date()}.csv\")\n",
    "out_tmr   = os.path.join(RESULTS_DIR, f\"prob_ara_tomorrow_{_now_date()}.csv\")\n",
    "df_today.to_csv(out_today, index=False)\n",
    "df_tmr.to_csv(out_tmr, index=False)\n",
    "\n",
    "print(f\"âœ… Saved (overwrite) â†’ {out_today} | rows={len(df_today)}\")\n",
    "print(f\"âœ… Saved (overwrite) â†’ {out_tmr}   | rows={len(df_tmr)}\")\n",
    "\n",
    "# preview\n",
    "display(df_today.head(15))\n",
    "display(df_tmr.head(15))\n",
    "# ============================================================================ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f811f0",
   "metadata": {},
   "source": [
    "## DIAG + WARMUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a454074",
   "metadata": {},
   "source": [
    "### v.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALL-IN-ONE WARMUP SERVICE (1 CELL, MANDIRI)\n",
    "# - Melengkapi cache_daily / cache_1m / cache_5m secara incremental\n",
    "# - Rolling 1mâ†’trim + downsample ke 5m untuk bagian lama\n",
    "# - Progress bar (tqdm), error report, smoketest opsional\n",
    "# - Konsisten UTC-naive (anti tz-naive vs tz-aware)\n",
    "# ============================================================\n",
    "\n",
    "# --------------- CONFIG ---------------\n",
    "from pathlib import Path\n",
    "import os, time, random, socket, warnings\n",
    "import pandas as pd\n",
    "\n",
    "# Folder dasar\n",
    "BASE_DIR        = Path(\".\")\n",
    "EMITEN_DIR      = BASE_DIR / \"emiten\"\n",
    "CACHE_DAILY_DIR = EMITEN_DIR / \"cache_daily\"\n",
    "CACHE_1M_DIR    = EMITEN_DIR / \"cache_1m\"\n",
    "CACHE_5M_DIR    = EMITEN_DIR / \"cache_5m\"\n",
    "RESULTS_DIR     = BASE_DIR / \"result\"\n",
    "\n",
    "# Roster (opsional). Jika None, otomatis cari file \"candidates_active_filtered_*.csv\" terbaru.\n",
    "ROSTER_PATH     = None\n",
    "\n",
    "# Retensi & fresh rules\n",
    "DAILY_BOOTSTRAP_Y               = 10     # bootstrap daily saat cache kosong\n",
    "CACHE_1M_WINDOW_D               = 7      # simpan 1m hanya 7 hari terakhir\n",
    "CACHE_5M_WINDOW_D               = 12     # simpan 5m 12 hari (sesuaikan 12â€“60 sesuai kebutuhan)\n",
    "INTRADAY_1M_FRESH_SLACK_MIN     = 20     # skip fetch 1m jika last_ts sudah dekat \"now\"\n",
    "INTRADAY_5M_FRESH_SLACK_MIN     = 30     # skip fetch 5m jika last_ts sudah dekat \"now\"\n",
    "\n",
    "# Filter hanya 30 menit awal Sesi-1 & 30 menit akhir Sesi-2 BEI (opsional)\n",
    "FILTER_SESSION_WINDOWS          = False  # True untuk aktifkan\n",
    "\n",
    "# yfinance retry/backoff\n",
    "MAX_RETRIES = 4\n",
    "BACKOFF_S   = [0.6, 1.5, 3.0, 6.0]       # ~exponential + jitter\n",
    "\n",
    "# Runner\n",
    "DO_SMOKETEST  = True\n",
    "SMOKE_TICKERS = [\"BBCA.JK\", \"ASII.JK\"]\n",
    "PRINT_EVERY   = 100\n",
    "DESC_LABEL    = \"Warmup(all-in-one)\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --------------- IMPORTS ---------------\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"yfinance belum terpasang. Jalankan: pip install yfinance pandas tqdm pytz\") from e\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _iter_progress(it, total, desc): return tqdm(it, total=total, ncols=90, desc=desc)\n",
    "except Exception:\n",
    "    def _iter_progress(it, total, desc): return it\n",
    "\n",
    "# --------------- STATE ---------------\n",
    "ERRORS = {\"dns\": [], \"missing\": [], \"timeout\": [], \"empty\": [], \"other\": []}\n",
    "STATS  = {\n",
    "    \"tickers\": 0,\n",
    "    \"daily_fetch\": 0, \"daily_skip\": 0,\n",
    "    \"m1_fetch\": 0, \"m1_skip\": 0, \"m1_fallback_5m\": 0,\n",
    "    \"m5_fetch\": 0, \"m5_skip\": 0,\n",
    "}\n",
    "\n",
    "# --------------- UTIL WAKTU (PAKSA UTC-NAIVE) ---------------\n",
    "def _naive(ts):\n",
    "    ts = pd.Timestamp(ts)\n",
    "    return ts.tz_convert(\"UTC\").tz_localize(None) if ts.tz is not None else ts\n",
    "\n",
    "def _now_utc():\n",
    "    return _naive(pd.Timestamp.utcnow())\n",
    "\n",
    "def _today_utc():\n",
    "    return _now_utc().normalize()\n",
    "\n",
    "def _now_date_tag():\n",
    "    try:\n",
    "        import pytz\n",
    "        tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "        return pd.Timestamp.now(tz).strftime(\"%Y%m%d\")\n",
    "    except Exception:\n",
    "        return pd.Timestamp.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "# --------------- FILE IO & SANITIZE ---------------\n",
    "def _idx_naive(df: pd.DataFrame):\n",
    "    if df is None or len(df)==0: return df\n",
    "    out = df.copy()\n",
    "    if not isinstance(out.index, pd.DatetimeIndex):\n",
    "        out.index = pd.to_datetime(out.index, errors=\"coerce\", utc=True).tz_convert(\"UTC\").tz_localize(None)\n",
    "    elif out.index.tz is not None:\n",
    "        out.index = out.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "    out = out[~out.index.duplicated(keep=\"last\")].sort_index()\n",
    "    return out\n",
    "\n",
    "def _sanitize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame()\n",
    "    out = _idx_naive(df)\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in out.columns]\n",
    "    if not keep: keep = list(out.columns)\n",
    "    out = out[keep].dropna(how=\"all\")\n",
    "    return out\n",
    "\n",
    "def _read_cache_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists(): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[0], index_col=0)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if \"Date\" in df.columns:\n",
    "                df = df.set_index(\"Date\")\n",
    "        except Exception:\n",
    "            return pd.DataFrame()\n",
    "    return _sanitize_ohlcv(df)\n",
    "\n",
    "def _resample_to_5m(df_1m: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_1m.empty: return df_1m\n",
    "    df = _idx_naive(df_1m)\n",
    "    agg = {\"Open\":\"first\",\"High\":\"max\",\"Low\":\"min\",\"Close\":\"last\",\"Volume\":\"sum\"}\n",
    "    if \"Adj Close\" in df.columns: agg[\"Adj Close\"] = \"last\"\n",
    "    df5 = df.resample(\"5min\", label=\"right\", closed=\"right\").agg(agg)\n",
    "    ohlc_cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in df5.columns]\n",
    "    df5 = df5.dropna(subset=ohlc_cols, how=\"all\")\n",
    "    return _sanitize_ohlcv(df5)\n",
    "\n",
    "def _write_cache_csv(path: Path, df_new: pd.DataFrame, window_days: int|None=None):\n",
    "    p = Path(path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new = _sanitize_ohlcv(df_new)\n",
    "\n",
    "    # merge lama + baru\n",
    "    if p.exists():\n",
    "        df_old = _read_cache_csv(p)\n",
    "        df = pd.concat([df_old, df_new]).sort_index()\n",
    "        df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    else:\n",
    "        df = df_new\n",
    "\n",
    "    now = _now_utc()\n",
    "    path_lower = str(p).lower().replace(\"\\\\\",\"/\")\n",
    "    is_1m   = \"cache_1m\"   in path_lower\n",
    "    is_5m   = \"cache_5m\"   in path_lower\n",
    "    is_daily= \"cache_daily\" in path_lower\n",
    "\n",
    "    # (opsional) filter window sesi BEI\n",
    "    if FILTER_SESSION_WINDOWS and (is_1m or is_5m) and not df.empty:\n",
    "        import pytz\n",
    "        jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "        dfl = df.copy()\n",
    "        dfl.index = dfl.index.tz_localize(\"UTC\").tz_convert(jkt)\n",
    "        mask = (\n",
    "            ((dfl.index.time >= pd.Timestamp(\"09:00\").time()) & (dfl.index.time < pd.Timestamp(\"09:30\").time())) |\n",
    "            ((dfl.index.time >= pd.Timestamp(\"14:30\").time()) & (dfl.index.time <= pd.Timestamp(\"15:00\").time()))\n",
    "        )\n",
    "        dfl = dfl[mask]\n",
    "        dfl.index = dfl.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "        df = dfl\n",
    "\n",
    "    if is_1m:\n",
    "        cutoff_1m = now - pd.Timedelta(days=window_days or CACHE_1M_WINDOW_D)\n",
    "        older = df[df.index < cutoff_1m]\n",
    "        tail  = df[df.index >= cutoff_1m]\n",
    "        # turun ke 5m untuk bagian lama\n",
    "        if not older.empty:\n",
    "            df5_add = _resample_to_5m(older)\n",
    "            p5 = CACHE_5M_DIR / p.name\n",
    "            if p5.exists(): df5_old = _read_cache_csv(p5)\n",
    "            else: df5_old = pd.DataFrame()\n",
    "            df5 = pd.concat([df5_old, df5_add]).sort_index()\n",
    "            df5 = df5[~df5.index.duplicated(keep=\"last\")]\n",
    "            cutoff_5m = now - pd.Timedelta(days=CACHE_5M_WINDOW_D)\n",
    "            df5 = df5[df5.index >= cutoff_5m]\n",
    "            df5.to_csv(p5)\n",
    "        tail.to_csv(p)  # simpan tail saja\n",
    "        return\n",
    "\n",
    "    if is_5m:\n",
    "        keep_days = window_days or CACHE_5M_WINDOW_D\n",
    "        cutoff_5m = now - pd.Timedelta(days=keep_days)\n",
    "        df = df[df.index >= cutoff_5m]\n",
    "        df.to_csv(p)\n",
    "        return\n",
    "\n",
    "    if is_daily:\n",
    "        # daily biasanya keep all; jika window_days diberikan, trim\n",
    "        if window_days:\n",
    "            cutoff_d = now.normalize() - pd.Timedelta(days=window_days)\n",
    "            df = df[df.index >= cutoff_d]\n",
    "        df.to_csv(p)\n",
    "        return\n",
    "\n",
    "    # default fallback\n",
    "    df.to_csv(p)\n",
    "\n",
    "# --------------- YFINANCE WRAPPER (RETRY) ---------------\n",
    "def yfdl(ticker, **kw):\n",
    "    # normalisasi waktu\n",
    "    if \"start\" in kw and kw[\"start\"] is not None: kw[\"start\"] = _naive(kw[\"start\"]).to_pydatetime()\n",
    "    if \"end\"   in kw and kw[\"end\"]   is not None: kw[\"end\"]   = _naive(kw[\"end\"]).to_pydatetime()\n",
    "    # argumen kompatibel lintas versi\n",
    "    kw.pop(\"timeout\", None)\n",
    "    kw.pop(\"threads\", None)\n",
    "    kw[\"progress\"]    = False\n",
    "    kw[\"auto_adjust\"] = False\n",
    "    kw[\"actions\"]     = False\n",
    "    kw[\"group_by\"]    = \"column\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            try: socket.gethostbyname(\"query2.finance.yahoo.com\")\n",
    "            except Exception: pass\n",
    "            df = yf.download(ticker, **kw)\n",
    "            df = _sanitize_ohlcv(df)\n",
    "            if df is not None and not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            low = str(e).lower()\n",
    "            if \"could not resolve host\" in low or \"temporary failure in name resolution\" in low:\n",
    "                ERRORS[\"dns\"].append(ticker); break\n",
    "            elif \"timed out\" in low:\n",
    "                ERRORS[\"timeout\"].append(ticker); continue\n",
    "            elif \"no price data found\" in low or \"possibly delisted\" in low:\n",
    "                ERRORS[\"missing\"].append(ticker); return pd.DataFrame()\n",
    "            else:\n",
    "                ERRORS[\"other\"].append(f\"{ticker}: {e}\")\n",
    "        time.sleep(BACKOFF_S[min(attempt, len(BACKOFF_S)-1)] + random.random()*0.5)\n",
    "    ERRORS[\"empty\"].append(ticker)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# --------------- INCREMENTAL FETCHERS ---------------\n",
    "def _fresh_enough(last_ts, now_ts, slack_min):\n",
    "    try:\n",
    "        return _naive(last_ts) >= (_naive(now_ts) - pd.Timedelta(minutes=slack_min))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_daily_incremental(ticker: str):\n",
    "    path = CACHE_DAILY_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    today = _today_utc()\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{DAILY_BOOTSTRAP_Y}y\", interval=\"1d\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=None)\n",
    "        STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_date = _naive(base.index.max()).normalize()\n",
    "    start, end = _naive(last_date + pd.Timedelta(days=1)), _naive(today + pd.Timedelta(days=1))\n",
    "    if start > end:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    fresh = yfdl(ticker, start=start, end=end, interval=\"1d\")\n",
    "    if fresh.empty:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=None)\n",
    "    STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "def get_1m_incremental(ticker: str):\n",
    "    path = CACHE_1M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_1M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_1M_WINDOW_D}d\", interval=\"1m\")\n",
    "        if fresh.empty:\n",
    "            df5, _ = get_5m_incremental(ticker)\n",
    "            STATS[\"m1_fallback_5m\"] += 1; return df5, \"fallback(5m)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_1M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=1), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"1m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "    STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "def get_5m_incremental(ticker: str):\n",
    "    path = CACHE_5M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_5M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_5M_WINDOW_D}d\", interval=\"5m\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_5M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=5), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"5m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "    STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "# --------------- TICKER SOURCE ---------------\n",
    "def _detect_roster_path():\n",
    "    cand = sorted(EMITEN_DIR.glob(\"candidates_active_filtered_*.csv\"))\n",
    "    return str(cand[-1]) if cand else None\n",
    "\n",
    "def _infer_tickers():\n",
    "    roster = ROSTER_PATH or _detect_roster_path()\n",
    "    if roster and Path(roster).exists():\n",
    "        df = pd.read_csv(roster)\n",
    "        for c in df.columns:\n",
    "            if c.lower() in (\"ticker\",\"symbol\",\"kode\",\"emiten\"):\n",
    "                return df[c].astype(str).str.strip().tolist()\n",
    "        return df.iloc[:,0].astype(str).str.strip().tolist()\n",
    "    # fallback: union nama file di cache\n",
    "    s = set()\n",
    "    for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR]:\n",
    "        if Path(d).exists():\n",
    "            for f in Path(d).glob(\"*.csv\"): s.add(f.stem)\n",
    "    return sorted(s)\n",
    "\n",
    "# --------------- PREP DIRS ---------------\n",
    "for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------- RUNNER + PROGRESS BAR ---------------\n",
    "for k in (\"dns\",\"missing\",\"timeout\",\"empty\",\"other\"): ERRORS[k].clear()\n",
    "for k in STATS: STATS[k] = 0\n",
    "\n",
    "tickers = _infer_tickers()\n",
    "STATS[\"tickers\"] = len(tickers)\n",
    "print(f\"Warmup start â€¢ tickers={len(tickers)} â€¢ 1m_win={CACHE_1M_WINDOW_D}d â€¢ 5m_win={CACHE_5M_WINDOW_D}d â€¢ daily_bootstrap={DAILY_BOOTSTRAP_Y}y\")\n",
    "t0 = time.time()\n",
    "\n",
    "for i, t in enumerate(_iter_progress(tickers, total=len(tickers), desc=DESC_LABEL), 1):\n",
    "    try:\n",
    "        _ = get_daily_incremental(t)\n",
    "        _ = get_1m_incremental(t)\n",
    "        _ = get_5m_incremental(t)\n",
    "        if PRINT_EVERY and (i % PRINT_EVERY == 0):\n",
    "            print(f\"[{i}/{len(tickers)}] {t}\")\n",
    "    except Exception as e:\n",
    "        ERRORS[\"other\"].append(f\"{t}: {e}\")\n",
    "\n",
    "# --------------- SAVE ERRORS ---------------\n",
    "rows = []\n",
    "for kind, vals in ERRORS.items():\n",
    "    for v in vals:\n",
    "        rows.append({\"kind\": kind, \"detail\": v})\n",
    "if rows:\n",
    "    out = RESULTS_DIR / f\"errors_warmup_{_now_date_tag()}.csv\"\n",
    "    pd.DataFrame(rows).drop_duplicates().to_csv(out, index=False)\n",
    "    print(f\"\\n[ERRORS] Saved â†’ {out}\")\n",
    "\n",
    "# --------------- SUMMARY ---------------\n",
    "elapsed = time.time() - t0\n",
    "print(\"\\n===== SUMMARY =====\")\n",
    "print(f\"Tickers         : {STATS['tickers']}\")\n",
    "print(f\"Daily   fetch/skip : {STATS['daily_fetch']}/{STATS['daily_skip']}\")\n",
    "print(f\"1m      fetch/skip : {STATS['m1_fetch']}/{STATS['m1_skip']}  | fallbackâ†’5m: {STATS['m1_fallback_5m']}\")\n",
    "print(f\"5m      fetch/skip : {STATS['m5_fetch']}/{STATS['m5_skip']}\")\n",
    "print(f\"Errors  dns/missing/timeout/empty/other : \"\n",
    "      f\"{len(ERRORS['dns'])}/{len(ERRORS['missing'])}/{len(ERRORS['timeout'])}/{len(ERRORS['empty'])}/{len(ERRORS['other'])}\")\n",
    "print(f\"Elapsed         : {elapsed:.2f}s\")\n",
    "\n",
    "# --------------- OPTIONAL SMOKETEST ---------------\n",
    "if DO_SMOKETEST:\n",
    "    print(\"\\n=== SMOKETEST ===\")\n",
    "    for _t in SMOKE_TICKERS:\n",
    "        try:\n",
    "            print(f\"\\n>>> {_t}\")\n",
    "            def _show(name, tup):\n",
    "                if isinstance(tup, tuple) and len(tup)==2: df, st = tup\n",
    "                else: df, st = tup, \"(unknown)\"\n",
    "                print(f\"{name:>5s}: {st}\")\n",
    "                try:\n",
    "                    if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "                        display(df.tail(3))\n",
    "                except Exception: pass\n",
    "            _show(\"DAILY\", get_daily_incremental(_t))\n",
    "            _show(\"1m\",    get_1m_incremental(_t))\n",
    "            _show(\"5m\",    get_5m_incremental(_t))\n",
    "        except Exception as e:\n",
    "            print(f\"[SMOKE ERROR] {_t}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24b8c2",
   "metadata": {},
   "source": [
    "### v.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# PARALLEL RUNNER (anti-timeout) â€” drop-in\n",
    "# v 1.2\n",
    "\n",
    "# ===========================================\n",
    "import time, concurrent.futures as cf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- TUNEABLES ---\n",
    "MAX_WORKERS        = 8          # 6â€“10 aman; sesuaikan bandwidth/IP kamu\n",
    "WORKER_TIMEOUT_S   = 15         # hard timeout per ticker task (menghindari nunggu 10s x 3 panggilan)\n",
    "YF_PAUSE_JITTER_S  = (0.05, 0.25)   # jeda kecil antar ticker di worker (kurangi spike)\n",
    "\n",
    "# --- helper kecil untuk jitter ---\n",
    "import random\n",
    "def _sleep_jitter(a, b):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# --- task per ticker (dipanggil di thread worker) ---\n",
    "def _warmup_one_ticker(ticker: str):\n",
    "    \"\"\"\n",
    "    Jalankan 3 langkah untuk 1 ticker.\n",
    "    Return ringkasan tuple (ticker, daily_stat, m1_stat, m5_stat, err_msg_or_None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # DAILY\n",
    "        d = get_daily_incremental(ticker)\n",
    "        _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "\n",
    "        # 1m\n",
    "        m1 = get_1m_incremental(ticker)\n",
    "        _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "\n",
    "        # 5m\n",
    "        m5 = get_5m_incremental(ticker)\n",
    "\n",
    "        # ambil status string kalau pair (df, \"stat\"), atau \"(ok)\" bila tidak tersedia\n",
    "        def _stat(x):\n",
    "            if isinstance(x, tuple) and len(x) == 2:\n",
    "                return str(x[1])\n",
    "            return \"(ok)\"\n",
    "        return (ticker, _stat(d), _stat(m1), _stat(m5), None)\n",
    "\n",
    "    except Exception as e:\n",
    "        # biar tetap tercatat, worker tidak meledakkan seluruh loop\n",
    "        return (ticker, None, None, None, str(e))\n",
    "\n",
    "# --- ambil daftar tickers dari fungsi yang sudah ada ---\n",
    "tickers = _infer_tickers()\n",
    "STATS[\"tickers\"] = len(tickers)\n",
    "print(f\"Parallel warmup â€¢ tickers={len(tickers)} â€¢ workers={MAX_WORKERS} â€¢ timeout={WORKER_TIMEOUT_S}s\")\n",
    "\n",
    "t0 = time.time()\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=len(tickers), ncols=90, desc=\"Warmup(parallel)\")\n",
    "\n",
    "futures = []\n",
    "results = []\n",
    "errors_local = []\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    # submit semua ticker\n",
    "    for tkr in tickers:\n",
    "        futures.append(ex.submit(_warmup_one_ticker, tkr))\n",
    "\n",
    "    # ambil hasil satu per satu dengan hard-timeout per future\n",
    "    for fut in futures:\n",
    "        try:\n",
    "            tkr, dstat, m1stat, m5stat, err = fut.result(timeout=WORKER_TIMEOUT_S)\n",
    "            if err:\n",
    "                # catat sebagai \"timeout/error\" umum â€” tidak menghambat yang lain\n",
    "                ERRORS[\"other\"].append(f\"{tkr}: {err}\")\n",
    "                errors_local.append((tkr, err))\n",
    "            results.append((tkr, dstat, m1stat, m5stat))\n",
    "        except cf.TimeoutError:\n",
    "            # kalau thread macet > WORKER_TIMEOUT_S, kita tandai & lanjut\n",
    "            ERRORS[\"timeout\"].append(\"ticker_task\")\n",
    "            errors_local.append((tkr, f\"worker-timeout>{WORKER_TIMEOUT_S}s\"))\n",
    "        finally:\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# --- simpan error report (opsional) ---\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "tag = _now_date_tag()\n",
    "\n",
    "if errors_local:\n",
    "    df_err = pd.DataFrame(errors_local, columns=[\"Ticker\", \"Error\"])\n",
    "    df_err.drop_duplicates().to_csv(RESULTS_DIR / f\"errors_warmup_parallel_{tag}.csv\", index=False)\n",
    "    print(f\"\\n[ERRORS] Saved â†’ result/errors_warmup_parallel_{tag}.csv\")\n",
    "\n",
    "# --- summary akhir ---\n",
    "elapsed = time.time() - t0\n",
    "print(\"\\n===== SUMMARY (parallel) =====\")\n",
    "print(f\"Tickers         : {STATS['tickers']}\")\n",
    "print(f\"Daily   fetch/skip : {STATS.get('daily_fetch',0)}/{STATS.get('daily_skip',0)}\")\n",
    "print(f\"1m      fetch/skip : {STATS.get('m1_fetch',0)}/{STATS.get('m1_skip',0)}  | fallbackâ†’5m: {STATS.get('m1_fallback_5m',0)}\")\n",
    "print(f\"5m      fetch/skip : {STATS.get('m5_fetch',0)}/{STATS.get('m5_skip',0)}\")\n",
    "print(f\"Errors  dns/missing/timeout/empty/other : \"\n",
    "      f\"{len(ERRORS['dns'])}/{len(ERRORS['missing'])}/{len(ERRORS['timeout'])}/{len(ERRORS['empty'])}/{len(ERRORS['other'])}\")\n",
    "print(f\"Elapsed         : {elapsed:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f91c7",
   "metadata": {},
   "source": [
    "### v.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PATCH TZ-SAFE (drop-in untuk v1.3.1) ====\n",
    "import pandas as pd\n",
    "\n",
    "def _ensure_aware_utc(ts=None):\n",
    "    \"\"\"\n",
    "    Kembalikan Timestamp yang PASTI tz-aware UTC.\n",
    "    - Jika input tz-naive -> tz_localize('UTC')\n",
    "    - Jika input tz-aware -> tz_convert('UTC')\n",
    "    - Jika None -> pakai sekarang (UTC)\n",
    "    \"\"\"\n",
    "    t = pd.Timestamp.utcnow() if ts is None else pd.Timestamp(ts)\n",
    "    if t.tz is None:\n",
    "        return t.tz_localize(\"UTC\")\n",
    "    return t.tz_convert(\"UTC\")\n",
    "\n",
    "def _is_jkt_market_time(ts_utc=None):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts_utc)\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5:   # Sabtu/Minggu\n",
    "        return False\n",
    "    tm = t_jkt.time()\n",
    "    # Sesi reguler BEI\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n",
    "\n",
    "def _is_jkt_session(ts):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts)\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5:\n",
    "        return False\n",
    "    tm = t_jkt.time()\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# v1.3.3 â€” ALL-IN-ONE\n",
    "# Warmup (parallel, tz-safe, retry, batch) + Robust Sanity + Toggles\n",
    "# ============================================================\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "from pathlib import Path\n",
    "import os, time, random, socket, warnings, math, traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Toggles\n",
    "DO_WARMUP        = True   # set False untuk skip warmup dan langsung sanity\n",
    "SANITY_ENABLE    = True   # set False untuk hanya warmup\n",
    "\n",
    "# Paths\n",
    "BASE_DIR        = Path(\".\")\n",
    "EMITEN_DIR      = BASE_DIR / \"emiten\"\n",
    "CACHE_DAILY_DIR = EMITEN_DIR / \"cache_daily\"\n",
    "CACHE_1M_DIR    = EMITEN_DIR / \"cache_1m\"\n",
    "CACHE_5M_DIR    = EMITEN_DIR / \"cache_5m\"\n",
    "RESULTS_DIR     = BASE_DIR / \"result\"\n",
    "ROSTER_PATH     = None  # ex: \"emiten/candidates_active_filtered_20250813.csv\" (opsional)\n",
    "\n",
    "# Retensi / freshness\n",
    "DAILY_BOOTSTRAP_Y               = 10\n",
    "CACHE_1M_WINDOW_D               = 7\n",
    "CACHE_5M_WINDOW_D               = 12\n",
    "INTRADAY_1M_FRESH_SLACK_MIN     = 0\n",
    "INTRADAY_5M_FRESH_SLACK_MIN     = 0\n",
    "FILTER_SESSION_WINDOWS          = False  # True: simpan hanya 09:00â€“09:30 & 14:30â€“15:00 WIB\n",
    "\n",
    "# yfinance retry/backoff\n",
    "MAX_RETRIES   = 4\n",
    "BACKOFF_S     = [0.6, 1.5, 3.0, 6.0]\n",
    "\n",
    "# Parallel runner\n",
    "MAX_WORKERS              = 6\n",
    "WORKER_TIMEOUT_S         = 22\n",
    "SUBMIT_BATCH             = 128\n",
    "MAX_RETRY_ROUNDS         = 2\n",
    "YF_PAUSE_JITTER_S        = (0.08, 0.30)\n",
    "OFF_HOURS_SKIP_INTRADAY  = False\n",
    "DESC_LABEL               = \"Warmup(parallel v1.3.3)\"\n",
    "\n",
    "# Sanity\n",
    "SANITY_SAMPLE_SIZE       = 10\n",
    "CHECK_BEI_HOURS          = True\n",
    "TOL_PCT_PRICE            = 0.001  # 0.1%\n",
    "TOL_VOL_DIFF             = 0.02   # 2%\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------- DEPENDENCIES ----------\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install dulu: pip install yfinance pandas requests tqdm pytz\") from e\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _iter_progress(it, total, desc): return tqdm(it, total=total, ncols=90, desc=desc)\n",
    "except Exception:\n",
    "    def _iter_progress(it, total, desc): return it\n",
    "\n",
    "# ---------- GLOBAL STATE ----------\n",
    "ERRORS = {\"dns\": [], \"missing\": [], \"timeout\": [], \"empty\": [], \"other\": []}\n",
    "STATS  = {\n",
    "    \"tickers\": 0,\n",
    "    \"daily_fetch\": 0, \"daily_skip\": 0,\n",
    "    \"m1_fetch\": 0, \"m1_skip\": 0, \"m1_fallback_5m\": 0,\n",
    "    \"m5_fetch\": 0, \"m5_skip\": 0,\n",
    "}\n",
    "\n",
    "# ensure dirs\n",
    "for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TIME HELPERS (tz-safe) ----------\n",
    "def _ensure_aware_utc(ts=None):\n",
    "    \"\"\"Return UTC-aware Timestamp regardless of input being naive/aware/None.\"\"\"\n",
    "    t = pd.Timestamp.utcnow() if ts is None else pd.Timestamp(ts)\n",
    "    if t.tz is None:\n",
    "        return t.tz_localize(\"UTC\")\n",
    "    return t.tz_convert(\"UTC\")\n",
    "\n",
    "def _naive(ts):\n",
    "    t = pd.Timestamp(ts)\n",
    "    if t.tz is None:\n",
    "        return t\n",
    "    return t.tz_convert(\"UTC\").tz_localize(None)\n",
    "\n",
    "def _now_utc():   return _naive(pd.Timestamp.utcnow())\n",
    "def _today_utc(): return _now_utc().normalize()\n",
    "\n",
    "def _now_date_tag():\n",
    "    try:\n",
    "        import pytz\n",
    "        tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "        return pd.Timestamp.now(tz).strftime(\"%Y%m%d\")\n",
    "    except Exception:\n",
    "        return pd.Timestamp.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "def _now_tag_full():\n",
    "    try:\n",
    "        import pytz\n",
    "        tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "        return pd.Timestamp.now(tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    except Exception:\n",
    "        return pd.Timestamp.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ---------- IO & SANITIZE ----------\n",
    "def _idx_naive(df: pd.DataFrame):\n",
    "    if df is None or len(df)==0: return df\n",
    "    out = df.copy()\n",
    "    if not isinstance(out.index, pd.DatetimeIndex):\n",
    "        out.index = pd.to_datetime(out.index, errors=\"coerce\", utc=True).tz_convert(\"UTC\").tz_localize(None)\n",
    "    elif out.index.tz is not None:\n",
    "        out.index = out.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "    return out[~out.index.duplicated(keep=\"last\")].sort_index()\n",
    "\n",
    "def _sanitize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame()\n",
    "    out = _idx_naive(df)\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in out.columns]\n",
    "    if not keep: keep = list(out.columns)\n",
    "    out = out[keep].dropna(how=\"all\")\n",
    "    return out\n",
    "\n",
    "def _read_cache_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists(): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[0], index_col=0)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if \"Date\" in df.columns: df = df.set_index(\"Date\")\n",
    "        except Exception:\n",
    "            return pd.DataFrame()\n",
    "    return _sanitize_ohlcv(df)\n",
    "\n",
    "def _resample_to_5m(df_1m: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_1m.empty: return df_1m\n",
    "    df = _idx_naive(df_1m)\n",
    "    agg = {\"Open\":\"first\",\"High\":\"max\",\"Low\":\"min\",\"Close\":\"last\",\"Volume\":\"sum\"}\n",
    "    if \"Adj Close\" in df.columns: agg[\"Adj Close\"] = \"last\"\n",
    "    out = df.resample(\"5min\", label=\"right\", closed=\"right\").agg(agg)\n",
    "    ohlc = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in out.columns]\n",
    "    out = out.dropna(subset=ohlc, how=\"all\")\n",
    "    return _sanitize_ohlcv(out)\n",
    "\n",
    "def _write_cache_csv(path: Path, df_new: pd.DataFrame, window_days: int|None=None):\n",
    "    p = Path(path); p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new = _sanitize_ohlcv(df_new)\n",
    "    if p.exists():\n",
    "        df_old = _read_cache_csv(p)\n",
    "        df = pd.concat([df_old, df_new]).sort_index()\n",
    "        df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    else:\n",
    "        df = df_new\n",
    "\n",
    "    now = _now_utc()\n",
    "    path_lower = str(p).lower().replace(\"\\\\\",\"/\")\n",
    "    is_1m   = \"cache_1m\" in path_lower\n",
    "    is_5m   = \"cache_5m\" in path_lower\n",
    "    is_daily= \"cache_daily\" in path_lower\n",
    "\n",
    "    # session windows filter (opsional)\n",
    "    if FILTER_SESSION_WINDOWS and (is_1m or is_5m) and not df.empty:\n",
    "        import pytz\n",
    "        jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "        dfl = df.copy()\n",
    "        dfl.index = dfl.index.tz_localize(\"UTC\").tz_convert(jkt)\n",
    "        mask = (\n",
    "            ((dfl.index.time >= pd.Timestamp(\"09:00\").time()) & (dfl.index.time < pd.Timestamp(\"09:30\").time())) |\n",
    "            ((dfl.index.time >= pd.Timestamp(\"14:30\").time()) & (dfl.index.time <= pd.Timestamp(\"15:00\").time()))\n",
    "        )\n",
    "        dfl = dfl[mask]\n",
    "        dfl.index = dfl.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "        df = dfl\n",
    "\n",
    "    if is_1m:\n",
    "        cutoff_1m = now - pd.Timedelta(days=window_days or CACHE_1M_WINDOW_D)\n",
    "        older = df[df.index < cutoff_1m]\n",
    "        tail  = df[df.index >= cutoff_1m]\n",
    "        if not older.empty:\n",
    "            df5_add = _resample_to_5m(older)\n",
    "            p5 = CACHE_5M_DIR / p.name\n",
    "            if p5.exists(): df5_old = _read_cache_csv(p5)\n",
    "            else: df5_old = pd.DataFrame()\n",
    "            df5 = pd.concat([df5_old, df5_add]).sort_index()\n",
    "            df5 = df5[~df5.index.duplicated(keep=\"last\")]\n",
    "            cutoff_5m = now - pd.Timedelta(days=CACHE_5M_WINDOW_D)\n",
    "            df5 = df5[df5.index >= cutoff_5m]\n",
    "            df5.to_csv(p5)\n",
    "        tail.to_csv(p); return\n",
    "\n",
    "    if is_5m:\n",
    "        keep_days = window_days or CACHE_5M_WINDOW_D\n",
    "        cutoff_5m = now - pd.Timedelta(days=keep_days)\n",
    "        df = df[df.index >= cutoff_5m]\n",
    "        df.to_csv(p); return\n",
    "\n",
    "    if is_daily:\n",
    "        if window_days:\n",
    "            cutoff_d = now.normalize() - pd.Timedelta(days=window_days)\n",
    "            df = df[df.index >= cutoff_d]\n",
    "        df.to_csv(p); return\n",
    "\n",
    "    df.to_csv(p)\n",
    "\n",
    "# ---------- yfinance via requests.Session (no curl) ----------\n",
    "os.environ[\"YF_USE_CURL\"] = \"0\"\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "_YF_SESSION = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5, connect=5, read=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=(429,500,502,503,504),\n",
    "    allowed_methods=False\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)\n",
    "_YF_SESSION.mount(\"https://\", adapter)\n",
    "_YF_SESSION.mount(\"http://\", adapter)\n",
    "_YF_SESSION.headers.update({\"User-Agent\": \"Mozilla/5.0 (warmup-service)\"})\n",
    "import importlib; importlib.reload(yf)\n",
    "\n",
    "def yfdl(ticker, **kw):\n",
    "    if \"start\" in kw and kw[\"start\"] is not None: kw[\"start\"] = _naive(kw[\"start\"]).to_pydatetime()\n",
    "    if \"end\"   in kw and kw[\"end\"]   is not None: kw[\"end\"]   = _naive(kw[\"end\"]).to_pydatetime()\n",
    "    kw.pop(\"threads\", None)\n",
    "    kw[\"progress\"]    = False\n",
    "    kw[\"auto_adjust\"] = False\n",
    "    kw[\"actions\"]     = False\n",
    "    kw[\"group_by\"]    = \"column\"\n",
    "    kw[\"session\"]     = _YF_SESSION\n",
    "    kw.setdefault(\"timeout\", 20)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            try: socket.gethostbyname(\"query2.finance.yahoo.com\")\n",
    "            except Exception: pass\n",
    "            df = yf.download(ticker, **kw)\n",
    "            df = _sanitize_ohlcv(df)\n",
    "            if df is not None and not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            low = str(e).lower()\n",
    "            if \"could not resolve host\" in low or \"temporary failure in name resolution\" in low:\n",
    "                ERRORS[\"dns\"].append(ticker); break\n",
    "            elif \"timed out\" in low:\n",
    "                ERRORS[\"timeout\"].append(ticker)\n",
    "            elif \"no price data found\" in low or \"possibly delisted\" in low:\n",
    "                ERRORS[\"missing\"].append(ticker); return pd.DataFrame()\n",
    "            else:\n",
    "                ERRORS[\"other\"].append(f\"{ticker}: {e}\")\n",
    "        time.sleep(BACKOFF_S[min(attempt, len(BACKOFF_S)-1)] + random.random()*0.8)\n",
    "    ERRORS[\"empty\"].append(ticker)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ---------- FRESH CHECK ----------\n",
    "def _fresh_enough(last_ts, now_ts, slack_min):\n",
    "    try:\n",
    "        return _naive(last_ts) >= (_naive(now_ts) - pd.Timedelta(minutes=slack_min))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ---------- INCREMENTAL FETCHERS ----------\n",
    "def get_daily_incremental(ticker: str):\n",
    "    path = CACHE_DAILY_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    today = _today_utc()\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{DAILY_BOOTSTRAP_Y}y\", interval=\"1d\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=None)\n",
    "        STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_date = _naive(base.index.max()).normalize()\n",
    "    start, end = _naive(last_date + pd.Timedelta(days=1)), _naive(today + pd.Timedelta(days=1))\n",
    "    if start > end:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    fresh = yfdl(ticker, start=start, end=end, interval=\"1d\")\n",
    "    if fresh.empty:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=None)\n",
    "    STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "def get_1m_incremental(ticker: str):\n",
    "    path = CACHE_1M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_1M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_1M_WINDOW_D}d\", interval=\"1m\")\n",
    "        if fresh.empty:\n",
    "            df5, _ = get_5m_incremental(ticker)\n",
    "            STATS[\"m1_fallback_5m\"] += 1; return df5, \"fallback(5m)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_1M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=1), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"1m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "    STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "def get_5m_incremental(ticker: str):\n",
    "    path = CACHE_5M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_5M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_5M_WINDOW_D}d\", interval=\"5m\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_5M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=5), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"5m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "    STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "# ---------- TICKER SOURCE ----------\n",
    "def _detect_roster_path():\n",
    "    cand = sorted(EMITEN_DIR.glob(\"candidates_active_filtered_*.csv\"))\n",
    "    return str(cand[-1]) if cand else None\n",
    "\n",
    "def _infer_tickers():\n",
    "    roster = ROSTER_PATH or _detect_roster_path()\n",
    "    if roster and Path(roster).exists():\n",
    "        df = pd.read_csv(roster)\n",
    "        for c in df.columns:\n",
    "            if c.lower() in (\"ticker\",\"symbol\",\"kode\",\"emiten\"):\n",
    "                return df[c].astype(str).str.strip().tolist()\n",
    "        return df.iloc[:,0].astype(str).str.strip().tolist()\n",
    "    s = set()\n",
    "    for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR]:\n",
    "        if Path(d).exists():\n",
    "            for f in Path(d).glob(\"*.csv\"): s.add(f.stem)\n",
    "    return sorted(s)\n",
    "\n",
    "# ---------- MARKET-TIME GUARD (TZ-SAFE) ----------\n",
    "def _is_jkt_market_time(ts_utc=None):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts_utc)        # tz-safe\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5: return False\n",
    "    tm = t_jkt.time()\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n",
    "\n",
    "def _prewarm_dns():\n",
    "    for host in (\"query1.finance.yahoo.com\",\"query2.finance.yahoo.com\"):\n",
    "        try: socket.gethostbyname(host)\n",
    "        except: pass\n",
    "\n",
    "def _sleep_jitter(a,b): time.sleep(random.uniform(a,b))\n",
    "def _stat_label(x): return (str(x[1]) if isinstance(x, tuple) and len(x)==2 else \"(ok)\")\n",
    "\n",
    "# ---------- PARALLEL RUNNER v1.3.3 ----------\n",
    "import concurrent.futures as cf\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _tqdm(x, **k): return tqdm(x, **k)\n",
    "except Exception:\n",
    "    def _tqdm(x, **k): return x\n",
    "\n",
    "def _warmup_one_ticker(ticker: str, do_intraday: bool):\n",
    "    try:\n",
    "        d = get_daily_incremental(ticker)\n",
    "        _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "        if do_intraday:\n",
    "            m1 = get_1m_incremental(ticker)\n",
    "            _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "            m5 = get_5m_incremental(ticker)\n",
    "        else:\n",
    "            m1 = \"(skipped-offhours)\"; m5 = \"(skipped-offhours)\"\n",
    "        return (ticker, _stat_label(d), _stat_label(m1), _stat_label(m5), None)\n",
    "    except Exception as e:\n",
    "        return (ticker, None, None, None, str(e))\n",
    "\n",
    "def run_parallel_warmup_v133(tickers):\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    all_rows, all_errs = [], []\n",
    "    rounds = MAX_RETRY_ROUNDS + 1\n",
    "    remain = list(tickers)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for r in range(rounds):\n",
    "        if not remain: break\n",
    "        do_intraday = (not OFF_HOURS_SKIP_INTRADAY) or _is_jkt_market_time()\n",
    "        label = f\"{DESC_LABEL} R{r}{' (intraday)' if do_intraday else ' (daily-only)'}\"\n",
    "        print(f\"\\nRound {r+1}/{rounds} â€¢ tickers={len(remain)} â€¢ workers={MAX_WORKERS} â€¢ timeout={WORKER_TIMEOUT_S}s â€¢ {('INTRA ON' if do_intraday else 'INTRA OFF')}\")\n",
    "        _prewarm_dns()\n",
    "\n",
    "        failed_this = []\n",
    "        pbar = tqdm(total=len(remain), ncols=90, desc=label)\n",
    "\n",
    "        batches = math.ceil(len(remain)/SUBMIT_BATCH)\n",
    "        idx = 0\n",
    "        for b in range(batches):\n",
    "            batch = remain[idx: idx+SUBMIT_BATCH]; idx += SUBMIT_BATCH\n",
    "            futs = []\n",
    "            with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "                for tkr in batch:\n",
    "                    futs.append(ex.submit(_warmup_one_ticker, tkr, do_intraday))\n",
    "                for fut in futs:\n",
    "                    try:\n",
    "                        tkr, dstat, m1stat, m5stat, err = fut.result(timeout=WORKER_TIMEOUT_S)\n",
    "                        all_rows.append({\"round\": r, \"ticker\": tkr, \"daily\": dstat, \"m1\": m1stat, \"m5\": m5stat, \"error\": err or \"\"})\n",
    "                        if err:\n",
    "                            ERRORS[\"other\"].append(f\"{tkr}: {err}\")\n",
    "                            all_errs.append((tkr, err, r))\n",
    "                            failed_this.append(tkr)\n",
    "                    except cf.TimeoutError:\n",
    "                        msg = f\"worker-timeout>{WORKER_TIMEOUT_S}s\"\n",
    "                        ERRORS[\"timeout\"].append(\"ticker_task\")\n",
    "                        all_rows.append({\"round\": r, \"ticker\": \"?\", \"daily\": \"\", \"m1\": \"\", \"m5\": \"\", \"error\": msg})\n",
    "                        all_errs.append((\"?\", msg, r))\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            _sleep_jitter(0.3, 0.8)\n",
    "        pbar.close()\n",
    "        remain = sorted(set(t for t in failed_this if t and t.endswith(\".JK\")))\n",
    "\n",
    "    tag = _now_tag_full()\n",
    "    if all_rows:\n",
    "        pd.DataFrame(all_rows).to_csv(RESULTS_DIR / f\"summary_parallel_{tag}.csv\", index=False)\n",
    "        print(f\"[SUMMARY CSV] result/summary_parallel_{tag}.csv\")\n",
    "    if all_errs:\n",
    "        pd.DataFrame(all_errs, columns=[\"Ticker\",\"Error\",\"Round\"]).drop_duplicates().to_csv(RESULTS_DIR / f\"errors_parallel_{tag}.csv\", index=False)\n",
    "        print(f\"[ERRORS CSV]  result/errors_parallel_{tag}.csv\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(\"\\n===== SUMMARY (parallel v1.3.3) =====\")\n",
    "    print(f\"Tickers total   : {STATS.get('tickers', len(tickers))}\")\n",
    "    print(f\"Daily   fetch/skip : {STATS.get('daily_fetch',0)}/{STATS.get('daily_skip',0)}\")\n",
    "    print(f\"1m      fetch/skip : {STATS.get('m1_fetch',0)}/{STATS.get('m1_skip',0)}  | fallbackâ†’5m: {STATS.get('m1_fallback_5m',0)}\")\n",
    "    print(f\"5m      fetch/skip : {STATS.get('m5_fetch',0)}/{STATS.get('m5_skip',0)}\")\n",
    "    print(f\"Errors  dns/missing/timeout/empty/other : \"\n",
    "          f\"{len(ERRORS['dns'])}/{len(ERRORS['missing'])}/{len(ERRORS['timeout'])}/{len(ERRORS['empty'])}/{len(ERRORS['other'])}\")\n",
    "    print(f\"Elapsed         : {elapsed:.2f}s\")\n",
    "    return all_rows, tag\n",
    "\n",
    "# ---------- ROBUST SANITY ----------\n",
    "def _safe_display(df):\n",
    "    try:\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        print(df.head(20).to_string())\n",
    "\n",
    "def _cols_ok(df):\n",
    "    if df is None or df.empty: return []\n",
    "    known = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"]\n",
    "    return [c for c in known if c in df.columns]\n",
    "\n",
    "def _age_days_safe(df):\n",
    "    if df is None or df.empty: return None\n",
    "    try:\n",
    "        last = df.index.max()\n",
    "        if pd.isna(last): return None\n",
    "        return int((pd.Timestamp.utcnow().normalize() - pd.Timestamp(last).normalize()).days)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _is_jkt_session(ts):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts)      # tz-safe\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5: return False\n",
    "    tm = t_jkt.time()\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n",
    "\n",
    "def _resample_1m_to_5m_safe(df_1m: pd.DataFrame):\n",
    "    if df_1m is None or df_1m.empty: return pd.DataFrame()\n",
    "    df = df_1m.copy()\n",
    "    agg = {}\n",
    "    if \"Open\" in df.columns:  agg[\"Open\"]  = \"first\"\n",
    "    if \"High\" in df.columns:  agg[\"High\"]  = \"max\"\n",
    "    if \"Low\" in df.columns:   agg[\"Low\"]   = \"min\"\n",
    "    if \"Close\" in df.columns: agg[\"Close\"] = \"last\"\n",
    "    if \"Volume\" in df.columns:agg[\"Volume\"]= \"sum\"\n",
    "    if \"Adj Close\" in df.columns: agg[\"Adj Close\"] = \"last\"\n",
    "    if not agg:\n",
    "        return pd.DataFrame()\n",
    "    out = df.resample(\"5min\", label=\"right\", closed=\"right\").agg(agg)\n",
    "    price_cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in out.columns]\n",
    "    if price_cols:\n",
    "        out = out.dropna(subset=price_cols, how=\"all\")\n",
    "    return _idx_naive(out)\n",
    "\n",
    "def run_sanity_random_robust(sample_size=SANITY_SAMPLE_SIZE, tol_price=TOL_PCT_PRICE, tol_vol=TOL_VOL_DIFF):\n",
    "    # Kumpulkan tickers dari cache yang ada\n",
    "    s = set()\n",
    "    for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR]:\n",
    "        if Path(d).exists():\n",
    "            for f in Path(d).glob(\"*.csv\"):\n",
    "                s.add(f.stem)\n",
    "    tickers = sorted(s)\n",
    "    if not tickers:\n",
    "        print(\"âš ï¸ Tidak ada file di cache_* untuk sanity.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sample = tickers if len(tickers) <= sample_size else random.sample(tickers, sample_size)\n",
    "\n",
    "    rows = []\n",
    "    for tkr in sample:\n",
    "        pD, p1, p5 = CACHE_DAILY_DIR/f\"{tkr}.csv\", CACHE_1M_DIR/f\"{tkr}.csv\", CACHE_5M_DIR/f\"{tkr}.csv\"\n",
    "        dfD, df1, df5 = _read_cache_csv(pD), _read_cache_csv(p1), _read_cache_csv(p5)\n",
    "\n",
    "        d_cols = _cols_ok(dfD)\n",
    "        d_dups = int(dfD.index.duplicated().sum()) if not dfD.empty else 0\n",
    "        d_mono = bool(dfD.index.is_monotonic_increasing) if not dfD.empty else True\n",
    "        d_last = str(dfD.index.max()) if not dfD.empty else \"\"\n",
    "        d_age  = _age_days_safe(dfD)\n",
    "        d_vol0 = int((dfD[\"Volume\"]==0).sum()) if (\"Volume\" in dfD.columns and not dfD.empty) else 0\n",
    "        d_na   = {c: float(dfD[c].isna().mean()) for c in dfD.columns} if not dfD.empty else {}\n",
    "\n",
    "        m1_cols = _cols_ok(df1)\n",
    "        m1_dups = int(df1.index.duplicated().sum()) if not df1.empty else 0\n",
    "        m1_mono = bool(df1.index.is_monotonic_increasing) if not df1.empty else True\n",
    "        m1_last = str(df1.index.max()) if not df1.empty else \"\"\n",
    "        m1_rows = len(df1)\n",
    "        m1_outside = int((~df1.index.to_series().map(_is_jkt_session)).sum()) if (CHECK_BEI_HOURS and not df1.empty) else 0\n",
    "        m1_vol0 = int((df1[\"Volume\"]==0).sum()) if (\"Volume\" in df1.columns and not df1.empty) else 0\n",
    "        m1_na   = {c: float(df1[c].isna().mean()) for c in df1.columns} if not df1.empty else {}\n",
    "\n",
    "        m5_cols = _cols_ok(df5)\n",
    "        m5_dups = int(df5.index.duplicated().sum()) if not df5.empty else 0\n",
    "        m5_mono = bool(df5.index.is_monotonic_increasing) if not df5.empty else True\n",
    "        m5_last = str(df5.index.max()) if not df5.empty else \"\"\n",
    "        m5_rows = len(df5)\n",
    "        m5_outside = int((~df5.index.to_series().map(_is_jkt_session)).sum()) if (CHECK_BEI_HOURS and not df5.empty) else 0\n",
    "        m5_vol0 = int((df5[\"Volume\"]==0).sum()) if (\"Volume\" in df5.columns and not df5.empty) else 0\n",
    "        m5_na   = {c: float(df5[c].isna().mean()) for c in df5.columns} if not df5.empty else {}\n",
    "\n",
    "        # Konsistensi 1m->5m (hari terakhir)\n",
    "        res_ok, mism_price, mism_vol = \"\", None, None\n",
    "        if not df1.empty and not df5.empty:\n",
    "            try:\n",
    "                last_day = pd.Timestamp(df1.index.max()).normalize()\n",
    "                d1 = df1.loc[df1.index.normalize() == last_day]\n",
    "                r5 = _resample_1m_to_5m_safe(d1)\n",
    "                d5 = df5.loc[df5.index.normalize() == last_day]\n",
    "                idx = r5.index.intersection(d5.index)\n",
    "                if not idx.empty:\n",
    "                    r5i, d5i = r5.loc[idx], d5.loc[idx]\n",
    "                    mism_price = False\n",
    "                    for col in [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in r5i.columns and c in d5i.columns]:\n",
    "                        base = d5i[col].replace(0, np.nan).astype(float)\n",
    "                        diff = (r5i[col].astype(float) - d5i[col].astype(float)).abs() / base\n",
    "                        if diff.dropna().gt(tol_price).any():\n",
    "                            mism_price = True; break\n",
    "                    mism_vol = False\n",
    "                    if \"Volume\" in r5i.columns and \"Volume\" in d5i.columns:\n",
    "                        basev = d5i[\"Volume\"].replace(0, np.nan).astype(float)\n",
    "                        diffv = (r5i[\"Volume\"].astype(float) - d5i[\"Volume\"].astype(float)).abs() / basev\n",
    "                        if diffv.dropna().gt(tol_vol).any():\n",
    "                            mism_vol = True\n",
    "                    res_ok = \"OK\" if (not mism_price and not mism_vol) else \"MISMATCH\"\n",
    "                else:\n",
    "                    res_ok = \"NO_OVERLAP\"\n",
    "            except Exception as e:\n",
    "                res_ok = f\"CHECK_ERROR: {e}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": tkr,\n",
    "            \"daily_rows\": len(dfD), \"daily_last\": d_last, \"daily_age_days\": d_age,\n",
    "            \"daily_cols\": \",\".join(d_cols), \"daily_monotonic\": d_mono, \"daily_dups\": d_dups,\n",
    "            \"daily_vol_zero\": d_vol0, \"daily_na_%\": round(sum(d_na.values())/max(1,len(d_na))*100,3) if d_na else None,\n",
    "            \"m1_rows\": m1_rows, \"m1_last\": m1_last, \"m1_cols\": \",\".join(m1_cols), \"m1_monotonic\": m1_mono,\n",
    "            \"m1_dups\": m1_dups, \"m1_outside_session\": m1_outside, \"m1_vol_zero\": m1_vol0,\n",
    "            \"m1_na_%\": round(sum(m1_na.values())/max(1,len(m1_na))*100,3) if m1_na else None,\n",
    "            \"m5_rows\": m5_rows, \"m5_last\": m5_last, \"m5_cols\": \",\".join(m5_cols), \"m5_monotonic\": m5_mono,\n",
    "            \"m5_dups\": m5_dups, \"m5_outside_session\": m5_outside, \"m5_vol_zero\": m5_vol0,\n",
    "            \"m5_na_%\": round(sum(m5_na.values())/max(1,len(m5_na))*100,3) if m5_na else None,\n",
    "            \"consistency_1m_to_5m\": res_ok, \"price_mismatch\": mism_price, \"vol_mismatch\": mism_vol,\n",
    "        })\n",
    "\n",
    "    df_report = pd.DataFrame(rows)\n",
    "    order = [\n",
    "        \"ticker\",\n",
    "        \"daily_rows\",\"daily_last\",\"daily_age_days\",\"daily_cols\",\"daily_monotonic\",\"daily_dups\",\"daily_vol_zero\",\"daily_na_%\",\n",
    "        \"m1_rows\",\"m1_last\",\"m1_cols\",\"m1_monotonic\",\"m1_dups\",\"m1_outside_session\",\"m1_vol_zero\",\"m1_na_%\",\n",
    "        \"m5_rows\",\"m5_last\",\"m5_cols\",\"m5_monotonic\",\"m5_dups\",\"m5_outside_session\",\"m5_vol_zero\",\"m5_na_%\",\n",
    "        \"consistency_1m_to_5m\",\"price_mismatch\",\"vol_mismatch\"\n",
    "    ]\n",
    "    df_report = df_report.reindex(columns=[c for c in order if c in df_report.columns])\n",
    "    out = RESULTS_DIR / f\"sanity_report_{_now_tag_full()}.csv\"\n",
    "    df_report.to_csv(out, index=False)\n",
    "    _safe_display(df_report)\n",
    "    print(f\"\\nâœ… Sanity report saved â†’ {out}\")\n",
    "    return df_report\n",
    "\n",
    "# ---------- WRAPPER EKSEKUSI DENGAN LOGGING ----------\n",
    "def run_all_with_logging():\n",
    "    # 1) Warmup (opsional)\n",
    "    if DO_WARMUP:\n",
    "        try:\n",
    "            tks = _infer_tickers()\n",
    "            STATS[\"tickers\"] = len(tks)\n",
    "            print(f\"Parallel warmup v1.3.3 â€¢ candidates={len(tks)}\")\n",
    "            _ = run_parallel_warmup_v133(tks)\n",
    "        except Exception as e:\n",
    "            logp = RESULTS_DIR / f\"error_runtime_warmup_{_now_tag_full()}.log\"\n",
    "            with open(logp, \"w\") as f:\n",
    "                f.write(\"WARMUP ERROR\\n\")\n",
    "                traceback.print_exc(file=f)\n",
    "            print(f\"âš ï¸ Warmup error tertangkap. Log â†’ {logp}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Warmup SKIPPED (DO_WARMUP=False).\")\n",
    "\n",
    "    # 2) Sanity (opsional)\n",
    "    if SANITY_ENABLE:\n",
    "        try:\n",
    "            print(\"\\n=== SANITY (random 10) ===\")\n",
    "            _ = run_sanity_random_robust(sample_size=SANITY_SAMPLE_SIZE)\n",
    "        except Exception as e:\n",
    "            logp = RESULTS_DIR / f\"error_runtime_sanity_{_now_tag_full()}.log\"\n",
    "            with open(logp, \"w\") as f:\n",
    "                f.write(\"SANITY ERROR\\n\")\n",
    "                traceback.print_exc(file=f)\n",
    "            print(f\"âš ï¸ Sanity error tertangkap. Log â†’ {logp}\")\n",
    "    else:\n",
    "        print(\"Sanity check dimatikan (SANITY_ENABLE=False).\")\n",
    "\n",
    "# --- RUN ONCE ---\n",
    "run_all_with_logging()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213750df",
   "metadata": {},
   "source": [
    "### v.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# v1.4 â€” ALWAYS-FRESH QUICK PATH + 15m FALLBACK\n",
    "# Warmup (parallel, tz-safe, retry, batch) + Robust Sanity + Toggles\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "MANUAL (v1.4)\n",
    "=================\n",
    "Goal: cache selalu fresh dengan performa hemat:\n",
    "1) Quick Daily Scan (global):\n",
    "   - Sebelum loop utama, kita hanya cek baris terakhir CSV harian untuk SEMUA emiten.\n",
    "   - Jika baris terakhir sudah menyentuh \"hari ini\" (zona Jakarta), daily untuk emiten tsb di-skip.\n",
    "   - Hanya emiten yang ketinggalan daily yang di-fetch incremental. (Hemat API call & I/O)\n",
    "\n",
    "2) Always-Fresh Intraday + Early-Exit:\n",
    "   - Di tiap emiten, sebelum panggil yfinance intraday, kita baca timestamp terakhir di cache 1m/5m/15m.\n",
    "   - Jika ketiganya masih dalam batas \"freshness slack\" (konfigurable), langsung early-exit (skip fetch).\n",
    "\n",
    "3) Rantai Fallback Intraday: 1m â†’ 5m â†’ 15m\n",
    "   - get_1m_incremental sudah otomatis fallback ke 5m saat 1m kosong.\n",
    "   - Jika 5m juga kosong/failed, v1.4 akan mencoba 15m sebagai fallback terakhir.\n",
    "   - Statistik fallback 5mâ†’15m dilacak di STATS[\"m5_fallback_15m\"].\n",
    "\n",
    "4) Retensi & Rollup:\n",
    "   - Cache 1m diringkas otomatis ke 5m untuk data yang lebih tua dari WINDOW 1m (hemat ukuran file).\n",
    "   - Cache 5m dibatasi WINDOW 5m. (Sesuai v1.3.3, tetap berlaku.)\n",
    "\n",
    "Cara Pakai\n",
    "---------\n",
    "- Pastikan dependencies: `pip install yfinance pandas requests tqdm pytz`\n",
    "- Letakkan file ini sebagai skrip harian (cron) di server kamu.\n",
    "- Jalankan langsung: `python warmup_sanity_v1.4.py`\n",
    "- Konfigurasi penting ada di blok CONFIG:\n",
    "    * ALWAYS_FRESH_MODE / QUICK_DAILY_CHECK / EARLY_EXIT_IF_INTRADAY_FRESH\n",
    "    * INTRADAY_*_FRESH_SLACK_MIN (1m/5m/15m)\n",
    "    * CACHE_*_WINDOW_D (retensi)\n",
    "    * OFF_HOURS_SKIP_INTRADAY (skip intraday di luar jam bursa JKT)\n",
    "\n",
    "Catatan\n",
    "-------\n",
    "- v1.4 mempertahankan semua fitur v1.3.3 (parallel runner, retry/backoff, rollup 1mâ†’5m, sanity robust),\n",
    "  sambil menambahkan quick path & 15m fallback. Struktur fungsi inti v1.3.3 tetap diacu agar kompatibel.\n",
    "- Untuk konteks implementasi v1.3.3 (baseline), lihat komentar sumber di akhir file.\n",
    "\"\"\"\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "from pathlib import Path\n",
    "import os, time, random, socket, warnings, math, traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Toggles\n",
    "DO_WARMUP        = True    # set False untuk skip warmup dan langsung sanity\n",
    "SANITY_ENABLE    = True    # set False untuk hanya warmup\n",
    "\n",
    "# Always-fresh toggles (baru v1.4)\n",
    "ALWAYS_FRESH_MODE              = True\n",
    "QUICK_DAILY_CHECK              = True   # scan semua emiten: fetch hanya yang stale\n",
    "EARLY_EXIT_IF_INTRADAY_FRESH   = False   # jika 1m/5m/15m masih fresh â†’ skip yfinance\n",
    "\n",
    "# Paths\n",
    "BASE_DIR        = Path(\".\")\n",
    "EMITEN_DIR      = BASE_DIR / \"emiten\"\n",
    "CACHE_DAILY_DIR = EMITEN_DIR / \"cache_daily\"\n",
    "CACHE_1M_DIR    = EMITEN_DIR / \"cache_1m\"\n",
    "CACHE_5M_DIR    = EMITEN_DIR / \"cache_5m\"\n",
    "CACHE_15M_DIR   = EMITEN_DIR / \"cache_15m\"   # baru v1.4\n",
    "RESULTS_DIR     = BASE_DIR / \"result\"\n",
    "ROSTER_PATH     = None  # ex: \"emiten/candidates_active_filtered_20250813.csv\" (opsional)\n",
    "\n",
    "# Retensi / freshness\n",
    "DAILY_BOOTSTRAP_Y               = 10\n",
    "CACHE_1M_WINDOW_D               = 7\n",
    "CACHE_5M_WINDOW_D               = 12\n",
    "INTRADAY_1M_FRESH_SLACK_MIN     = 0\n",
    "INTRADAY_5M_FRESH_SLACK_MIN     = 0\n",
    "INTRADAY_15M_FRESH_SLACK_MIN    = 0    # baru v1.4\n",
    "FILTER_SESSION_WINDOWS          = False  # True: simpan hanya 09:00â€“09:30 & 14:30â€“15:00 WIB\n",
    "\n",
    "# yfinance retry/backoff\n",
    "MAX_RETRIES   = 4\n",
    "BACKOFF_S     = [0.6, 1.5, 3.0, 6.0]\n",
    "\n",
    "# Parallel runner\n",
    "MAX_WORKERS              = 6\n",
    "WORKER_TIMEOUT_S         = 22\n",
    "SUBMIT_BATCH             = 128\n",
    "MAX_RETRY_ROUNDS         = 2\n",
    "YF_PAUSE_JITTER_S        = (0.08, 0.30)\n",
    "OFF_HOURS_SKIP_INTRADAY  = False\n",
    "DESC_LABEL               = \"Warmup(parallel v1.4)\"\n",
    "\n",
    "# Sanity\n",
    "SANITY_SAMPLE_SIZE       = 10\n",
    "CHECK_BEI_HOURS          = True\n",
    "TOL_PCT_PRICE            = 0.001  # 0.1%\n",
    "TOL_VOL_DIFF             = 0.02   # 2%\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------- DEPENDENCIES ----------\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install dulu: pip install yfinance pandas requests tqdm pytz\") from e\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _iter_progress(it, total, desc): return tqdm(it, total=total, ncols=90, desc=desc)\n",
    "except Exception:\n",
    "    def _iter_progress(it, total, desc): return it\n",
    "\n",
    "# ---------- GLOBAL STATE ----------\n",
    "ERRORS = {\"dns\": [], \"missing\": [], \"timeout\": [], \"empty\": [], \"other\": []}\n",
    "STATS  = {\n",
    "    \"tickers\": 0,\n",
    "    \"daily_fetch\": 0, \"daily_skip\": 0,\n",
    "    \"m1_fetch\": 0, \"m1_skip\": 0, \"m1_fallback_5m\": 0,\n",
    "    \"m5_fetch\": 0, \"m5_skip\": 0, \"m5_fallback_15m\": 0,  # baru v1.4\n",
    "    \"m15_fetch\": 0, \"m15_skip\": 0,\n",
    "}\n",
    "\n",
    "# ensure dirs\n",
    "for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR, CACHE_15M_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- TIME HELPERS (tz-safe) ----------\n",
    "def _ensure_aware_utc(ts=None):\n",
    "    \"\"\"Return UTC-aware Timestamp regardless of input being naive/aware/None.\"\"\"\n",
    "    t = pd.Timestamp.utcnow() if ts is None else pd.Timestamp(ts)\n",
    "    if t.tz is None:\n",
    "        return t.tz_localize(\"UTC\")\n",
    "    return t.tz_convert(\"UTC\")\n",
    "\n",
    "def _naive(ts):\n",
    "    t = pd.Timestamp(ts)\n",
    "    if t.tz is None:\n",
    "        return t\n",
    "    return t.tz_convert(\"UTC\").tz_localize(None)\n",
    "\n",
    "\n",
    "def _now_utc():   return _naive(pd.Timestamp.utcnow())\n",
    "def _today_utc(): return _now_utc().normalize()\n",
    "\n",
    "\n",
    "def _now_date_tag():\n",
    "    try:\n",
    "        import pytz\n",
    "        tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "        return pd.Timestamp.now(tz).strftime(\"%Y%m%d\")\n",
    "    except Exception:\n",
    "        return pd.Timestamp.utcnow().strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "def _now_tag_full():\n",
    "    try:\n",
    "        import pytz\n",
    "        tz = pytz.timezone(\"Asia/Jakarta\")\n",
    "        return pd.Timestamp.now(tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    except Exception:\n",
    "        return pd.Timestamp.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ---------- IO & SANITIZE ----------\n",
    "def _idx_naive(df: pd.DataFrame):\n",
    "    if df is None or len(df)==0: return df\n",
    "    out = df.copy()\n",
    "    if not isinstance(out.index, pd.DatetimeIndex):\n",
    "        out.index = pd.to_datetime(out.index, errors=\"coerce\", utc=True).tz_convert(\"UTC\").tz_localize(None)\n",
    "    elif out.index.tz is not None:\n",
    "        out.index = out.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "    return out[~out.index.duplicated(keep=\"last\")].sort_index()\n",
    "\n",
    "\n",
    "def _sanitize_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame()\n",
    "    out = _idx_naive(df)\n",
    "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"] if c in out.columns]\n",
    "    if not keep: keep = list(out.columns)\n",
    "    out = out[keep].dropna(how=\"all\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _read_cache_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists(): return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[0], index_col=0)\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            if \"Date\" in df.columns: df = df.set_index(\"Date\")\n",
    "        except Exception:\n",
    "            return pd.DataFrame()\n",
    "    return _sanitize_ohlcv(df)\n",
    "\n",
    "\n",
    "def _resample_to_5m(df_1m: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_1m.empty: return df_1m\n",
    "    df = _idx_naive(df_1m)\n",
    "    agg = {\"Open\":\"first\",\"High\":\"max\",\"Low\":\"min\",\"Close\":\"last\",\"Volume\":\"sum\"}\n",
    "    if \"Adj Close\" in df.columns: agg[\"Adj Close\"] = \"last\"\n",
    "    out = df.resample(\"5min\", label=\"right\", closed=\"right\").agg(agg)\n",
    "    ohlc = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in out.columns]\n",
    "    out = out.dropna(subset=ohlc, how=\"all\")\n",
    "    return _sanitize_ohlcv(out)\n",
    "\n",
    "\n",
    "def _write_cache_csv(path: Path, df_new: pd.DataFrame, window_days: int|None=None):\n",
    "    p = Path(path); p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_new = _sanitize_ohlcv(df_new)\n",
    "    if p.exists():\n",
    "        df_old = _read_cache_csv(p)\n",
    "        df = pd.concat([df_old, df_new]).sort_index()\n",
    "        df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    else:\n",
    "        df = df_new\n",
    "\n",
    "    now = _now_utc()\n",
    "    path_lower = str(p).lower().replace(\"\\\\\",\"/\")\n",
    "    is_1m   = \"cache_1m\" in path_lower\n",
    "    is_5m   = \"cache_5m\" in path_lower\n",
    "    is_15m  = \"cache_15m\" in path_lower\n",
    "    is_daily= \"cache_daily\" in path_lower\n",
    "\n",
    "    # session windows filter (opsional)\n",
    "    if FILTER_SESSION_WINDOWS and (is_1m or is_5m or is_15m) and not df.empty:\n",
    "        import pytz\n",
    "        jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "        dfl = df.copy()\n",
    "        dfl.index = dfl.index.tz_localize(\"UTC\").tz_convert(jkt)\n",
    "        mask = (\n",
    "            ((dfl.index.time >= pd.Timestamp(\"09:00\").time()) & (dfl.index.time < pd.Timestamp(\"09:30\").time())) |\n",
    "            ((dfl.index.time >= pd.Timestamp(\"14:30\").time()) & (dfl.index.time <= pd.Timestamp(\"15:00\").time()))\n",
    "        )\n",
    "        dfl = dfl[mask]\n",
    "        dfl.index = dfl.index.tz_convert(\"UTC\").tz_localize(None)\n",
    "        df = dfl\n",
    "\n",
    "    if is_1m:\n",
    "        cutoff_1m = now - pd.Timedelta(days=window_days or CACHE_1M_WINDOW_D)\n",
    "        older = df[df.index < cutoff_1m]\n",
    "        tail  = df[df.index >= cutoff_1m]\n",
    "        if not older.empty:\n",
    "            df5_add = _resample_to_5m(older)\n",
    "            p5 = CACHE_5M_DIR / p.name\n",
    "            if p5.exists(): df5_old = _read_cache_csv(p5)\n",
    "            else: df5_old = pd.DataFrame()\n",
    "            df5 = pd.concat([df5_old, df5_add]).sort_index()\n",
    "            df5 = df5[~df5.index.duplicated(keep=\"last\")]\n",
    "            cutoff_5m = now - pd.Timedelta(days=CACHE_5M_WINDOW_D)\n",
    "            df5 = df5[df5.index >= cutoff_5m]\n",
    "            df5.to_csv(p5)\n",
    "        tail.to_csv(p); return\n",
    "\n",
    "    if is_5m:\n",
    "        keep_days = window_days or CACHE_5M_WINDOW_D\n",
    "        cutoff_5m = now - pd.Timedelta(days=keep_days)\n",
    "        df = df[df.index >= cutoff_5m]\n",
    "        df.to_csv(p); return\n",
    "\n",
    "    if is_15m:\n",
    "        # Ikuti window 5m (atau atur sendiri bila perlu)\n",
    "        keep_days = window_days or CACHE_5M_WINDOW_D\n",
    "        cutoff_15m = now - pd.Timedelta(days=keep_days)\n",
    "        df = df[df.index >= cutoff_15m]\n",
    "        df.to_csv(p); return\n",
    "\n",
    "    if is_daily:\n",
    "        if window_days:\n",
    "            cutoff_d = now.normalize() - pd.Timedelta(days=window_days)\n",
    "            df = df[df.index >= cutoff_d]\n",
    "        df.to_csv(p); return\n",
    "\n",
    "    df.to_csv(p)\n",
    "\n",
    "# ---------- FAST TAIL READER (untuk quick check) ----------\n",
    "\n",
    "def _read_last_ts_csv(path: Path):\n",
    "    \"\"\"Baca timestamp baris terakhir dari CSV tanpa load full file. Return naive UTC Timestamp atau None.\"\"\"\n",
    "    try:\n",
    "        if not path.exists() or path.stat().st_size == 0:\n",
    "            return None\n",
    "        with open(path, \"rb\") as f:\n",
    "            f.seek(0, 2)\n",
    "            end = f.tell()\n",
    "            size = min(4096, end)\n",
    "            f.seek(end - size)\n",
    "            chunk = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        lines = [ln for ln in chunk.strip().splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            return None\n",
    "        last = lines[-1]\n",
    "        if last.lower().startswith((\"date,\", \"index,\")):\n",
    "            last = lines[-2] if len(lines) >= 2 else None\n",
    "        if not last:\n",
    "            return None\n",
    "        first_field = last.split(\",\")[0].strip()\n",
    "        ts = pd.to_datetime(first_field, errors=\"coerce\", utc=True)\n",
    "        if ts is pd.NaT:\n",
    "            return None\n",
    "        return ts.tz_convert(\"UTC\").tz_localize(None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _jkt_today_utc_naive():\n",
    "    import pytz\n",
    "    jkt = pd.Timestamp.now(pytz.timezone(\"Asia/Jakarta\")).normalize()\n",
    "    return jkt.tz_convert(\"UTC\").tz_localize(None)\n",
    "\n",
    "\n",
    "# ---------- yfinance via requests.Session (no curl) ----------\n",
    "os.environ[\"YF_USE_CURL\"] = \"0\"\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "_YF_SESSION = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5, connect=5, read=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=(429,500,502,503,504),\n",
    "    allowed_methods=False\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)\n",
    "_YF_SESSION.mount(\"https://\", adapter)\n",
    "_YF_SESSION.mount(\"http://\", adapter)\n",
    "_YF_SESSION.headers.update({\"User-Agent\": \"Mozilla/5.0 (warmup-service)\"})\n",
    "import importlib; importlib.reload(yf)\n",
    "\n",
    "\n",
    "def yfdl(ticker, **kw):\n",
    "    if \"start\" in kw and kw[\"start\"] is not None: kw[\"start\"] = _naive(kw[\"start\"]).to_pydatetime()\n",
    "    if \"end\"   in kw and kw[\"end\"]   is not None: kw[\"end\"]   = _naive(kw[\"end\"]).to_pydatetime()\n",
    "    kw.pop(\"threads\", None)\n",
    "    kw[\"progress\"]    = False\n",
    "    kw[\"auto_adjust\"] = False\n",
    "    kw[\"actions\"]     = False\n",
    "    kw[\"group_by\"]    = \"column\"\n",
    "    kw[\"session\"]     = _YF_SESSION\n",
    "    kw.setdefault(\"timeout\", 20)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            try: socket.gethostbyname(\"query2.finance.yahoo.com\")\n",
    "            except Exception: pass\n",
    "            df = yf.download(ticker, **kw)\n",
    "            df = _sanitize_ohlcv(df)\n",
    "            if df is not None and not df.empty:\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            low = str(e).lower()\n",
    "            if \"could not resolve host\" in low or \"temporary failure in name resolution\" in low:\n",
    "                ERRORS[\"dns\"].append(ticker); break\n",
    "            elif \"timed out\" in low:\n",
    "                ERRORS[\"timeout\"].append(ticker)\n",
    "            elif \"no price data found\" in low or \"possibly delisted\" in low:\n",
    "                ERRORS[\"missing\"].append(ticker); return pd.DataFrame()\n",
    "            else:\n",
    "                ERRORS[\"other\"].append(f\"{ticker}: {e}\")\n",
    "        time.sleep(BACKOFF_S[min(attempt, len(BACKOFF_S)-1)] + random.random()*0.8)\n",
    "    ERRORS[\"empty\"].append(ticker)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ---------- FRESH CHECK ----------\n",
    "\n",
    "def _fresh_enough(last_ts, now_ts, slack_min):\n",
    "    try:\n",
    "        return _naive(last_ts) >= (_naive(now_ts) - pd.Timedelta(minutes=slack_min))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ---------- INCREMENTAL FETCHERS ----------\n",
    "\n",
    "def get_daily_incremental(ticker: str):\n",
    "    path = CACHE_DAILY_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    today = _today_utc()\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{DAILY_BOOTSTRAP_Y}y\", interval=\"1d\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=None)\n",
    "        STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_date = _naive(base.index.max()).normalize()\n",
    "    start, end = _naive(last_date + pd.Timedelta(days=1)), _naive(today + pd.Timedelta(days=1))\n",
    "    if start > end:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    fresh = yfdl(ticker, start=start, end=end, interval=\"1d\")\n",
    "    if fresh.empty:\n",
    "        STATS[\"daily_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=None)\n",
    "    STATS[\"daily_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "\n",
    "def get_1m_incremental(ticker: str):\n",
    "    path = CACHE_1M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_1M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_1M_WINDOW_D}d\", interval=\"1m\")\n",
    "        if fresh.empty:\n",
    "            df5, _ = get_5m_incremental(ticker)\n",
    "            STATS[\"m1_fallback_5m\"] += 1; return df5, \"fallback(5m)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_1M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=1), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"1m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_1M_WINDOW_D)\n",
    "        STATS[\"m1_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_1M_WINDOW_D)\n",
    "    STATS[\"m1_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "\n",
    "def get_5m_incremental(ticker: str):\n",
    "    path = CACHE_5M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    hzn  = _naive(now - pd.Timedelta(days=CACHE_5M_WINDOW_D))\n",
    "    if base.empty:\n",
    "        fresh = yfdl(ticker, period=f\"{CACHE_5M_WINDOW_D}d\", interval=\"5m\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_5M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    start = max(last_ts + pd.Timedelta(minutes=5), hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"5m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m5_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "    STATS[\"m5_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "\n",
    "def get_15m_incremental(ticker: str):\n",
    "    \"\"\"Ambil/merge interval 15m ke CACHE_15M_DIR/<ticker>.csv.\"\"\"\n",
    "    path = CACHE_15M_DIR / f\"{ticker}.csv\"\n",
    "    base = _read_cache_csv(path)\n",
    "    now  = _now_utc().floor(\"min\")\n",
    "    # Ambil horizon default 30d jika kosong\n",
    "    if base.empty:\n",
    "        start = _naive(now - pd.Timedelta(days=30))\n",
    "        fresh = yfdl(ticker, start=start, end=_naive(now), interval=\"15m\")\n",
    "        if fresh.empty:\n",
    "            STATS[\"m15_skip\"] += 1; return base, \"skip(empty)\"\n",
    "        _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m15_fetch\"] += 1; return _read_cache_csv(path), \"fetch(bootstrap)\"\n",
    "    last_ts = _naive(base.index.max())\n",
    "    if _fresh_enough(last_ts, now, INTRADAY_15M_FRESH_SLACK_MIN):\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m15_skip\"] += 1; return base, \"skip(fresh)\"\n",
    "    # overlap sedikit\n",
    "    start = last_ts + pd.Timedelta(minutes=15)\n",
    "    hzn   = _naive(now - pd.Timedelta(days=CACHE_5M_WINDOW_D))\n",
    "    start = max(start, hzn)\n",
    "    fresh = yfdl(ticker, start=_naive(start), end=_naive(now), interval=\"15m\")\n",
    "    if fresh.empty:\n",
    "        _write_cache_csv(path, base, window_days=CACHE_5M_WINDOW_D)\n",
    "        STATS[\"m15_skip\"] += 1; return base, \"skip(empty)\"\n",
    "    _write_cache_csv(path, fresh, window_days=CACHE_5M_WINDOW_D)\n",
    "    STATS[\"m15_fetch\"] += 1; return _read_cache_csv(path), \"fetch(incremental)\"\n",
    "\n",
    "\n",
    "# ---------- TICKER SOURCE ----------\n",
    "\n",
    "def _detect_roster_path():\n",
    "    cand = sorted(EMITEN_DIR.glob(\"candidates_active_filtered_*.csv\"))\n",
    "    return str(cand[-1]) if cand else None\n",
    "\n",
    "\n",
    "def _infer_tickers():\n",
    "    roster = ROSTER_PATH or _detect_roster_path()\n",
    "    if roster and Path(roster).exists():\n",
    "        df = pd.read_csv(roster)\n",
    "        for c in df.columns:\n",
    "            if c.lower() in (\"ticker\",\"symbol\",\"kode\",\"emiten\"):\n",
    "                return df[c].astype(str).str.strip().tolist()\n",
    "        return df.iloc[:,0].astype(str).str.strip().tolist()\n",
    "    s = set()\n",
    "    for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR, CACHE_15M_DIR]:\n",
    "        if Path(d).exists():\n",
    "            for f in Path(d).glob(\"*.csv\"): s.add(f.stem)\n",
    "    return sorted(s)\n",
    "\n",
    "\n",
    "# ---------- MARKET-TIME GUARD (TZ-SAFE) ----------\n",
    "\n",
    "def _is_jkt_market_time(ts_utc=None):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts_utc)        # tz-safe\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5: return False\n",
    "    tm = t_jkt.time()\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n",
    "\n",
    "\n",
    "def _prewarm_dns():\n",
    "    for host in (\"query1.finance.yahoo.com\",\"query2.finance.yahoo.com\"):\n",
    "        try: socket.gethostbyname(host)\n",
    "        except: pass\n",
    "\n",
    "\n",
    "def _sleep_jitter(a,b): time.sleep(random.uniform(a,b))\n",
    "\n",
    "def _stat_label(x): return (str(x[1]) if isinstance(x, tuple) and len(x)==2 else \"(ok)\")\n",
    "\n",
    "\n",
    "# ---------- QUICK DAILY SCAN (global) ----------\n",
    "FRESH_DAILY_SET = set()\n",
    "STALE_DAILY_SET = set()\n",
    "\n",
    "\n",
    "def scan_daily_freshness(all_tickers: list[str]) -> tuple[set[str], set[str]]:\n",
    "    fresh, stale = set(), set()\n",
    "    today_utc = _jkt_today_utc_naive()\n",
    "    for t in all_tickers:\n",
    "        p = CACHE_DAILY_DIR / f\"{t}.csv\"\n",
    "        last = _read_last_ts_csv(p)\n",
    "        if last is not None and pd.Timestamp(last).normalize() >= today_utc:\n",
    "            fresh.add(t)\n",
    "        else:\n",
    "            stale.add(t)\n",
    "    return fresh, stale\n",
    "\n",
    "\n",
    "# ---------- PARALLEL RUNNER v1.4 ----------\n",
    "import concurrent.futures as cf\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _tqdm(x, **k): return tqdm(x, **k)\n",
    "except Exception:\n",
    "    def _tqdm(x, **k): return x\n",
    "\n",
    "\n",
    "def _warmup_one_ticker(ticker: str, do_intraday: bool):\n",
    "    try:\n",
    "        # === DAILY (quick-scan aware) ===\n",
    "        if ALWAYS_FRESH_MODE and QUICK_DAILY_CHECK and ticker in FRESH_DAILY_SET:\n",
    "            dstat = \"skip(daily-fresh-quick)\"\n",
    "            d = (None, dstat)\n",
    "        else:\n",
    "            d = get_daily_incremental(ticker)\n",
    "        _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "\n",
    "        # === INTRADAY EARLY-EXIT ===\n",
    "        if do_intraday and ALWAYS_FRESH_MODE and EARLY_EXIT_IF_INTRADAY_FRESH:\n",
    "            now = _now_utc().floor(\"min\")\n",
    "            p1 = CACHE_1M_DIR / f\"{ticker}.csv\"\n",
    "            p5 = CACHE_5M_DIR / f\"{ticker}.csv\"\n",
    "            p15= CACHE_15M_DIR / f\"{ticker}.csv\"\n",
    "            last_1  = _read_last_ts_csv(p1)\n",
    "            last_5  = _read_last_ts_csv(p5)\n",
    "            last_15 = _read_last_ts_csv(p15)\n",
    "            m1_ok  = last_1  is not None and _fresh_enough(last_1,  now, INTRADAY_1M_FRESH_SLACK_MIN)\n",
    "            m5_ok  = last_5  is not None and _fresh_enough(last_5,  now, INTRADAY_5M_FRESH_SLACK_MIN)\n",
    "            m15_ok = last_15 is not None and _fresh_enough(last_15, now, INTRADAY_15M_FRESH_SLACK_MIN)\n",
    "            if m1_ok and m5_ok and m15_ok:\n",
    "                return (ticker, _stat_label(d), \"skip(fresh-quick)\", \"skip(fresh-quick)\", \"skip(fresh-quick)\", None)\n",
    "\n",
    "        # === INTRADAY ===\n",
    "        if do_intraday:\n",
    "            m1_df, m1_stat = get_1m_incremental(ticker)\n",
    "            _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "            m5_df, m5_stat = get_5m_incremental(ticker)\n",
    "\n",
    "            # 15m fallback jika 1m & 5m tidak memberikan data berguna\n",
    "            need_15m = (\n",
    "                (m1_df is None or getattr(m1_df, \"empty\", False)) and\n",
    "                (m5_df is None or getattr(m5_df, \"empty\", False) or \"skip(empty)\" in str(m5_stat).lower())\n",
    "            )\n",
    "            if need_15m:\n",
    "                STATS[\"m5_fallback_15m\"] += 1\n",
    "                _sleep_jitter(*YF_PAUSE_JITTER_S)\n",
    "                m15_df, m15_stat = get_15m_incremental(ticker)\n",
    "            else:\n",
    "                m15_df, m15_stat = None, \"skip(15m-not-needed)\"\n",
    "        else:\n",
    "            m1_stat = \"(skipped-offhours)\"; m5_stat = \"(skipped-offhours)\"; m15_stat = \"(skipped-offhours)\"\n",
    "\n",
    "        return (ticker, _stat_label(d), str(m1_stat), str(m5_stat), str(m15_stat), None)\n",
    "    except Exception as e:\n",
    "        return (ticker, None, None, None, None, str(e))\n",
    "\n",
    "\n",
    "def run_parallel_warmup_v14(tickers):\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    all_rows, all_errs = [], []\n",
    "    rounds = MAX_RETRY_ROUNDS + 1\n",
    "    remain = list(tickers)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for r in range(rounds):\n",
    "        if not remain: break\n",
    "        do_intraday = (not OFF_HOURS_SKIP_INTRADAY) or _is_jkt_market_time()\n",
    "        label = f\"{DESC_LABEL} R{r}{' (intraday)' if do_intraday else ' (daily-only)'}\"\n",
    "        print(f\"\\nRound {r+1}/{rounds} â€¢ tickers={len(remain)} â€¢ workers={MAX_WORKERS} â€¢ timeout={WORKER_TIMEOUT_S}s â€¢ {('INTRA ON' if do_intraday else 'INTRA OFF')}\")\n",
    "        _prewarm_dns()\n",
    "\n",
    "        failed_this = []\n",
    "        pbar = tqdm(total=len(remain), ncols=90, desc=label)\n",
    "\n",
    "        batches = math.ceil(len(remain)/SUBMIT_BATCH)\n",
    "        idx = 0\n",
    "        for b in range(batches):\n",
    "            batch = remain[idx: idx+SUBMIT_BATCH]; idx += SUBMIT_BATCH\n",
    "            futs = []\n",
    "            with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "                for tkr in batch:\n",
    "                    futs.append(ex.submit(_warmup_one_ticker, tkr, do_intraday))\n",
    "                for fut in futs:\n",
    "                    try:\n",
    "                        tkr, dstat, m1stat, m5stat, m15stat, err = fut.result(timeout=WORKER_TIMEOUT_S)\n",
    "                        all_rows.append({\"round\": r, \"ticker\": tkr, \"daily\": dstat, \"m1\": m1stat, \"m5\": m5stat, \"m15\": m15stat, \"error\": err or \"\"})\n",
    "                        if err:\n",
    "                            ERRORS[\"other\"].append(f\"{tkr}: {err}\")\n",
    "                            all_errs.append((tkr, err, r))\n",
    "                            failed_this.append(tkr)\n",
    "                    except cf.TimeoutError:\n",
    "                        msg = f\"worker-timeout>{WORKER_TIMEOUT_S}s\"\n",
    "                        ERRORS[\"timeout\"].append(\"ticker_task\")\n",
    "                        all_rows.append({\"round\": r, \"ticker\": \"?\", \"daily\": \"\", \"m1\": \"\", \"m5\": \"\", \"m15\": \"\", \"error\": msg})\n",
    "                        all_errs.append((\"?\", msg, r))\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "            _sleep_jitter(0.3, 0.8)\n",
    "        pbar.close()\n",
    "        remain = sorted(set(t for t in failed_this if t and t.endswith(\".JK\")))\n",
    "\n",
    "    tag = _now_tag_full()\n",
    "    if all_rows:\n",
    "        pd.DataFrame(all_rows).to_csv(RESULTS_DIR / f\"summary_parallel_{tag}.csv\", index=False)\n",
    "        print(f\"[SUMMARY CSV] result/summary_parallel_{tag}.csv\")\n",
    "    if all_errs:\n",
    "        pd.DataFrame(all_errs, columns=[\"Ticker\",\"Error\",\"Round\"]).drop_duplicates().to_csv(RESULTS_DIR / f\"errors_parallel_{tag}.csv\", index=False)\n",
    "        print(f\"[ERRORS CSV]  result/errors_parallel_{tag}.csv\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(\"\\n===== SUMMARY (parallel v1.4) =====\")\n",
    "    print(f\"Tickers total   : {STATS.get('tickers', len(tickers))}\")\n",
    "    print(f\"Daily   fetch/skip : {STATS.get('daily_fetch',0)}/{STATS.get('daily_skip',0)}\")\n",
    "    print(f\"1m      fetch/skip : {STATS.get('m1_fetch',0)}/{STATS.get('m1_skip',0)}  | fallbackâ†’5m: {STATS.get('m1_fallback_5m',0)}\")\n",
    "    print(f\"5m      fetch/skip : {STATS.get('m5_fetch',0)}/{STATS.get('m5_skip',0)}  | fallbackâ†’15m: {STATS.get('m5_fallback_15m',0)}\")\n",
    "    print(f\"15m     fetch/skip : {STATS.get('m15_fetch',0)}/{STATS.get('m15_skip',0)}\")\n",
    "    print(f\"Errors  dns/missing/timeout/empty/other : \"\n",
    "          f\"{len(ERRORS['dns'])}/{len(ERRORS['missing'])}/{len(ERRORS['timeout'])}/{len(ERRORS['empty'])}/{len(ERRORS['other'])}\")\n",
    "    print(f\"Elapsed         : {elapsed:.2f}s\")\n",
    "    return all_rows, tag\n",
    "\n",
    "\n",
    "# ---------- ROBUST SANITY (tetap) ----------\n",
    "\n",
    "def _safe_display(df):\n",
    "    try:\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        print(df.head(20).to_string())\n",
    "\n",
    "\n",
    "def _cols_ok(df):\n",
    "    if df is None or df.empty: return []\n",
    "    known = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\",\"Volume\"]\n",
    "    return [c for c in known if c in df.columns]\n",
    "\n",
    "\n",
    "def _age_days_safe(df):\n",
    "    if df is None or df.empty: return None\n",
    "    try:\n",
    "        last = df.index.max()\n",
    "        if pd.isna(last): return None\n",
    "        return int((pd.Timestamp.utcnow().normalize() - pd.Timestamp(last).normalize()).days)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _is_jkt_session(ts):\n",
    "    import pytz\n",
    "    jkt = pytz.timezone(\"Asia/Jakarta\")\n",
    "    t_utc = _ensure_aware_utc(ts)      # tz-safe\n",
    "    t_jkt = t_utc.tz_convert(jkt)\n",
    "    if t_jkt.weekday() >= 5: return False\n",
    "    tm = t_jkt.time()\n",
    "    return (pd.Timestamp(\"09:00\").time() <= tm < pd.Timestamp(\"11:30\").time()) or \\\n",
    "           (pd.Timestamp(\"13:30\").time() <= tm <= pd.Timestamp(\"15:00\").time())\n",
    "\n",
    "\n",
    "def _resample_1m_to_5m_safe(df_1m: pd.DataFrame):\n",
    "    if df_1m is None or df_1m.empty: return pd.DataFrame()\n",
    "    df = df_1m.copy()\n",
    "    agg = {}\n",
    "    if \"Open\" in df.columns:  agg[\"Open\"]  = \"first\"\n",
    "    if \"High\" in df.columns:  agg[\"High\"]  = \"max\"\n",
    "    if \"Low\" in df.columns:   agg[\"Low\"]   = \"min\"\n",
    "    if \"Close\" in df.columns: agg[\"Close\"] = \"last\"\n",
    "    if \"Volume\" in df.columns:agg[\"Volume\"]= \"sum\"\n",
    "    if \"Adj Close\" in df.columns: agg[\"Adj Close\"] = \"last\"\n",
    "    if not agg:\n",
    "        return pd.DataFrame()\n",
    "    out = df.resample(\"5min\", label=\"right\", closed=\"right\").agg(agg)\n",
    "    price_cols = [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in out.columns]\n",
    "    if price_cols:\n",
    "        out = out.dropna(subset=price_cols, how=\"all\")\n",
    "    return _idx_naive(out)\n",
    "\n",
    "\n",
    "def run_sanity_random_robust(sample_size=SANITY_SAMPLE_SIZE, tol_price=TOL_PCT_PRICE, tol_vol=TOL_VOL_DIFF):\n",
    "    # Kumpulkan tickers dari cache yang ada\n",
    "    s = set()\n",
    "    for d in [CACHE_DAILY_DIR, CACHE_1M_DIR, CACHE_5M_DIR, CACHE_15M_DIR]:\n",
    "        if Path(d).exists():\n",
    "            for f in Path(d).glob(\"*.csv\"):\n",
    "                s.add(f.stem)\n",
    "    tickers = sorted(s)\n",
    "    if not tickers:\n",
    "        print(\"âš ï¸ Tidak ada file di cache_* untuk sanity.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sample = tickers if len(tickers) <= sample_size else random.sample(tickers, sample_size)\n",
    "\n",
    "    rows = []\n",
    "    for tkr in sample:\n",
    "        pD, p1, p5 = CACHE_DAILY_DIR/f\"{tkr}.csv\", CACHE_1M_DIR/f\"{tkr}.csv\", CACHE_5M_DIR/f\"{tkr}.csv\"\n",
    "        dfD, df1, df5 = _read_cache_csv(pD), _read_cache_csv(p1), _read_cache_csv(p5)\n",
    "\n",
    "        d_cols = _cols_ok(dfD)\n",
    "        d_dups = int(dfD.index.duplicated().sum()) if not dfD.empty else 0\n",
    "        d_mono = bool(dfD.index.is_monotonic_increasing) if not dfD.empty else True\n",
    "        d_last = str(dfD.index.max()) if not dfD.empty else \"\"\n",
    "        d_age  = _age_days_safe(dfD)\n",
    "        d_vol0 = int((dfD[\"Volume\"]==0).sum()) if (\"Volume\" in dfD.columns and not dfD.empty) else 0\n",
    "        d_na   = {c: float(dfD[c].isna().mean()) for c in dfD.columns} if not dfD.empty else {}\n",
    "\n",
    "        m1_cols = _cols_ok(df1)\n",
    "        m1_dups = int(df1.index.duplicated().sum()) if not df1.empty else 0\n",
    "        m1_mono = bool(df1.index.is_monotonic_increasing) if not df1.empty else True\n",
    "        m1_last = str(df1.index.max()) if not df1.empty else \"\"\n",
    "        m1_rows = len(df1)\n",
    "        m1_outside = int((~df1.index.to_series().map(_is_jkt_session)).sum()) if (CHECK_BEI_HOURS and not df1.empty) else 0\n",
    "        m1_vol0 = int((df1[\"Volume\"]==0).sum()) if (\"Volume\" in df1.columns and not df1.empty) else 0\n",
    "        m1_na   = {c: float(df1[c].isna().mean()) for c in df1.columns} if not df1.empty else {}\n",
    "\n",
    "        m5_cols = _cols_ok(df5)\n",
    "        m5_dups = int(df5.index.duplicated().sum()) if not df5.empty else 0\n",
    "        m5_mono = bool(df5.index.is_monotonic_increasing) if not df5.empty else True\n",
    "        m5_last = str(df5.index.max()) if not df5.empty else \"\"\n",
    "        m5_rows = len(df5)\n",
    "        m5_outside = int((~df5.index.to_series().map(_is_jkt_session)).sum()) if (CHECK_BEI_HOURS and not df5.empty) else 0\n",
    "        m5_vol0 = int((df5[\"Volume\"]==0).sum()) if (\"Volume\" in df5.columns and not df5.empty) else 0\n",
    "        m5_na   = {c: float(df5[c].isna().mean()) for c in df5.columns} if not df5.empty else {}\n",
    "\n",
    "        # Konsistensi 1m->5m (hari terakhir)\n",
    "        res_ok, mism_price, mism_vol = \"\", None, None\n",
    "        if not df1.empty and not df5.empty:\n",
    "            try:\n",
    "                last_day = pd.Timestamp(df1.index.max()).normalize()\n",
    "                d1 = df1.loc[df1.index.normalize() == last_day]\n",
    "                r5 = _resample_1m_to_5m_safe(d1)\n",
    "                d5 = df5.loc[df5.index.normalize() == last_day]\n",
    "                idx = r5.index.intersection(d5.index)\n",
    "                if not idx.empty:\n",
    "                    r5i, d5i = r5.loc[idx], d5.loc[idx]\n",
    "                    mism_price = False\n",
    "                    for col in [c for c in [\"Open\",\"High\",\"Low\",\"Close\"] if c in r5i.columns and c in d5i.columns]:\n",
    "                        base = d5i[col].replace(0, np.nan).astype(float)\n",
    "                        diff = (r5i[col].astype(float) - d5i[col].astype(float)).abs() / base\n",
    "                        if diff.dropna().gt(tol_price).any():\n",
    "                            mism_price = True; break\n",
    "                    mism_vol = False\n",
    "                    if \"Volume\" in r5i.columns and \"Volume\" in d5i.columns:\n",
    "                        basev = d5i[\"Volume\"].replace(0, np.nan).astype(float)\n",
    "                        diffv = (r5i[\"Volume\"].astype(float) - d5i[\"Volume\"].astype(float)).abs() / basev\n",
    "                        if diffv.dropna().gt(tol_vol).any():\n",
    "                            mism_vol = True\n",
    "                    res_ok = \"OK\" if (not mism_price and not mism_vol) else \"MISMATCH\"\n",
    "                else:\n",
    "                    res_ok = \"NO_OVERLAP\"\n",
    "            except Exception as e:\n",
    "                res_ok = f\"CHECK_ERROR: {e}\"\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": tkr,\n",
    "            \"daily_rows\": len(dfD), \"daily_last\": d_last, \"daily_age_days\": d_age,\n",
    "            \"daily_cols\": \",\".join(d_cols), \"daily_monotonic\": d_mono, \"daily_dups\": d_dups,\n",
    "            \"daily_vol_zero\": d_vol0, \"daily_na_%\": round(sum(d_na.values())/max(1,len(d_na))*100,3) if d_na else None,\n",
    "            \"m1_rows\": m1_rows, \"m1_last\": m1_last, \"m1_cols\": \",\".join(m1_cols), \"m1_monotonic\": m1_mono,\n",
    "            \"m1_dups\": m1_dups, \"m1_outside_session\": m1_outside, \"m1_vol_zero\": m1_vol0,\n",
    "            \"m1_na_%\": round(sum(m1_na.values())/max(1,len(m1_na))*100,3) if m1_na else None,\n",
    "            \"m5_rows\": m5_rows, \"m5_last\": m5_last, \"m5_cols\": \",\".join(m5_cols), \"m5_monotonic\": m5_mono,\n",
    "            \"m5_dups\": m5_dups, \"m5_outside_session\": m5_outside, \"m5_vol_zero\": m5_vol0,\n",
    "            \"m5_na_%\": round(sum(m5_na.values())/max(1,len(m5_na))*100,3) if m5_na else None,\n",
    "            \"consistency_1m_to_5m\": res_ok, \"price_mismatch\": mism_price, \"vol_mismatch\": mism_vol,\n",
    "        })\n",
    "\n",
    "    df_report = pd.DataFrame(rows)\n",
    "    order = [\n",
    "        \"ticker\",\n",
    "        \"daily_rows\",\"daily_last\",\"daily_age_days\",\"daily_cols\",\"daily_monotonic\",\"daily_dups\",\"daily_vol_zero\",\"daily_na_%\",\n",
    "        \"m1_rows\",\"m1_last\",\"m1_cols\",\"m1_monotonic\",\"m1_dups\",\"m1_outside_session\",\"m1_vol_zero\",\"m1_na_%\",\n",
    "        \"m5_rows\",\"m5_last\",\"m5_cols\",\"m5_monotonic\",\"m5_dups\",\"m5_outside_session\",\"m5_vol_zero\",\"m5_na_%\",\n",
    "        \"consistency_1m_to_5m\",\"price_mismatch\",\"vol_mismatch\"\n",
    "    ]\n",
    "    df_report = df_report.reindex(columns=[c for c in order if c in df_report.columns])\n",
    "    out = RESULTS_DIR / f\"sanity_report_{_now_tag_full()}.csv\"\n",
    "    df_report.to_csv(out, index=False)\n",
    "    _safe_display(df_report)\n",
    "    print(f\"\\nâœ… Sanity report saved â†’ {out}\")\n",
    "    return df_report\n",
    "\n",
    "\n",
    "# ---------- WRAPPER EKSEKUSI DENGAN LOGGING ----------\n",
    "\n",
    "def run_all_with_logging():\n",
    "    global FRESH_DAILY_SET, STALE_DAILY_SET\n",
    "\n",
    "    # 0) Ticker roster\n",
    "    tks = _infer_tickers()\n",
    "    STATS[\"tickers\"] = len(tks)\n",
    "\n",
    "    # 0.5) Quick Daily Scan (opsional)\n",
    "    if ALWAYS_FRESH_MODE and QUICK_DAILY_CHECK:\n",
    "        FRESH_DAILY_SET, STALE_DAILY_SET = scan_daily_freshness(tks)\n",
    "        print(f\"QuickDaily: fresh={len(FRESH_DAILY_SET)} stale={len(STALE_DAILY_SET)} (of {len(tks)})\")\n",
    "    else:\n",
    "        FRESH_DAILY_SET, STALE_DAILY_SET = set(), set(tks)\n",
    "\n",
    "    # 1) Warmup (opsional)\n",
    "    if DO_WARMUP:\n",
    "        try:\n",
    "            print(f\"Parallel warmup v1.4 â€¢ candidates={len(tks)}\")\n",
    "            _ = run_parallel_warmup_v14(tks)\n",
    "        except Exception as e:\n",
    "            logp = RESULTS_DIR / f\"error_runtime_warmup_{_now_tag_full()}.log\"\n",
    "            with open(logp, \"w\") as f:\n",
    "                f.write(\"WARMUP ERROR\\n\")\n",
    "                traceback.print_exc(file=f)\n",
    "            print(f\"âš ï¸ Warmup error tertangkap. Log â†’ {logp}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Warmup SKIPPED (DO_WARMUP=False).\")\n",
    "\n",
    "    # 2) Sanity (opsional)\n",
    "    if SANITY_ENABLE:\n",
    "        try:\n",
    "            print(\"\\n=== SANITY (random 10) ===\")\n",
    "            _ = run_sanity_random_robust(sample_size=SANITY_SAMPLE_SIZE)\n",
    "        except Exception as e:\n",
    "            logp = RESULTS_DIR / f\"error_runtime_sanity_{_now_tag_full()}.log\"\n",
    "            with open(logp, \"w\") as f:\n",
    "                f.write(\"SANITY ERROR\\n\")\n",
    "                traceback.print_exc(file=f)\n",
    "            print(f\"âš ï¸ Sanity error tertangkap. Log â†’ {logp}\")\n",
    "    else:\n",
    "        print(\"Sanity check dimatikan (SANITY_ENABLE=False).\")\n",
    "\n",
    "\n",
    "# --- RUN ONCE ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_with_logging()\n",
    "\n",
    "# Baseline v1.3.3 taken from the user's previous script as reference for compatibility and structure.\n",
    "# (See chat/file reference).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
